{"index":{"_index":"pages","_type":"blog","_id":"blog_controlling-an-arduino-from-the-browser-using-deepstream_readme.md"}}
{"filePath":"blog/controlling-an-arduino-from-the-browser-using-deepstream/readme.md","title":"Controlling an Arduino from the browser using deepstream.io","content":"\r\n\r\nWriting a deepstream client only requires a TCP connection, basic string manipulation and a bit of JSON parsing. This means it can run on any hardware platform with any programming language. And to prove it I’m going to attempt to get it working on an Arduino uno!\r\n\r\nWhat we will be covering in this blog entry is building a basic client with the ability to login and recieve/publish events. The same process can then be applied to achieve the full set of functionality including records, RPCs and webRTC.\r\n\r\nLet's first set the requirements:\r\n\r\n* Arduino can setup a TCP connection\r\n* Arduino can authenticate\r\n* Arduino can recieve events telling it to switch the LED on or off\r\n* Arduino can listen if any clients are subscribed to temperature events\r\n* Arduino can send out the temperature whenever it changes\r\n* Bonus: Setup another client to recieve/send events\r\n\r\n# The board\r\n\r\nYou'll need the following components:\r\n* A W5100 [ethernet shield](//www.Arduino.cc/en/Main/ArduinoBoardEthernet)\r\n* An Arduino [uno](//www.Arduino.cc/en/Main/ArduinoBoardUno)\r\n* A red LED\r\n* A temperature sensor\r\n\r\nYou can easily use different communication components such as [WiFi](//www.Arduino.cc/en/Guide/ArduinoWiFiShield101) or [bridge](//www.Arduino.cc/en/Reference/YunClientConstructor), but make sure to use their associated libraries instead.\r\n\r\nWiring it all together the board ended up looking like this:\r\n\r\n<img width=\"100%\" src=\"./basic-deepstream-arduino-setup.jpg\" />\r\n\r\nForgive me for not using the correct color cables, I ran out of a few\r\nin a soldering accident =(\r\n\r\nNow for the fun part!\r\n\r\n# Connecting\r\n\r\nThe first thing we need to do is to be able to get the Arduino board to connect to deepstream. Since I'm using a W5100 [ethernet shield](//www.Arduino.cc/en/Main/ArduinoBoardEthernet) all I have to do is include the library and call `ethernet.begin( mac )` inside of the setup.\r\n\r\n```clike\r\nlog(\"Getting IP...\");\r\n//Each shield has a different IP, you can find them on a sticker\r\n//on new ones on the back\r\nEthernet.begin( mac );\r\n//Print the IP you got. If it is 0.0.0.0 it means you didn't\r\n//get an IP assigned and you would need to reset your board\r\nlog( Ethernet.localIP() );\r\n```\r\n\r\nOnce I get my IP I can try connecting to deepstream!\r\n\r\n```clike\r\nString state = \"AWAITING_AUTHENTICATION\";\r\nconst IP deepstreamHost = {192,168,0,201};\r\nconst int deepstreamPort = 6021;\r\n\r\nlog(\"Connecting...\");\r\n\r\nif (client.connect( deepstreamHost, deepstreamPort )) {\r\n    log(\"Connected!\");\r\n} else {\r\n    log(\"Connection faiLED\");\r\n}\r\n```\r\n\r\nNote: I initialized [the connection state](/docs/common/constants/#connection-state) with AWAITING_AUTHENTICATION. This can also be initialized with CLOSE and then changed to AWAITING_AUTHENTICATION once a connection is successfully opened.\r\n\r\n# Constructing Messages\r\n\r\nI wrote a bunch of generic utility methods to make it easier for me to create deepstream messages for this example. Take a look at [how messages are structured](/info/protocol/message-structure-overview/) for more details.\r\n\r\n```clike\r\nString part = String( char( 31 ));\r\nString split = String( char( 30 ));\r\n\r\nString createMessage( String type, String action ) {\r\n    return String( type + part + action + split );\r\n}\r\n\r\nString createMessage( String type, String action, String data1 ) {\r\n    return String( type + part + action + part + data1 + split );\r\n}\r\n\r\nString createMessage( String type, String action, String data1, String data2 ) {\r\n    return String( type + part + action + part + data1 + part + data2 + split );\r\n}\r\n```\r\n\r\nAs you can see all the utility methods do are take parameters and concatenate them with the correct message part and end characters. It certainly isn't the most efficient manner of concatenating strings' but its simplicity is great for this example.\r\n\r\n# Logging In\r\n\r\nNow that we have a connected client and a bunch of utility methods, lets start looking into getting logged in!\r\n\r\nThe first thing we need to do is to send our credentials. Once we are in the  AWAITING_AUTHENTICATING state we are ready to send the login details. We will login using the Arduino ID to ensure it has permissions to actually trigger and subscribe to events. All that's needed is to pass in the ID via the [login message](/info/protocol/all-messages/#auth-login).\r\n\r\n```clike\r\nif( state == \"AWAITING_AUTHENTICATION\" ) {\r\n    client.print( createMessage( \"A\", \"REQ\", \"{\\\"Arduino\\\":\\\"x6C1gD\\\"}\" ) );\r\n    state = \"AUTHENTICATING\";\r\n }\r\n```\r\n\r\nAnd now the client has to wait until it recieves a [successful response](/info/protocol/all-messages/#auth-loginsuccess).\r\n\r\n```clike\r\nif( state == \"AUTHENTICATING\" && message == createMessage( \"A\", \"A\" ) ) {\r\n  state = \"OPEN\";\r\n  log( \"Connection State: \" + state );\r\n}\r\nelse {\r\n  log( \"Unknown message recieved:\" + message );\r\n}\r\nmessage = \"\";\r\n```\r\n\r\n# Subscribing and Recieving Data\r\n\r\nNow that you're logged in you'll want to [subscribe](/info/protocol/all-messages/#event-subscribe) to events. In order to create an event subscription you'll have to create a message with an event topic, subscribe action and the event you're interested in.\r\n\r\n```clike\r\nlog( \"Subscribing to event led-red\" );\r\nclient.print( createMessage( \"E\", \"S\", \"led-red\" ) );\r\n```\r\n\r\nBut you're now getting an unknown message back\r\n\r\n```clike\r\nE|A|S|led-red+\r\n```\r\n\r\nLooks like the server responded with a [subscribe ack](/info/protocol/all-messages/#event-subscribe). Awesome, means your connection is working! You don't actually need to do anything with the ack, we're just logging it for assurance.\r\n\r\n```clike\r\nif( state == \"OPEN\" && message == createMessage( \"E\", \"A\", \"S\", \"led-red\" ) ) {\r\n    log( \"Subscribe to event led-red AcknowLEDged\" );\r\n}\r\n```\r\n\r\nAnd now we can start listening to [incoming events](/info/protocol/all-messages/#event-publish)! Since this is just a basic LED the only options we have are switching it on or off, which are just true or false [types](/docs/common/constants/#data-types) that deepstream minimizes to a single T or F character.\r\n\r\n```clike\r\nif( state == \"OPEN\" && message == createMessage( \"E\", \"EVT\", \"led-red\", \"T\" ) ) {\r\n      log( \"Turn led-red On\" );\r\n      digitalWrite(LED_PIN, HIGH);\r\n} else if( state == \"OPEN\" && message == createMessage( \"E\", \"EVT\", \"led-red\",\"F\" ) ) {\r\n    log( \"Turn led-red OFF\" );\r\n    digitalWrite(LED_PIN, LOW);\r\n}\r\n```\r\n\r\nAnd now we can switch on our LED from any client by just emiting the led-red [event](/info/protocol/all-messages/#event-publish).\r\n\r\n```javascript\r\nds.event.emit( 'led-red', true );\r\n```\r\n\r\n# Listening and Publishing Data\r\n\r\nSo that's over half the work done. But IoT devices are also often used for providing data. So let's get a temperature value streamed to deepstream whenever it changes.\r\n\r\nHold on! Before we start just sending data into the world, it would be good to know whether or not something is interested in the event. Especially with devices where power consumption is important it's good to only send things when you know someone is cares.\r\n\r\nIn this case, we only want to emit temperature changes when a client [listening](/info/protocol/all-messages/#event-listen) to an event named 'temperature'.\r\n\r\n```clike\r\nlog( \"Lisening to event temperature\" );\r\nclient.print( createMessage( \"E\", \"L\", \"/^temperature$/\" ) );\r\n```\r\n\r\nAs with 'subscribes', the server will respond with an [ack](/info/protocol/all-messages/#event-listen) to confirm it recieved the message.\r\n\r\n```clike\r\nif( state == \"OPEN\" && message == createMessage( \"E\", \"A\", \"L\", \"/^temperature$/\" ) ) {\r\n    log( \"Listening to event pattern temperature Acknowledged\" );\r\n}\r\n```\r\n\r\nAnd we should now be notified whenever there's at least one client [listening](/info/protocol/all-messages/#event-listenmatch) or when all clients [stop](/info/protocol/all-messages/#event-listenunmatch).\r\n\r\n```clike\r\nboolean publishingTemperature = false;\r\nif( state == \"OPEN\" && message == createMessage( \"E\", \"SP\", \"/^temperature$/\", \"temperature\" ) ) {\r\n    publishingTemperature = true;\r\n    log( \"A Client is interested in the temperature!\" );\r\n} else if( state == \"OPEN\" && message == createMessage( \"E\", \"SL\", \"/^temperature$/\", \"temperature\" ) ) {\r\n    publishingTemperature = true;\r\n    log( \"Clients are no longer interested in the temperature\" );\r\n}\r\n```\r\n\r\nSince we don't actually have to recieve anything to send the values, the logic to send the data is outside the Ethernet [`client.available()`](//www.arduino.cc/en/Reference/ClientAvailable).\r\n\r\nSending an event just requires a message to be sent with the correct fields. Note the use of [type](/docs/common/constants/#data-types) N in the data-payload. This lets clients who recieve the event to a number to know it's a number.\r\n\r\n```clike\r\nif( publishingTemperature == true ) {\r\n    client.print( createMessage( \"E\", \"EVT\", \"temperature\", String( \"N\" + newTemp ) ) );\r\n}\r\n```\r\n\r\n# The browser client\r\n\r\n<img width=\"100%\" src=\"./basic-smarthome-portal.jpg\" />\r\n\r\nIf you run the `index.html` page in your browser you can now switch the LED on or off as well as get updates temperature updates on your virtual smart home dashboard!\r\n\r\n# Summary\r\n\r\nAnd that's it! Everything needed to get an Arduino connected to deepstream!\r\n\r\nThis tutorial is meant to walk you through the basic authentication and event messages so you get a feel for how the client and server interacts. There is a wealth of different options within the deepstream spec that can enhance this example, please look at the [documentation](/tutorials/core/writing-a-client/) for an in-depth guide.\r\n\r\nA couple of examples how the Ardruino client can be enhanced:\r\n\r\n* The LED light could be a record so it can just continue from it's previous state when first logging in\r\n\r\n* RPCs will also provide a great solution on cheap IoT devices as they can delegate resource intensive methods to a more powerful client, or cut the problem into small enough portions that can be distributed over a large IoT network.\r\n\r\n* Logging could occur remotely using events so that you can monitor issues anywhere without using the Ardruino Serial logger ( as long as the deepstream client is logged in! )\r\n\r\nI hope this article inspired you to experiment with deepstream messaging, or just to play around with this example on different IoT devices!\r\n\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_deepstreamhub-raises-one-million-seed-round_readme.md"}}
{"filePath":"blog/deepstreamhub-raises-one-million-seed-round/readme.md","title":"deepstreamHub raises $1 Million seed round to turn realtime into a platform","content":"\r\n\r\nEver since we started working on deepstream.io, our open source realtime server, we’ve felt that it could be the foundation for something bigger: a platform that enables developers of all backgrounds to build stunning realtime apps. A platform that connects to a wealth of tools, services and third party systems. A platform called deepstreamHub.\r\n\r\nWe’ve been working hard over the last months to build this platform and today we are one step closer to that goal. We are thrilled to announce that we’ve closed a $1 Million seed round with BlueYard Capital.\r\n\r\nIt is an exciting experience to know that already over ten thousand developers are using deepstream to build realtime apps and that now some of the most pivotal figures in Berlin’s tech scene share our vision to develop it into a platform and make deepstreamHub a reality.\r\n\r\n### What is deepstreamHub?\r\n\r\nDeepstreamhub provides your application's realtime backend, so you don't have to build it yourself. It is an integrated service platform that browsers, desktops, servers, mobile apps and IoT devices can connect to using tiny client libraries.\r\n\r\nDeepstreamHub provides three core concepts that form the building blocks of any successful realtime app: data­sync (as you might know from e.g. Firebase), pub/sub (as you might know from e.g. Pusher/PubNub) and request/response (as in e.g. Http or MeteorJS).\r\n\r\nThese building blocks by themselve provide a great foundation for any app, but deepstreamHub is about taking a step beyond ­ about creating a platform of easily accessible realtime services.\r\n\r\nIt lets every client perform realtime searches, send and receive emails, texts or push notifications, schedule periodic tasks, push jobs to queues and access an array of other development features.\r\n\r\nBut we won’t stop there. DeepstreamHub is about providing a rich and open ecosystem of third party integrations that allow users to stream data from and to messengers like Slack, developer platforms like GitHub, social networks, IoT endpoints, payment providers, open data sets etc.\r\n\r\n![deepstreamHub dashboard](dshub-select-channel.png)\r\n\r\n### Who is deepstreamHub for?\r\n\r\nUsers are increasingly expecting real time apps. And any team building one ­ from social messaging, collaborative document editing or realtime geolocation services to split­second stock price delivery and IoT data distribution ­ can save time and development cost by integrating with  deepstreamHub.\r\n\r\ndeepstreamHub’s extremely fast message delivery makes also makes it a good fit for demanding use cases like multiplayer gaming or finance, while its third party integrations make it a great time­saver for tech­verticals like messenger­as­a­platform, aggregation dashboards or process monitoring.\r\n\r\n### Why open source first?\r\n\r\nDeepstreamHub is based on deepstream.io, a realtime server that’s fully open source and has already seen great traction in the community. While it seems counterintuitive to make the core of your service available for free, this is increasingly becoming an industry standard ­ and for two very good reasons:\r\n\r\nA great and supportive community. We believe in open source and are proud to say that despite its young age deepstream.io is already powering production apps and has gathered a great community of explorers, tinkerers and contributors.\r\n\r\nLess technical risk / vendor lock­in: A common and justified concern about building applications on top of platform­as­a­service offerings is the associated third party risk. The recent shut­down of popular services like parse.com has increased this concern. By open sourcing the core of our stack from day one, we provide a valuable fallback and alleviate this risk.\r\n\r\n<div class=\"img-box\">\r\n    <img src=\"founders-01.png\" alt=\"DeepstreamHub Founders\" />\r\n    <label>deepstreamHub founders Wolfram Hempel &amp; Yasser Fadl</label>\r\n</div>\r\n\r\n### What’s next?\r\n\r\nDeepstreamHub is an ambitious undertaking that can change the way realtime applications are built, so we are working on it feverishly until it is ready to release into the wild.. If you’d like to be among the first to take it for a test­drive, make sure to sign up [here](https://deepstreamhub.com/#newsletter). In the meantime we will also continue to develop deepstream.io together with our community.\r\n\r\nStay up to date by following us on [Twitter](https://twitter.com/deepstreamIO) and if you’d like to join us in reshaping the realtime web, browse our [open positions](https://deepstreamhub.com/careers/)\r\n\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_introducing-deepstreamhub_readme.md"}}
{"filePath":"blog/introducing-deepstreamhub/readme.md","title":"Introducing deepstreamHub","content":"\r\n\r\nEver since we’ve started working on deepstream.io, we’ve been toying with the idea of using it to build something bigger: A platform that makes it easy for developers from all backgrounds to build great real-time apps, using their own data as well as live feeds from a myriad of services.\r\n\r\nToday, we’re ready to announce that we’ve started developing just that: **deepstreamHub**.\r\n\r\n![deepstreamHub](deepstream-hub-750.png)\r\n\r\ndeepstreamHub will be a hosted deepstream cluster in the cloud, comparable to Firebase, Pusher or realtime.co, but offering the full deepstream feature set of data-sync, pub-sub, request-response and WebRTC.\r\n\r\nWhere it gets really interesting though are deepstreamHub’s __Channels__. Channels let you stream data from third party services directly into your frontend – without any integration code.\r\n\r\n### Here’s how it works:\r\n\r\nChannels let you connect to a wide array of services. Collaboration tools (e.g. Trello, Google Docs), communication (e.g. Slack), dev-tools (e.g. Github), social networks, content management systems etc…\r\n\r\nWe're also planning premium channels for Financial Data, Sports Results, Payment, CRM systems and so on. Whichever channels you add to your app, the process is always the same:\r\n\r\n1) Select one or more channels:\r\n\r\n![Selecting a channel](dshub-select-channel.png)\r\n\r\n2) Add the channel:\r\n\r\nView the channel description to get a clear view on the type of data the channel provides\r\n\r\n![Adding a channel](dshub-add-channel.png)\r\n\r\n3) Authenticate:\r\n\r\nAuthenticate with the third party service to work with deepstreamHub\r\n\r\n![Authenticate a channel](dshub-authenticate-channel.png)\r\n\r\n4) Use it\r\n\r\nAnd that’s it! From here on your channel’s real-time data is available to all connected front or backend services as ds-records, lists or events.\r\n\r\nIf you’d like to learn more, follow us on [twitter](//twitter.com/deepstreamIO) or subscribe to our newsletter. And of course, give deepstream.io a try…it’s actually quite neat.\r\n\r\n\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_load-balancing-websocket-connections_readme.md"}}
{"filePath":"blog/load-balancing-websocket-connections/readme.md","title":"Load Balancing Websocket Connections","content":"\r\n\r\nLoadbalancing for Websockets sucks. But I guess we can't complain. HTTP has been around for 27 years and we had plenty of time to develop a mature infrastructure to handle traffic for even the biggest websites.\r\n\r\nWebSockets on the other hand only became a standard in 2011 and we're just starting to create the infrastructure necessary to use them at scale.\r\n\r\n## So what's the problem?\r\nIn a word: Concurrency. Traditional load balancing approaches are geared towards short lived requests that yield an immediate response. This means that even a traffic heavy site with a million requests per minute that take ~10ms to complete will stay well below 200 concurrent connections at any given point.\r\n\r\nWebsockets on the other hand are persistent - this means that a large number of connections needs to be kept open simultaneously. This comes with a number of challenges:\r\n\r\n### File Descriptor Limits\r\nFile descriptors are used by operating systems to allocate files, connections and a number of other concepts. Every time a loadbalancer proxies a connection, it creates two file descriptors - one for the incoming and one for the outgoing part.\r\n\r\nEach open file descriptor consumes a tiny amount of memory, the [limits of which can be freely assigned](http://www.cyberciti.biz/faq/linux-increase-the-maximum-number-of-open-files/) - a good rule of thumb is to allow 256 descriptors for every 4MB of RAM available. For a system with 8GB of RAM, this gets us about half a million concurrent connections - a good start, but not exactly Facebook dimensions just yet.\r\n\r\n### Ephemeral Port Limits\r\nEvery time a loadbalancer connects to a backend server, it uses an \"Ephemeral Port\". Theoretically, 65.535 of these ports are available, yet most modern Linux distributions [limit the range to 28.232 by default](http://www.ncftp.com/ncftpd/doc/misc/ephemeral_ports.html). This still doesn't sound too bad, but ports don't become available straight away after they've been used. Instead they enter a [TIME_WAIT state](http://www.isi.edu/touch/pubs/infocomm99/infocomm99-web/) to make sure they're not missing any packages. This state can last up to a minute, severely limiting the range of outgoing ports.\r\n\r\n### Session allocation for multi-protocol requests\r\nMost real world bi-directional connectivity implementations (e.g. [socket.io](http://socket.io/) or [SignalR](http://signalr.net/) ) use a mix of Websockets and a supporting protocol, usually HTTP long-polling.\r\nThis was traditionally done as a fallback for browsers lacking Websocket support, but is still a good idea as the leading HTTP request can help convince Firewalls and network switches to process the following Websocket request.\r\n\r\nThe trouble is: Both HTTP and WebSocket requests need to be routed to the same backend server by the load-balancer (sticky sessions). There are two ways to do this, both of which come with their own set of problems:\r\n\r\n- **source-IP-port Hashing** calculates a hash based on the client's signature. This is a simple and - most importantly - stateless way to allocate incoming connections to the same endpoint, but it's very coarse. If a large company's internal network lives behind a single NAT (Network Address Translation) gateway, it will look to the loadbalancer like a single client and all connections will be routed to the same endpoint.\r\n\r\n- **cookie injection** adds a cookie to the incoming HTTP and Websocket requests. Depending on the implementation this can mean that all loadbalancers need to keep a shared table of cookie-to-endpoint mappings. It also requires the loadbalancer to be the SSL-Termination point (the bit of the network infrastructure that decrypts incoming HTTPS and WSS traffic) in order to be able to manipulate the request.\r\n\r\n## The Solution(s)\r\nLoadbalancing Websockets is a tough problem, but not an unsolvable one. Various solutions exist. They can broadly be categorized as: DNS, Hardware Layer 3 and Software Layer 3 or Layer 7. Phew, sounds tricky... let's look at them one by one:\r\n\r\n### DNS Loadbalancing\r\nThe Domain Name System is a decentralized network of nodes that sits between you and the server you want to reach. It translates domains (example.com) into IPs and... let me stop here, I'm sure you know all this already.\r\nWhat's important in our context is that DNS has a many-to-many relationship between domains and IPs. A single A-Record (domain) or C-NAME (subdomain) can resolve to multiple IPs and the DNS will route requests in a round robin fashion.\r\n\r\n**The upsides** are that DNS is incredibly resilient and scalable. Should it ever be unavailable, your problem is most likely more of an apocalyptic nature than a technical one. DNS also isn't something you'd need to maintain, so you pretty much get your load balancing for free.\r\n\r\n**The downsides** are that DNS loadbalancing is very basic. DNS doesn't perform healthchecks (although some cloud provider based name servers do, e.g. [AWS Route 53](https://aws.amazon.com/route53/)), doesn't provide SSL termination, doesn't allow for complex weighting algorithms and will continue routing traffic to configured endpoints, regardsless if they're reachable or not. DNS Zone files are also heavily cached, so changes can take a while to propagate.\r\n\r\n### Hardware Layer 3 / 4 load balancers\r\nOk, what's with these layers? The notion of layers stems from the [Open Systems Interconnection model](https://en.wikipedia.org/wiki/OSI_model), an attempt of categorizing network interaction in an abstract, technology independent way. Highly simplified:\r\n\r\n- **Layer 3** is the abstract networking layer - this is where the internet protocol (IP) lives and raw packets are sent.\r\n\r\n- **Layer 4** is the transport layer that has concepts of acknowledgments, resends etc. It's the realm of the Transmission Control Protocol (TCP).\r\n\r\n- Moving a few layers up, the final bit that's important for load balancing is **layer 7**, the application layer. This is where complex, content aware and feature rich HTTP messages are sent.\r\n\r\nThe fastest and most powerful loadbalancing mechanism after DNS are hardware level 3 switches. They usually come in the shape of \"blades\" that can be slotted into a blade server rack.\r\n\r\n![Level 3 Switch](./switch.jpg)\r\n\r\nIt is however 2016 and most of us have probably gotten quite comfortable leaving this sort of thing to [AWS](https://aws.amazon.com/), [GCP](https://cloud.google.com/), [Rackspace](https://www.rackspace.com/) or [Digital Ocean](https://www.digitalocean.com/) etc., so we'll just leave it at that.\r\n\r\n### Layer 3 / 4 / 7 software loadbalancing\r\nUsing a software loadbalancer is in many ways the easiest and most flexible solution. There are a lot of great servers to choose from for this task, e.g. [Nginx with the Stream module enabled](https://deepstream.io/tutorials/integrations/other-nginx/), [HA Proxy](http://www.haproxy.org/) or [Apache with mod_proxy](https://httpd.apache.org/docs/current/mod/mod_proxy.html) to name just a few.\r\n\r\n![Software load balancer](software-load-balancer.png)\r\n\r\n**The upsides** are plentiful. Most solutions in this space can perform a multitude of tasks, such as health checks, SSL termination, cookie injection or IP hashing. They're (comparatively) easy to set up and maintain, well documented and fast enough for most usecases.\r\n\r\n**The downsides** are associated with connection concurrency: Software loadbalancers/reverse proxies run on a single machine. They are subject to that machine's File Descriptor and Ephemeral Port limitations and often heavily utilise it resources. This provides a hard limit to the number of concurrent connections that can be handled which makes the LB the scalability bottleneck / single point of failure within an architecture.\r\n\r\n### Orchestrator approach\r\nAn interesting alternative to traditional load balancing concepts is the use of an Orchestration Server. This server keeps track of the available backend nodes, performs health checks and cluster management tasks. On top of this it keeps an array of external URLs of backend servers and provides an HTTP API for clients to retrieve an endpoint URL.\r\n\r\n![Orchestrator](./orchestrator.png)\r\n\r\nOrchestration Servers are a common concept in cluster architectures, e.g. [Apache Zookeeper](https://zookeeper.apache.org/) or [Redis Sentinel](http://redis.io/topics/sentinel)\r\n\r\n**The upsides** are mainly scalability and flexibility. Orchestration servers provide a simple HTTP API to retrieve endpoint URLs to clients. This API is very lightweight and can be traditionally loadbalanced. Since clients will connect to endpoints directly, there are no limitations associated with concurrency and no need to inject cookies or use other methods to allocate sessions.\r\nWhat makes this solution particularly powerful is its flexibility which allows for the implementation of high level ressource utilisation concepts. An orchestrator can base its endpoint allocation on very detailed data from the backend nodes and the incoming client connection.\r\n\r\n**The downsides** are that every endpoint needs to be publicly accessible. For TCP connections this requires a dedicated IP per endpoint. Likewise, this solution lacks the additional layer of security that's added by a load balancer. To offset this, it might make sense to front each endpoint with its own reverse proxy.\r\n\r\n## Conclusion\r\nSo, what's the best choice? As so often, the only true answer is: It depends!\r\nAll loadbalancing approaches described above will help facilitate small to medium deployments effectively, but for a truly scalable setup you're probably best of with a hybrid approach.\r\n\r\nSomething often used in practise is a combination of multiple smaller groups of backend servers, fronted by a software load balancer. DNS in turn is used to route traffic to the individual load balancers. This adds an extra layer of indirection and can be enormously scalable, but comes with added complexity and an additional network hop for information to pass through.\r\n\r\n![DNS + load balancer](./dsn-load-balancer.png)\r\n\r\nOrchestration server / endpoint allocation approaches on the other hand allow for very large deployments and make it possible to manage the available resources most efficiently, but require a lot of bespoke development.\r\n\r\nCloud Hosting providers can take a lot of heavy lifting of you, but even there the support for large scale websocket deployments is still in its infancy.\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_listen-benefits_readme.md"}}
{"filePath":"blog/listen-benefits/readme.md","title":"The benefits of listening","content":"\r\n\r\nRealtime systems have a reputation of being expensive to run. Lets take a look at why - and what we can do to make it better. But before we start, let’s take a quick look at how many realtime systems work today.\r\n\r\nRealtime uses the [push](https://en.wikipedia.org/wiki/Push_technology) paradigm to tell the world when something happens. This means it sends updates immediately rather than wait to be asked.\r\n\r\nWhere this all gets really interesting is on the very other end of your stack, in the land of databases, your server code and integrations with third-party data. Pretty much the rest of the iceberg that makes your application functional but is hidden away behind a beautiful minimalistic interface. Those systems tend to be run in a bit of isolation, meaning they often don't know or care if any clients are actually interested in what they are publishing or if they're literally just creating redundant data no one will ever notice (other than the people from devops monitoring your resource consumption).\r\n\r\nLet’s take a weather app for example. As you can see in the image below, you have a large amount of changing data being constantly pumped into the system, although the client is only interested in Germany.\r\n\r\n![Usual PubSub Workflow](usual-pubsub-workflow.png)\r\n\r\n### So where does listening come in?\r\n\r\nThe goal in any realtime system is to limit the amount of moving data in order to reduce the total required resources and costs. One way to achieve this is by making sure that at least one client has actually requested something before you start pushing data to it.\r\n\r\nThis works amazingly well for events, as the responsibility is taken off the publisher to submit all events constantly. If you look at this in context of hardware, like IoT sensors, it could result in huge improvements in energy usage, since the device can suspend activity until it knows it's needed. In context of software, it reduces the load on the system significantly since only required events would ever be fired. As you can see in the image below, a publisher that is aware of what data is requested has much less activity with the same result.\r\n\r\n![Listening PubSub Workflow](pubsub-with-listening-workflow.png)\r\n\r\nFinally, this approach shines the most when the data being sent out is actually driven by the name of your subscription. For example, subscribing to 'weather/germany-berlin-1w' tells the publisher to send information for one week forecasts.\r\n#### How to implement\r\n\r\nThe provider listens to events matching the name 'weather/'\r\n\r\n```javascript\r\nds.event.listen( '^weather/.*', onMatch );\r\n```\r\n\r\nAnd the `onMatch` callback will be notified for all the current matches in the system, new ones that come in and old ones that no one is interested in anymore.\r\n\r\n```javascript\r\nfunction onMatch( subject, isInterested ) {\r\n    //Subject is the name of the event or record that is being subscribed to\r\n    //If isInterested is true it means you should start publishing data, if it's false it means you should stop\r\n}\r\n```\r\n\r\nIf the provider can no longer provide the data, it can stop listening by calling:\r\n\r\n```javascript\r\nds.event.unlisten( '^weather/.*', onMatch );\r\n```\r\n\r\n#### How it scales\r\n\r\nYou don't want more than one publisher updating the same record or emitting the same event. But you do want to scale your publishers so they can cope with huge amounts of data coming into your system, often from the same third party service. You can use the listen functionality to easily split concerns by handling more specific requests across multiple publishers.\r\n\r\nFor example, you could create two identical providers except where one listens to requests to all countries starting with the first half of the alphabet '^weather/[a-k]' and the other to the second half '^weather/[m-z]'. This can be split into as many publishers as you see fit to easily scale your processes.\r\n\r\nFor more details and code, take a look at our [tutorial](../../tutorials/core/active-data-providers/)."}
{"index":{"_index":"pages","_type":"blog","_id":"blog_migrating-parse-com-to-deepstream-io_readme.md"}}
{"filePath":"blog/migrating-parse-com-to-deepstream-io/readme.md","title":"Migrating from Parse.com to deepstream.io","content":"\r\n\r\nWriting this feels almost irreverent. Parse.com was a great platform and despite its stability issues one of the most innovative and clever technologies in the realtime space. It was one of the great inspirations for deepstream.io and it took us very much by surprise to [see it being shut down](//blog.parse.com/announcements/moving-on/).\r\n\r\n![parse.com and deepstream.io](parse-deepstream.png)\r\n\r\n#### What’s deepstream.io and how can it help with my parse.com app?\r\n[deepstream.io](//deepstream.io/) is a scalable open source realtime server written in NodeJS. It can be used in both browsers and backend services through client SDKs, offers powerful user handling and permissioning, can be connected to databases, caches and message queues and scales horizontally via clustering.\r\n\r\nDeepstream clients are small, self-contained and framework agnostic. They can easily be used with [React](/tutorials/integrations/frontend-react/), [Angular](/tutorials/integrations/frontend-angular/), [Knockout](/tutorials/integrations/frontend-knockout/), [Backbone](//backbonejs.org/) or whatever else your heart desires.\r\n\r\n#### What do deepstream.io and parse.com have in common?\r\ndeepstream.io offers data-sync via `records` which are very similar to `Parse.Objects`. Records have a unique ID, `get()` and `set()` methods and can be used to store, manipulate and observe data in schemaless JSON structures.\r\n\r\ndeepstream.io is designed as a modular platform, which makes it possible to replicate Parse.com’s more advanced features using deepstream connectors. This means that advanced realtime queries and GeoPoints are supported through [deepstream’s RethinkDB integration](/tutorials/integrations/db-rethinkdb/) and mobile push notifications can be achieved [via AWS SNS](../publishing-aws-sns-messages-to-browsers-tutorial/)\r\n\r\n#### What does deepstream.io support that Parse.com does not?\r\nDeepstream doesn't just offer data-sync via `records`, but also publish-subscribe (comparable to Pusher or PubNub) via `events`, request-response via `RPC` (remote procedure call) and even WebRTC video call management.\r\n\r\n#### What does Parse.com support that deepstream.io does not?\r\ndeepstream.io is a server, not a service. Things like data browsing, permissions or active sessions are done programmatically, rather than trough a dashboard.\r\nWhilst deepstream record’s can be bigger than `Parse.Objects`, (1MB instead of 128k), it does not support binary files. So there is no substitute for `Parse.File`.\r\nFinally, Parse.com offers a significantly wider range of available client's and SDKs for different languages. [This is something we are actively working on, any help is very much appreciated](//github.com/deepstreamIO/deepstream.io/issues?q=is%3Aissue+is%3Aopen+label%3Anew-client)\r\n\r\n#### Migrating from Parse.Object to deepstream records\r\nRecords are the key concept for deepstream's data-sync. They are a unit of schemaless JSON data that's synced between clients.\r\nRecords are simply meant to be used, rather than extended. Each deepstream record is identified by a unique name. In difference to Parse, these names have to be created explicitly. It's also worth noting that there is no difference between creating a new record and retrieving an existing one, you always just use `getRecord()`\r\n\r\n```javascript\r\n//parse.com\r\nGameScore = Parse.Object.extend(\"GameScore\");\r\ngameScore = new GameScore();\r\n\r\n//deepstream.io\r\ngameScore = ds.record.getRecord( 'gamescore/yankees-vs-mets-2015' );\r\ngameScore = ds.record.getRecord( 'gamescore/' + ds.getUid() ); // or just generate a Unique ID\r\n```\r\n\r\nThis highlights an important difference between Parse and deepstream. Following Backbone.js principles, Parse automatically organizes objects in collections. When you specify a class of type `GameScore`, Parse saves every new instance automatically to the GameScore collection.\r\n[Deepstream also offers a concept of collections, called `Lists`](/tutorials/core/datasync-lists/), but records have to be explicitly added to them.\r\n\r\n#### Getting values\r\nBoth `Parse.Object` and deepstream records have a method called `get()` that works almost identically.\r\n```javascript\r\n//deepstream.io\r\ndata = gameScore.get(); //returns the entire data\r\nplayerName = gameScore.get( 'playerName' ); //retrieves a value\r\n```\r\n\r\nAs deepstream records can be any JSON structure, deepstream's `get()` can be used with paths in JSON notation as well, e.g.\r\n\r\n```javascript\r\nfirstname = gameScore.get( 'player.firstname' ); //Use dot notation to traverse objects\r\nlastSpouse = gameScore.get( 'player.spouses[0]' ); //Arrays work as well\r\n```\r\n\r\n#### Setting values\r\nSetting values is pretty much the same between Parse and deepstream. Both have a `set()` method that takes a key (or even a JSON path in deepstream) and a value. **The main difference is that deepstream sends a small, atomic update message every time `set()` is called to immediately sync the state between all connected clients. So there is no equivalent of Parse's `save()` call**\r\n\r\n```javascript\r\n\r\n//Calling set() with one parameter sets the entire data\r\ngameScore.set({\r\n    player: {\r\n        firstname: 'Sergio',\r\n        lastname: 'Santos'\r\n    },\r\n    spouses: [ 'Kristen Santos' ]\r\n});\r\n\r\n//Calling set with a key or path and a value\r\ngameScore.set( 'result', '3-0' );\r\ngameScore.set( 'player.middleName', 'Jose'); //If a path doesn't exist, it willb e created\r\n```\r\n\r\n#### Listening for changes\r\ndeepstream records update in realtime. Any change made by any client is immediately synced across all connected clients. To listen for changes, use `subscribe()`\r\n\r\n```javascript\r\n//Using subscribe with just a callback function calls this function for any change\r\ngameScore.subscribe(function( data ){});\r\n\r\n//Using a path notifies you whenever that path's value has changed\r\ngameScore.subscribe( 'homeTeam', function( homeTeamScore ){});\r\n```\r\n\r\n#### Discarding and Deleting Objects\r\nDeepstream records have a `delete()` method which works similar to Parse's `destroy()`: It deletes the record from the database. Once the record is deleted, all connected clients will receive a `delete` event on their records.\r\n\r\nNot to be confused with `delete()` is `discard()`. Calling `discard()` frees up bandwidth by telling the deepstream server that you are no longer interested in updates for this record.\r\n\r\n#### Relational Data\r\ndeepstream let's you create trees of relational data by combining lists and records. Lists don't contain data themselves, but are observable collections of record names. As records can reference lists and lists can point to records this allows you to create and manipulate one-to-many and many-to-many relationships that are synced between all connected clients.\r\n\r\n#### Users, Roles & Permissions\r\ndeepstream is built to interface with other user management systems, e.g. databases, single-sign-on systems or active directories. As a result, it uses a functional style of authentication and permissioning, rather than Parse's configuration based approach.\r\n\r\nThe center of deepstream's permissioning is a `permission-handler`, a class that can be registered with your server and exposes three methods\r\n\r\n```javascript\r\nserver.set( 'permissionHandler', {\r\n    isValidUser: function( connectionData, authData, callback ) {\r\n        //authorize or reject incoming client connections here\r\n    },\r\n    canPerformAction: function( username, message, callback ) {\r\n        //permission individual operations here\r\n    },\r\n    onClientDisconnect: function( username ){\r\n        //optional clean-up once a client has disconnected\r\n    }\r\n});\r\n```\r\n\r\nEvery time a client establishes a new connection to the server, it needs to login:\r\n\r\n```javascript\r\nds.login({\r\n    user: 'LisaA',\r\n    password: 'sesame'\r\n}, function( success, errorCode, errorMessage ){\r\n    //...\r\n});\r\n```\r\n\r\nThere is a lot to be said about permissioning. To gain a good understanding, have a look at the [authentication](/tutorials/core/security-overview/#authentication) and [permissioning](/tutorials/core/permission-conf-simple/) tutorials.\r\n\r\n#### Where to go from here\r\nTo get a feel for how deepstream.io works and if it might be a good fit for your application's realtime backend, head over to the [getting started tutorial](/tutorials/core/getting-started-quickstart/).\r\n\r\nIf you have any questions, just raise them on [StackOverflow](//stackoverflow.com/questions/tagged/deepstream.io), join our [Slack Channel](//deepstream-slack.herokuapp.com/) or get in touch via Twitter ([@deepstreamIO](//twitter.com/deepstreamIO))."}
{"index":{"_index":"pages","_type":"blog","_id":"blog_publishing-aws-sns-messages-to-browsers-tutorial_readme.md"}}
{"filePath":"blog/publishing-aws-sns-messages-to-browsers-tutorial/readme.md","title":"Publishing AWS SNS messages to browsers","content":"\r\n\r\nSending the same message to Smartphones, Email inboxes and VoIP clients on an Amazon scale? Sounds great! And that’s exactly what AWS’ Simple Notification Service lets you do. It’s a realtime-pub-sub-hub that lets you fan out the same data to AWS services like SQS or Lambda as well as to a wide range of connected devices.\r\nThere’s one thing though it doesn’t do: send your message directly to web browsers. Here’s how to close this gap with a bit of deepstream.io.\r\n\r\n### The basic setup\r\nIn this tutorial, we’ll build a “sns-provider”, a node process that subscribes to a SNS topic and forwards its messages as deepstream events. For this to work, our provider needs to\r\n\r\n- Authenticate against AWS\r\n- Become a SNS HTTP endpoint\r\n- Subscribe to a SNS topic\r\n- Connect to deepstream\r\n- Forward incoming messages as deepstream events\r\n\r\nPhew, that sounds like a lot! But, it’s actually not that bad. Here’s what our setup will look like\r\n\r\n![Basic Setup](aws-sns-to-deepstream.png)\r\n\r\nIf you prefer diving right into the code, check out the [github repo](//github.com/deepstreamIO/ds-tutorial-aws-sns)\r\n\r\n### A bit of a disclaimer\r\nFor brevities sake, this tutorial skimps on security. AWS security is extremely powerful, but also complex enough to have [whole books written about it](//media.amazonwebservices.com/AWS_Security_Best_Practices.pdf). We’ll be using http instead of https, temporary security tokens (which is a good idea), but store them as plain text in our code (which is not) and a deepstream.io server that’s open to the world, instead of [properly permissioned](/tutorials/core/security-overview/#authentication).\r\nThis tutorial also assumes that you're already familiar with deepstream's basic concepts. If not, quickly head over to the [getting started](/tutorials/core/getting-started-quickstart/) tutorial - don't worry, I'll wait.\r\n\r\n### Let’s get started: Creating a topic.\r\nWe’ll start by creating a topic via the AWS console. Topics are the fundamental concept for message routing in a pub-sub architecture: Many subscribers listen to a topic and are notified whenever something publishes a message on that topic. Basically, it’s like JavaScript events, but you have to create a topic before you can subscribe.\r\nLet’s create a topic called “news” via the console:\r\n\r\n![Creating a SNS topic via the console](create-sns-topic.png)\r\n\r\n### Create the http server\r\nNext up, we need to start an http server. Why? The way SNS notifies your application of incoming messages is via HTTP.  For this to work, your application needs to:\r\n\r\n-   Start a HTTP server\r\n-   Subscribe to a topic and tell SNS which URL it should send updates to\r\n-   Receive a “do you really want to receive updates?” message on that URL\r\n-   Confirm that message\r\n\r\nAnd from there on, it receives all updates as incoming http requests. A few things to note about these requests:\r\n\r\n-   All incoming requests are POST requests\r\n-   The message data is JSON encoded\r\n-   Each message contains a \" x-amz-sns-message-type” header that tells you which kind of message it is (e.g. “SubscriptionConfirmation” or “Notification”)\r\n\r\nWith this in mind, here’s what our http server looks like\r\n\r\n```javascript\r\nfunction createHttpServer() {\r\n\tvar server = new http.Server();\r\n\tserver.on( 'request', function( request, response ){\r\n\t\trequest.setEncoding( 'utf8' );\r\n\r\n\t\t//concatenate POST data\r\n\t\tvar msgBody = '';\r\n\t\trequest.on( 'data', function( data ){\r\n\t\t\tmsgBody += data;\r\n\t\t});\r\n\t\trequest.on( 'end', function(){\r\n\t\t\tvar msgData = parseJSON( msgBody );\r\n\t\t\tvar msgType = request.headers[ 'x-amz-sns-message-type' ];\r\n\t\t\thandleIncomingMessage( msgType, msgData );\r\n\t\t});\r\n\r\n\t\t// SNS doesn't care about our response as long as it comes\r\n\t\t// with a HTTP statuscode of 200\r\n\t\tresponse.end( 'OK' );\r\n\t});\r\n\r\n\tserver.listen( 6001, subscribeToSnS );\r\n}\r\n```\r\n\r\nOh, and before I forget: Your http server needs to be accessible from the internet. If you're running it locally, you might want to use a tool like [ngrok](//ngrok.com/) to make this possible.\r\n\r\n### Subscribing to a topic\r\nUsing the [AWS-SDK for Node.js](//aws.amazon.com/sdk-for-node-js/), subscribing to a topic is as easy as\r\n\r\n```javascript\r\nvar AWS = require('aws-sdk');\r\nvar sns = new AWS.SNS();\r\n\r\nsns.subscribe({\r\n\tProtocol: 'http',\r\n\t//You don't just subscribe to \"news\", but the whole Amazon Resource Name (ARN)\r\n\tTopicArn: 'arn:aws:sns:eu-central-1:792569207202:news',\r\n\tEndpoint: '//your-endpoint-url.com'\r\n}, function( error, data ) {\r\n\tconsole.log( error || data );\r\n});\r\n```\r\n\r\nBut for this to work, we need to get two security steps out of the way first: Permission a user to use SNS and add its credentials to the Node SDK.\r\n\r\nHere's how to permission a user via the AWS console:\r\n\r\n![Attaching SNS user policy](attaching-SNS-policy.png)\r\n\r\nNext, we need to create a security token for this user. Using the [AWS command line tool](//aws.amazon.com/cli/) this is as simple as\r\n\r\n```text\r\naws sts get-session-token\r\n```\r\n\r\nThis will display a set of credentials that we can use to configure our SDK.\r\n\r\n```javascript\r\nvar AWS = require('aws-sdk');\r\nAWS.config.update({\r\n\t\"secretAccessKey\": \"...\",\r\n\t\"sessionToken\": \"...\",\r\n\t\"accessKeyId\": \"...\",\r\n\t\"region\": \"eu-central-1\"\r\n});\r\nvar sns = new AWS.SNS();\r\n```\r\n\r\nAs mentioned before, embedding your credentials into the code might be easy, but it isn't safe. Have a look at the [config documentation](//docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-configuring.html) to find a better way to store credentials.\r\n\r\n### Confirming your subscription\r\nGreat, now our call to `sns.subscribe` should work. A few milliseconds after we've made it, our http server should receive a message, asking it to confirm the subscription. To process it, it's time to fill in the `handleIncomingMessage( msgType, msgData );` method from the http-server snippet.\r\n\r\n```javascript\r\nfunction handleIncomingMessage( msgType, msgData ) {\r\n\tif( msgType === 'SubscriptionConfirmation') {\r\n\t\t//confirm the subscription.\r\n\t\tsns.confirmSubscription({\r\n\t\t\tToken: msgData.Token,\r\n\t\t\tTopicArn: msgData.TopicArn\r\n\t\t}, onAwsResponse );\r\n\t} else if ( msgType === 'Notification' ) {\r\n\t\t// That's where the actual messages will arrive\r\n\t} else {\r\n\t\tconsole.log( 'Unexpected message type ' + msgType );\r\n\t}\r\n}\r\n```\r\n\r\n### Starting a deepstream.io server\r\nNow it's time to start our deepstream server. Just create a separate js file (e.g. start-server.js) with\r\n\r\n```javascript\r\nvar DeepstreamServer = require( 'deepstream.io' ),\r\n\tserver = new DeepstreamServer();\r\n\r\nserver.start();\r\n```\r\n\r\nand run it\r\n\r\n![Deepstream.io startup](deepstream-startup.png)\r\n\r\nAs the console shows, the server is now listening for TCP connections (e.g. from a node client) on port `6021` and for browser connections on port `6020`. If you'd prefer different hosts/ports, please have a look at the [configuration docs](//deepstream.io/docs/deepstream.html).\r\n\r\nAll that's left now is to connect our SNS-provider to the deepstream server:\r\n\r\n```javascript\r\nvar deepstreamClient = require( 'deepstream.io-client-js' );\r\nvar ds = deepstreamClient( 'localhost:6021' ).login();\r\n```\r\n\r\n### Forwarding SNS messages as deepstream events\r\nOk, time for the last piece of the puzzle. Every time we receive a message from SNS, we want to forward it as a deepstream event. (Events are deepstream's pub-sub mechanism. They work exactly like a JavaScript event emitter, distributed across many clients).\r\n\r\nEach SNS message has a \"Subject\" and a \"Message\" - both of which are plain text. This leaves us with a couple of choices: We could use the topic as an event name, send JSON as the message body or come up with a totally different mechanism. But for now, let's keep things simple: Every time we receive a message, we'll use the Subject as an event-name and send the message content as event-data... or in code, inside our `handleIncomingMessage` method:\r\n\r\n```javascript\r\nelse if ( msgType === 'Notification' ) {\r\n\tds.event.emit( msgData.Subject, msgData.Message );\r\n}\r\n```\r\n\r\nSubscribing to this event from a browser is now as easy as\r\n\r\n```javascript\r\nds.event.subscribe( 'someEvent', function( data ){\r\n\t//...\r\n});\r\n```\r\n\r\nAnd that's it. Thanks for holding out with me for so long. If you'd like to learn more about events in deepstream, head over to the [Events & RPC tutorial](//deepstream.io/tutorials/events-and-rpcs.html)\r\nor have a look at the [event documentation](//deepstream.io/docs/client.event.html)\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_realtime-framework-overview_readme.md"}}
{"filePath":"blog/realtime-framework-overview/readme.md","title":"An overview of realtime libraries and frameworks","content":"\r\n\r\nTime to build a realtime app. Whether it's just a few notification popups or a fully synchronized collaboration app, you’re probably doing the right thing - users have come to expect to see changes happen when they happen - not just once the page is refreshed.\r\n\r\nBut as realtime functionality becomes the standard, the ecosystem of servers, libraries and frameworks that power it is growing massively - and it becomes increasingly harder to choose a good starting point.\r\n\r\nThis post tries to provide an overview of the types of technologies out there, what they do and which to choose. But, with the amount of choices available, we had to limit it a little bit, so this post focuses on technologies that are:\r\n\r\n- #### self hosted\r\nthere’s a lot of great services and PaaS’ out there that can help you build realtime apps, e.g. [Pusher](https://pusher.com/), [PubNub](https://www.pubnub.com/), [Firebase](https://www.firebase.com/), [realtime.co](https://framework.realtime.co/messaging/) or soon [deepstreamHub](https://deepstreamhub.com/). For this post we’ll purely concentrate on backends that you host yourself.\r\n\r\n- #### open source\r\nthere’s a number of proprietary realtime servers and frameworks, e.g. [Caplin Liberator](https://www.caplin.com/developer/component/liberator), [Kaazing](https://kaazing.com/), [Light Streamer](http://lightstreamer.com/) or [Migratory Data](http://migratorydata.com/) which are often used in banking and financial trading. This post purely focuses on open source and freely available libraries\r\n\r\n- #### support browsers and mobile clients\r\nthis post is about technologies that deliver and receive realtime data from browsers and mobile clients. That means we'll exclude libraries that help with TCP / UDP messaging, IoT data collection, multiplayer gaming or financial data processing etc.\r\n\r\nOh, and one more thing (makes Steve Jobs turnaround). You're reading this on the deepstream.io website. Deepstream.io is an open realtime server - and a pretty awesome one at that. (you can find it further down in the \"hybrid\" section). This shouldn't mean that there's any bias in this overview - if you feel that's not the case, we've missed something or got it wrong, [please let us know](mailto:info@deepstreamhub.com).\r\n\r\n### Websocket &amp; Messaging libraries\r\nIn order to build any kind of realtime functionality, you need to establish a bi-directional connection between client and server and send messages over it. This is the foundation for any higher level concept, but depending on your requirements, a low level websocket library might already be enough.\r\n\r\n#### But wait, can't I just use WebSockets directly?\r\nYes, you could. Pretty much all modern browsers, Android and iOS versions offer native Websocket support.\r\nBut no, you shouldn't! Websocket libraries do a few things that you will definitely want in production. They give you:\r\n\r\n- #### heartbeats / keep alive calls\r\nthese are tiny messages that are send on pre-defined intervals. They make sure that the other side is still responsive and prevent certain network constructs like proxies or firewalls from cutting your connection due to inactivity.\r\n\r\n- #### reconnecting\r\nif your connection drops, you want it to be re-established. Some libraries also queue messages while the connection is down and resend them once it becomes available again.\r\n\r\n- #### http fallback\r\nif WebSockets are not available, some libraries fall back to http long polling and other techniques to mimic bi-directional communication (collectively known as \"comet\"). Depending on your target audience this might not be a necessity, but is still a good choice if your app needs to be available within large corporate infrastructures.\r\n\r\n#### [engine.io](https://github.com/socketio/engine.io)\r\nEngine.io is the transport layer that powers socket.io, but can also be used as a standalone library. It incorporates a few unusual design choices, such as starting with http long-polling to ensure immediate connectivity and only upgrade the connection to websocket a bit later.\r\nIt's what we chose for browser communication in deepstream.io after extensive testing and comparing and have never regretted it since.\r\n\r\n#### [WS](https://github.com/websockets/ws)\r\nA solid, barebone WebSocket server for node. Used by engine.io\r\n\r\n#### [SockJS](https://github.com/sockjs/sockjs-client)\r\nA fast JavaScript / Node.js abstraction layer for websockets, supporting a number of fallback techniques\r\n\r\n#### [Primus](https://github.com/primus/primus)\r\nIf commitment is not for you, Primus is. Its not a connectivity library in itself, but an abstraction layer that allows you to switch your connectivity library once the initial romance has worn off.\r\n\r\n#### [Tornado](http://www.tornadoweb.org/)\r\nTornado is a general purpose networking library for Python, offering Websocket abstractions and fallbacks.\r\n\r\n#### [web-socket-js](https://github.com/gimite/web-socket-js)\r\nFlash based websocket implementation\r\n\r\n#### [libwebsocket](https://libwebsockets.org/)\r\nReally fast websocket implementation in low level C.\r\n\r\n#### [Fleck](https://github.com/statianzo/Fleck)\r\nFleck is a WebSocket implementation in C#\r\n\r\n#### [Atmosphere](https://github.com/Atmosphere/atmosphere)\r\nAtmosphere is a general websocket abstraction layer for JVM compatible languages. It's focused to run on Java and J2EE application servers and comes with an ecosystem of extension points to connect it to caches like Redis or Hazelcast.\r\n\r\n#### [Java Web Socket](http://java-websocket.org/)\r\nExactly what the name suggests\r\n\r\n#### [Mojolicious](http://mojolicious.org/)\r\nA websocket implementation with fallbacks for Perl\r\n\r\n#### [SignalR](http://signalr.net/)\r\nWebSocket abstraction for .NET\r\n\r\n#### [SuperWebSocket](http://superwebsocket.codeplex.com/)\r\nTCP/Websocket library for .NET\r\n\r\n#### [Plezy](http://www.plezi.io/)\r\nWebSocket abstraction for Ruby\r\n\r\n### Publish / Subscribe libraries\r\nPub/Sub is a simple, widely adopted pattern for realtime messaging. If you've ever used an event-emitter, you already know the concept. Many clients subscribe to a topic (sometimes also called an event, namespace or channel) and other clients publish messages on that topic. This is a simple and scalable pattern for many-to-many communication.\r\n\r\n#### [socket.io](http://socket.io/)\r\nDefinitely the most popular entry in this list. Socket.io is a Node.js library that comes with clients in many languages. It offers direct messaging and pub-sub based on rooms (think chat-rooms) with the ability to distribute load across multiple nodes using a Redis server in the background.\r\n\r\n#### [socket cluster](http://socketcluster.io/)\r\nA pub-sub framework built in Node.js that focuses on load-balancing connections across a larger cluster of processes.\r\n\r\n#### [Faye](http://faye.jcoglan.com/)\r\nOne of the first pub/sub systems around. Faye offers servers in both Ruby and Node and implements the Bayeux Protocol, designed 2007 by the Dojo foundation. Similar to socket.io, Faye relies on Redis pub/sub to scale across multiple nodes. It does however lack support for Redis clusters which limits its scalability to a single Redis instance.\r\n\r\n#### [Firehose.io](http://firehose.io/)\r\nA simple, Ruby based pub-sub system with integration points for Backbone and Ember.\r\n\r\n#### [Centrifugo](https://github.com/centrifugal/centrifugo)\r\nA backend agnostic pub-sub system written in Go\r\n\r\n#### [Phoenix Framework](http://www.phoenixframework.org/)\r\nPhoenix is a whole web-app framework for Erlang, but with a focus on pub/sub via channels.\r\n\r\n### Data-Sync\r\nPub/Sub has been the default pattern for realtime apps since day one and is still the best choice for many usecases. But one of its most widespread uses in realtime apps is actually pretty terrible - it goes something like this:\r\n\r\nUser A changes some data and hits save. The data is send via ajax to a server and stored in a database. The client now publishes an event, e.g. `data-update/<some-id>`. Other clients receive this event, make an ajax request to the server and download the updated data.\r\n\r\nSounds familiar? Don't worry, this has been the default solution for many companies and years. But now we've come up with something better: data-sync.\r\n\r\nData-sync models your data as observable objects or tree-structures that can be manipulated by clients and servers. Any change to the data is immediately  synchronised across all connected clients.\r\n\r\nThis significantly reduces request-overhead, makes applications faster and development simpler.\r\n\r\n#### [Horizon](https://horizon.io/)\r\nThe youngest, but certainly one of the most promising entries in this list is RethinkDB’s Horizon - a Node.js server and associated JavaScript client library that sits on top of RethinkDB and exposes its database and realtime querying capabilities to browser apps.\r\n\r\nHorizon supports authentication via openAuth and JSON webtoken and access control via TOML configured permissions for user groups\r\n\r\n#### [FeatherJS](http://feathersjs.com/)\r\nFeatherJS is a RESTful CRUD framework for Node build on top of express.js that sends out events via a socket.io connection whenever an object is created, updated or deleted, allowing clients to react accordingly.\r\n\r\n### Remote Procedure Call (RPC) / Remote Method Invocation (RMI)\r\nThis one is not strictly a realtime concept, but more the classical REST/HTTP request/response workflow, mapped onto a bi-directional connection. RPC allows you to call methods on a remote server and process their response. This can be a powerful concept in itself, but it becomes even more powerful when used in conjunction with other realtime concepts like pub/sub or data-sync... which brings us to the next section:\r\n\r\n### Hybrids\r\nMost production use cases benefit from a combination of realtime concepts. In a voting system you might want to send a user’s vote as a remote method invocation - which increment the number of votes on a data-sync object as well as block the voting user from voting more than once. For multiplayer games it can make sense to rely on simple pub-sub messaging to update player positions whilst using data-sync to keep track of scores.\r\n\r\nAs a result, many of the most popular realtime frameworks are hybrids that offer multiple, interacting realtime concepts to both clients and servers.\r\n\r\n\r\n#### [Deepstream.io](https://deepstream.io/)\r\nDeepstream.io is a realtime server that combines data-sync with pub/sub and request response. It’s un-opinionated and offers an open ecosystem of connectors for databases, caches and messaging systems. Its unobtrusive client plays well with any M*C framework and comes with plugins for React, Polymer and other libraries.\r\n\r\nDeepstream focuses on ease of use and speed. With an average below [1ms for clusters under load]( https://deepstream.io/info/performance-single-node-vs-cluster.html) it’s a good fit even for demanding use cases like multiplayer gaming or financial trading.\r\n\r\n\r\n#### [Meteor](https://www.meteor.com/)\r\nMeteor is a full stack framework for realtime apps, featuring data-sync, pub/sub, RPCs, realtime search and a host of other features.\r\nIt offers a highly structured and opinionated way to set up and connect servers and databases, build clients and test and deploy applications. It comes with its own, self-contained ecosystem, including a proprietary (potentially soon to be deprecated?) [package manager](https://atmospherejs.com/) and originally its own client side templating library.\r\n\r\nIt’s probably fair to say that Meteor is one of the most polarizing entries in the web technology space. Meteor fans praise it for its ease of use, the development time it saves by offering a well-orchestrated stack of components and its high levels of resilience, including an in-browser data-store and robust syncing routines.\r\n\r\nIts critics frequently mention Meteor’s inflexibility, its inability to swap critical components (e.g. to use it with a database other than MongoDB), slow speed and lack of transparency of what happens behind the scenes.\r\nWith over 30M in Venture Funding and extensive Facebook campaigns, Meteor is also one of the few commercial offerings in this space.\r\n\r\nAt the bottom line, Meteor comes down to the usual trade-off presented by full stack frameworks: If your use case is a good fit for Meteor, it will save you an awful lot of development time and headaches. If you seek flexibility, you're most likely better of with a combination of smaller, more specialized components\r\n\r\n#### [Kuzzle.io](http://kuzzle.io/)\r\nA new entry to this list and still in beta, Kuzzle is an interesting new contender for the Meteor crown. It's a Node.js server that connects to ElasticSearch and Redis and offers data-sync, pub-sub and realtime querying via a number of protocols such as WebSocket, REST, AMQP, MQTT and STOMP.\r\n\r\n#### [Autobahn](http://autobahn.ws/)\r\nAutobahn offers Pub/Sub and RPC. It's a server implementing the WAMP (Web Application Messaging Protocol) spec and offers a selection of clients for JS, Python, Android and C++.\r\n\r\n#### [CometD](https://cometd.org/)\r\nCometD has been one of the first entries in the realtime space. It combines pub/sub with rpcs and supports browser connectivity via a stack of comet techniques.\r\n\r\n### Other resources and further reading\r\nFor further overviews, have a look at the [great blog](http://www.leggetter.co.uk/) of our former colleague Phil Leggetter, especially his [realtime web technologies guide](http://www.leggetter.co.uk/real-time-web-technologies-guide/)\r\n\r\n\r\n\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_realtime-microservices_readme.md"}}
{"filePath":"blog/realtime-microservices/readme.md","title":"Deepstream Microservices","content":"\r\n\r\nNot too long ago, this post would have started with a flaming manifesto for microservices. “Break down the monolith!”, “Divide and Conquer!”… you know the drill. Today though, that’s hardly necessary. In fact, cloud platforms like AWS make it next to impossible NOT to architect your backend as a swarm of loosely coupled processes.\r\n\r\nSo… no more passionate manifestos? Well, maybe one more: Make them realtime!\r\n\r\n### Realtime?\r\nMost microservices interact in one of two ways: Via HTTP communication or via message queues. With deepstream, we’d like to suggest a third: Distributed state.\r\n\r\nAt its core, deepstream is a container for distributed states which can be observed and manipulated by frontend clients and backend services alike. This makes for a potent recipe for realtime apps.\r\n\r\nLet's look at a couple of real world usecases to see what this would look like.\r\n\r\n<h3 class=\"section\">The simplest usecase - no custom backend at all.</h3>\r\n![deepstream ToDo MVC](deepstream-todomvc.gif)\r\n\r\nLet's start with a simple todo list. Syncing your todos between clients and storing them in a database takes no custom backend code whatsoever. All you need is a deepstream server that your clients connect to and optionally a database or cache.\r\n\r\n![Most basic deepstream setup](diagramm-simple-setup.png)\r\n\r\n<h3 class=\"section\">Connecting to a third party API</h3>\r\n\r\nFor many SaaS applications, extensibility and an active developer community is key to success. Slack is a perfect example for this: it offers both outgoing and incoming \"webhooks\" - http endpoints that will be called when messages are to be sent or received.\r\n\r\n![deepstream Slack integration](deepstream-slack-integration.gif)\r\n\r\nTo connect deepstream to Slack, we’d need to create a “provider” - a small process that sits between both endpoints and forwards messages in either direction.\r\n\r\n![deepstream Slack integration diagram](diagramm-deepstream-slack-integration.png)\r\n\r\nOur Slack provider connects as a client to deepstream on one side and spins up a HTTP server on the other. Whenever an entry is added to a todo-list, the provider gets notified and sends an HTTP request to Slack. Whenever a trigger-word is mentioned on Slack, it receives an HTTP request and stores its payload as a deepstream record.\r\n\r\nPlease find a simple implementation of the deepstream-slack connector used in the example above [here](slack-connector.js)\r\n\r\n<h3 class=\"section\">Connecting to a database</h3>\r\n\r\n![realtime search with deepstream](deepstream-realtime-search.gif)\r\n\r\nOften it makes sense to derive enhanced data from deepstream's database, for instance for aggregated values (Sums, Averages) or extended query functionality.\r\n\r\ndeepstream offers a number of out of the box providers for common requirements. To achieve realtime search in your document (as in the example above), just get the latest [rethinkdb search provider from the downloads page](https://deepstream.io/download/) and plug it in between your server and your database.\r\n\r\n![database search provider](diagramm-database.png)\r\n\r\n<h3 class=\"section\">But how does this scale?</h3>\r\n\r\nVery well indeed. Deepstream is built to scale horizontally via a message bus and performs well as a messaging and data backbone for realtime applications.\r\n\r\nSince state is synced between all connected servers it doesn't matter which endpoint your microservices connect to. On top of that, deepstream has a number of built-in features that help help scale backends more easily, e.g. via load balancing remote procedure calls between multiple redundant providers.\r\n\r\n![ deepstream microservice scalability](diagramm-scalability.png)\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_the-unsexy-iot_readme.md"}}
{"filePath":"blog/the-unsexy-iot/readme.md","title":"The unsexy IoT","content":"\r\n\r\nA few years ago, when the first Rasperry Pi’s hit the scene, I was working at a tech company building trading apps. Like so many, the company had a strong focus on engineering with about 70 developers pitted against just 4 designers.\r\n\r\nOne day, a package arrived. Naturally, the man that came to pick it up had a beard - but a well groomed one. He - of course - wore a checkered shirt - but I couldn’t help notice that it was not only ironed, but also matched his shoes. And yes, he carried a laptop everywhere. But the laptop he carried was a Mac and the bag he carried it in was vintage leather. In short, he was a designer - and the package contained the first Raspberry PI I’ve ever seen.\r\n\r\nSurprisingly few of the techies cared about this strange machine at first, but the UX team got excited straight away, started playing with it and argued about its cool new use cases.\r\n\r\n#### How come?\r\nRaspberry PIs are part of a new generation of clever IoT devices: Arduinos, Nest Thermostats or Philip’s remote controllable light bulbs - easily accessible, well designed and sleek (ok, maybe not Arduinos).\r\nThey appeal to a wide audience and spark the imagination of users, founders and investors alike. And most importantly, they excite us about what's to come: smart homes, self driving cars and connected everything - in short: the sexy IoT.\r\n\r\nBut, the Internet of Things isn’t new. At all. And while more than five million Rasperry PIs or a potentially even bigger number of Arduino copies sold around the world make for impressive numbers, they are severely dwarfed by the other IoT. The older IoT. The unsexy one.\r\n\r\n<div class=\"img-box\">\r\n    <img src=\"thermostats.png\" alt=\"Thermostats\" />\r\n    <label>Nest Thermostat and Siemens Climatix 600 Controller</label>\r\n</div>\r\n\r\n#### The other IoT\r\nMany of the IoT’s core use cases: Building and home automation, environmental monitoring or infrastructure management not only precede the current generation of devices, they precede the World Wide Web itself.\r\nBACnet, still one of the most widely used building automation protocols was developed in 1987, LonTalk in 1988 and X10, a granddaddy of modern smart home protocols like ZigBee or Z-Wave goes back as far as 1975.\r\n\r\nSo, how come the IoT only recently became the “next big thing”?\r\nI would argue that the IoT has always been big, it’s the personalized IoT that has taken the spotlight lately. This is fueled by the alignment of a number of trends\r\n\r\n- Hardware production costs dropped significantly, making the IoT cheaper and facilitating micro-appliances like Amazon’s Dash button or Roost’s smart batteries\r\n\r\n- We have become more comfortable with sharing personal data, e.g. when it comes to incorporating wearables or fitness trackers into our daily routine.\r\n\r\n- Producing faster microprocessors more cheaply enables IoT devices to run high level programming languages like Node that have a wider degree of adoption\r\n\r\n#### But it’s not like the old IoT has stagnated either\r\n<br />\r\n\r\n\r\n- It is now almost completely based on open protocols, rather than the proprietary ones of the past that were used to tie customers to a specific vendor.\r\n\r\n- IPv6 has massively increased the number of available IPs, allowing every sensor and relay within a system to be addressable from the public internet\r\n\r\n- UX and Usability is playing an increasing role in the design of Management Systems. (It’s still a long way away from e.g. Nest’s products, but there is a clear trend)\r\n\r\n#### Crossing the chasm\r\nHowever, at the moment, there is still a huge chasm dividing both IoT’s. The reasons for this are complex, but amongst them are the differences in culture, technology, agility and enterprise-suitability that distinguish the companies in both spaces.\r\n\r\nThere is, however, enormous opportunity in crossing this chasm. The new IoT provides valuable data about its users, their habits, daily routines and preferences. The old IoT generates detailed information about the environment they interact in. Combining both with big data crunching technology will lead to unprecedented (and slightly scary) insights into individual user’s behavioral patterns as well as into the systems they compose.\r\n\r\n#### What does it take to unite both IoTs?\r\nThere are a number of possible integration and normalisation approaches for aggregating both IoTs.\r\nFor [deepstreamHub](//deepstreamhub.com/), our upcoming realtime data platform we use a concept of channels: pre-built integrations with third party services that allow you to directly stream realtime data, say Slack Messages or Stock Prices into your app.\r\nBut channels are bidirectional and could as well be used to control building management systems, process automation or even integrated healthcare. As long as you have a TCP endpoint, all it takes is to point your deepstreamHub channel towards it and its data and commands become available as a high level API within all connected browsers and backend processes.\r\n\r\nWe’ve only just started looking into the possibilities of interfacing with the old IoT. If you’ve got experience in the space and feel that this is either an absolutely superb or completely terrible idea: we’re very eager to hear your thoughts! ([@deepstreamIO](//twitter.com/deepstreamIO))\r\n\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_using-passportjs_readme.md"}}
{"filePath":"blog/using-passportjs/readme.md","title":"Using PassportJS for Facebook and Twitter oAuth","content":"\r\n\r\nLet's face it, we each have multiple user profiles and maintaining all those different passwords just never worked. So when OAuth got introduced we let out a sigh of relief knowing those days are behind us. Behold, the second sigh, where deepstream can use the same OAuth-tokens to authenticate your users!\r\n\r\nIn this blog we'll be looking at using the power of [express](//expressjs.com/), [PassportJS](//PassportJS.org/) and a nifty [plugin](//github.com/demux/use-express-middleware) by [@arnary](//twitter.com/arnary) to allow us to automatically login to deepstream using Facebook or Twitter.\r\n\r\n### What is PassportJS?\r\nBefore we get started it's certainly worth looking at [PassportJS](//PassportJS.org/). It's an authentication middleware for Node.js that currently supports over 300 login-strategies, including Facebook, Twitter and Github.\r\n\r\n### Why use it with deepstream?\r\ndeepstream has a powerful permission-handler which can allow you to implement any type of user validation. By combining it with PassportJS we can authenticate users by checking if their session token is valid and determine if they can login accordingly.\r\n\r\n### How can it be done?\r\nI would recommend looking at the express/PassportJS Facebook [example](//github.com/passport/express-4.x-facebook-example/blob/master/server.js) to get a feel for how it works, but the basic idea is pretty simple. Let's take Facebook as an example:\r\n\r\n1. Load your application\r\n2. Request to login via Facebook\r\n3. Login to Facebook ( if not already logged in )\r\n4. Connect to deepstream\r\n\r\nLet's take a look at how the three different parts are implemented.\r\n\r\n### PassportJS\r\nPassportJS needs to be initialized and the desired strategies added.\r\n\r\n```javascript\r\nvar passport = require( 'passport' );\r\nvar passportFacebook = require( 'passport-facebook' );\r\n\r\nvar initializedPassport = passport.initialize();\r\nvar passportSession = passport.session();\r\n\r\npassport.serializeUser( function( user, cb ) {\r\n\t//nothing to do here as we use the username as it is\r\n\tcb( null, user );\r\n} );\r\n\r\npassport.deserializeUser( function( obj, cb ) {\r\n\t//again, we just pass the username forward\r\n\tcb( null, obj );\r\n} );\r\n\r\npassport.use( new passportFacebook( {\r\n\t\tclientID: FACEBOOK_ID,\r\n\t\tclientSecret: FACEBOOK_SECRET,\r\n\t\tcallbackURL: '//localhost/login/facebook/return'\r\n\t},\r\n\t( accessToken, refreshToken, profile, cb ) => {\r\n\t\treturn cb( null, profile );\r\n}));\r\n```\r\n\r\n### Express\r\nExpress is required by PassportJS to handle the authentication callbacks we expect from oAuth providers as well as to forward us to the correct authentication provider if we don't already have a session.\r\n\r\n```javascript\r\nvar http = require( 'http' );\r\nvar express = require( 'express' );\r\nvar expressSession = require( 'express-session' );\r\n\r\nvar app = express();\r\nvar httpServer = http.createServer(app);\r\nvar session = expressSession({\r\n\tsecret: '60dd06aa-cf8e-4cf8-8925-6de720015ebf',\r\n\tresave: false,\r\n\tsaveUninitialized: false,\r\n\tname: 'sid'\r\n});\r\n\r\napp.use( express.static( '../client' ) );\r\napp.use( session );\r\napp.use( initializedPassport );\r\napp.use( passportSession );\r\n\r\napp.get( '/login/facebook', passport.authenticate('facebook') );\r\n\r\napp.get( '/login/facebook/return', \r\n\t\tpassport.authenticate('facebook', { failureRedirect: '/login' }),\r\n\t\t( req, res ) => {\r\n\t\t\t\tres.redirect('/');\r\n} );\r\n```\r\n\r\n### Deepstream\r\nDeepstream can use the same middleware intialised previously to check whether or not the client attempting to login has a session, and allow or deny access accordingly.\r\n\r\n```javascript\r\nvar middleware = [ session, initializedPassport, passportSession ];\r\nvar Deepstream = require( 'deepstream.io' );\r\nvar useExpressMiddleware = require( 'use-express-middleware' );\r\nvar PermissionHandler = require( './permission-handler' );\r\n\r\nvar deepstream = new Deepstream();\r\ndeepstream.set( 'urlPath', '/deepstream' );\r\ndeepstream.set( 'httpServer', httpServer );\r\ndeepstream.set( 'permissionHandler', {\r\n\tisValidUser: function( connectionData, authData, callback ) {\r\n\t\tuseExpressMiddleware( connectionData.headers, middleware, ( req, res ) => {\r\n\t\t\tif( req.user ) {\r\n\t\t\t\tcallback( null, req.user.id );\r\n\t\t\t} else {\r\n\t\t\t\tcallback( 'Login Denied' );\r\n\t\t\t}\r\n\t\t\t} );\r\n\t},\r\n\tcanPerformAction: function( id, message, callback ) {\r\n\t\tcallback( null, true );\r\n\t}\r\n});\r\ndeepstream.start();\r\n\r\nhttpServer.listen( config.HTTP_PORT, function() {\r\n\tconsole.log( 'HTTP server listening on', process.env.HTTP_PORT );\r\n} );\r\n```\r\n\r\nAnd that's it! PassportJS and Express take care of actually allowing the user to create and store a session, and deepstream uses its PermissionHandler to authenticate connections with valid session ids.\r\n\r\nYou can see a working application using both Facebook and Twitter here. You have to setup a [Facebook](//developers.facebook.com/docs/apps/register) and [Twitter](//apps.twitter.com/) application to get your application key and secret.\r\n\r\n<img src=\"passportjs.gif\" alt=\"PassportJS with Facebook\" />\r\n\r\n<a class=\"mega\" href=\"//github.com/deepstreamIO/ds-tutorial-passport-auth\"><i class=\"fa fa-github\"></i>PassportJS with Facebook and Twitter</a>\r\n"}
{"index":{"_index":"pages","_type":"blog","_id":"blog_what-is-realtime_readme.md"}}
{"filePath":"blog/what-is-realtime/readme.md","title":"What is realtime?","content":"\r\n\r\nAs the guys behind deepstream.io we spent an awful lot of time at conferences and meetups with cool names like up.front, 12min.me or HackNTell. And we talk to an awful lot of people. What do we talk about? You’ve guessed it: Realtime technologies.\r\n\r\nBut every couple of conferences, we run into the same statement. Usually made by someone who’s very clever, very technical and feels strongly about a lot of things. The statement is:\r\n\r\n<blockquote>“There’s no such thing as realtime!”</blockquote>\r\n\r\n Really? Is there not? Let’s look into this.\r\n\r\n### The philosophical answer\r\nOk, philosophically they’re right. There is no such thing as realtime. Just as there is no such thing as “presence” or “now”. Every information we receive is delayed. Every ray of light that reaches our eyes has bounced of its source a few nanoseconds before, every soundwave that penetrates our ears needed to travel there first. And even once the information is received by our senses, it needs to be processed by our brain. So when exactly is “now” in this equation? Stretched out over numerous transmission, relaying and processing steps.\r\n\r\n### The technical answer\r\nBut the sort of realtime we’re talking about is not a philosophical concept, it’s a technical one. And in technology, there is very much such a thing as realtime. Here’s a good definition:\r\n\r\n<blockquote>“We speak of ‘realtime’ when information triggered<br /> by an event is delivered in a timeframe in which said information is still relevant”</blockquote>\r\n\r\nAh, now that makes more sense. The key concepts here are “events”, something that creates information, “delivery”, the act of getting the information to where it’s needed and “deadlines”, a point in time after which said information becomes useless.\r\n\r\nThe last concept is the most challenging one as different kinds of information come with very different deadlines. You might have noticed that Twitter, Facebook and LinkedIn made their walls realtime - they update every once a while when someone publishes a new post. There is however not much urgency in getting this information to you. Your colleague’s career move or your friend’s birthday pictures will still be as interesting to you in ten minutes as they are now.\r\n\r\nOther realtime use-cases however require a more speedy delivery. Seeing your Uber-driver approach on the map needs to happen within just seconds. And if your friend’s Google-Docs changes aren’t synced with your document quickly, things get messy very soon.\r\n\r\n<div class=\"img-box\">\r\n    <img src=\"the-merchant.png\" alt=\"Trading Platform\" />\r\n    <label>Realtime Trading has some of the most demanding latency requirements</label>\r\n</div>\r\n\r\nBut then there are also the classical fields of proper, adult, no-nonsense realtime: Finance and IoT. In Markets where a lot of people are trading a tiny set of things, e.g. Foreign Exchange, prices update many times a second. And there’s only ever one valid price at a time. So the moment the next price becomes available, the last price becomes meaningless - and so does your realtime system if it fails to keep up.\r\n\r\nRealtime reporting for the Internet of Things might not be quite as fast, but the consequences of missing an update or loosing a message can be all that much graver. If you’re running a storage facility for flammable gases, stored in high pressure tanks, you want your sensor’s pressure readings delivered without delay. Always.\r\n\r\n<div class=\"img-box\">\r\n    <img src=\"gastanks.jpg\" alt=\"IoT\" />\r\n    <label>The industrial IoT is one of the earliest realtime usecases</label>\r\n</div>\r\n\r\n### On to the “realtime web”\r\nContinuously sending event-streams over persistent connections and processing them as they happen has been done in multiplayer-gaming, manufacturing and finance for many years, but it has also increasingly found its way into apps, browser-software and websites\r\n.\r\nThe techies amongst you might now be thinking “Ah, Websockets!”. And you are right. WebSockets provide a proper and standardized method of bidirectional communication between browser and server - meaning both can send messages to each other whenever they choose to. But the same has been possible for quite a while, utilizing a series of Hacks known as “Comet”. Comet allowed you to achieve WebSocket-like behaviour - albeit in a clunky and error prone fashion.\r\n\r\nSo, how come the realtime web is only recently gaining so much traction?\r\nBecause there’s more to it than bi-directional communication. Much more.\r\n\r\nChanging expectations for instance: Wouldn’t it feel strange if gmail made you hit a refresh button to find out if you have new mails? If you had to reload the page every time to see if your Trello board has updated?\r\nYet the Amazon shopping basket in my browser remains at a stoic zero, even though I’ve been piling items into it using the mobile app. That would have been perfectly normal just years ago, but now it starts to feel almost broken…\r\n\r\nWhat we’re experiencing is the natural progression of Technology: It becomes more lifelike. Just as movies started in black and white, added sound, color, 3D (and hopefully soon actual 3D) the world wide web started as a humble document management system, added forms, page reloads, Ajax and as a next step realtime communication."}
{"index":{"_index":"pages","_type":"install","_id":"install_aws-linux_readme.md"}}
{"filePath":"install/aws-linux/readme.md","title":"Deploying on AWS","description":"Learn how to deploy deepstream on AWS Linux AMI","content":"\r\nAmazon offers its own flavour of Linux on its EC2 machines. You can install deepstream on AWS Linux using the YUM package manager. Just copy and paste the following script into your terminal.\r\n\r\n```bash\r\nsudo wget https://bintray.com/deepstreamio/rpm/rpm -O /etc/yum.repos.d/bintray-deepstreamio-rpm.repo\r\nsudo yum install -y deepstream.io\r\n```\r\n\r\n## Starting deepstream\r\nAfter installing, use the deepstream command to start the server via its [command line interface](/docs/server/command-line-interface/).\r\n```bash\r\ndeepstream start\r\n```\r\n\r\n![Starting deepstream on linux](../linux-start.png)\r\n\r\n### Configuring deepstream\r\nYou can either change deepstream's [configuration file](/docs/server/configuration/) directly in `/etc/deepstream` or create a copy and run deepstream with the `-c` flag. (Important, make sure to update all relative paths within the configuration after copying it).\r\n\r\n```bash\r\n$ cd ~\r\n$ cp /etc/deepstream/* .\r\n$ ls\r\nconfig.yml  permissions.yml  users.yml\r\n$ deepstream start -c config.yml\r\n```\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_centos_readme.md"}}
{"filePath":"install/centos/readme.md","title":"Installing on CentOS","description":"Learn how to install deepstream on CentOS","content":"\r\n\r\nDeepstream is available via the YUM package manager. Just add the repository to your repositories file, and install deepstream using the following script.\r\n\r\n```bash\r\nsudo wget https://bintray.com/deepstreamio/rpm/rpm -O /etc/yum.repos.d/bintray-deepstreamio-rpm.repo\r\nsudo yum install -y deepstream.io\r\n```\r\n\r\n## Starting deepstream\r\nAfter installing, use the deepstream command to start the server via its [command line interface](/docs/server/command-line-interface/).\r\n```bash\r\ndeepstream start\r\n```\r\n\r\n![Starting deepstream on linux](../linux-start.png)\r\n\r\n### Configuring deepstream\r\nYou can either change deepstream's [configuration file](/docs/server/configuration/) directly in `/etc/deepstream` or create a copy and run deepstream with the `-c` flag. (Important, make sure to update all relative paths within the configuration after copying it).\r\n\r\n```bash\r\n$ cd ~\r\n$ cp /etc/deepstream/* .\r\n$ ls\r\nconfig.yml  permissions.yml  users.yml\r\n$ deepstream start -c config.yml\r\n```\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_debian_readme.md"}}
{"filePath":"install/debian/readme.md","title":"Installing on Debian","description":"Installing Deepstream on Debian Linux","content":"\r\n\r\nDeepstream is available via the APT package manager and currently supports Debian `jessie`.\r\n\r\nTo install, paste the following script into your terminal\r\n\r\n```bash\r\n# the source command makes the distro_name available as a variable\r\n# the echo command creates a source list entry string for the deepstream repo\r\n# the tee command appends it to APT's list of package sources\r\necho \"deb http://dl.bintray.com/deepstreamio/deb jessie main\" | sudo tee -a /etc/apt/sources.list\r\n\r\n# downloads the key the distribution is signed with\r\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 379CE192D401AB61\r\n\r\n# updates APT's registry with the newly available packages\r\nsudo apt-get update\r\n\r\n# installs deepstream. -y skips \"are you sure?\" question\r\nsudo apt-get install -y deepstream.io\r\n```\r\n\r\nResulting in\r\n\r\n![installing deepstream on debian](./debian-install.png)\r\n\r\n## Starting deepstream\r\ndeepstream can be started via its [command line interface](/docs/server/command-line-interface/).\r\n```bash\r\ndeepstream start\r\n```\r\n\r\n![Starting deepstream on linux](../linux-start.png)\r\n\r\n### Configuring deepstream\r\nYou can either change deepstream's [configuration file](../../docs/server/configuration/) directly in `/etc/deepstream` or create a copy and run deepstream with the `-c` flag. (Important, make sure to update all relative paths within the configuration after copying it).\r\n\r\n```bash\r\n$ cd ~\r\n$ cp /etc/deepstream/* .\r\n$ ls\r\nconfig.yml  permissions.yml  users.yml\r\n$ deepstream start -c config.yml\r\n```\r\n\r\n### Downloading deepstream's source code\r\nYou can also get the package and sources directly from [deepstream's release page](https://github.com/deepstreamIO/deepstream.io/releases)"}
{"index":{"_index":"pages","_type":"install","_id":"install_docker-compose_readme.md"}}
{"filePath":"install/docker-compose/readme.md","title":"Docker Compose","description":"Run deepstream together with a RethinkDB storage provider, a Redis cache provider and a RethinkDB search provider which allows to subscribe to realtime queries.","content":"\r\n\r\nGet more details about Docker in general and about the deepstream standalone Docker image in the [Docker&nbsp;image&nbsp;tutorial](../docker-image/)\r\n\r\n### What is Docker Compose?\r\n\r\n[Compose](https://docs.docker.com/compose/) is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file to configure your application's services. Then, using a single command, you create and start all the services from your configuration.\r\n\r\n### How to use Docker compose with deepstream?\r\n\r\nDocker compose consumes a YAML file which contains a description of the containers and how there are linked.\r\nSetup it looks like this:\r\n\r\n_docker-docker.yml_:\r\n\r\n```yaml\r\nversion: '2'\r\nservices:\r\n    deepstream:\r\n        image: deepstreamio/deepstream.io\r\n        ports:\r\n            - \"6020:6020\"\r\n            - \"6021:6021\"\r\n        volumes:\r\n            - ./conf:/usr/local/deepstream/conf\r\n            - ./var:/usr/local/deepstream/var\r\n        depends_on:\r\n            - redis\r\n            - rethinkdb\r\n    deepstream-search-provider:\r\n        image: deepstreamio/deepstream.io-provider-search-rethinkdb\r\n        environment:\r\n            - DEEPSTREAM_HOST=deepstream\r\n            - DEEPSTREAM_PORT=6021\r\n            - RETHINKDB_HOST=rethinkdb\r\n        depends_on:\r\n            - deepstream\r\n    redis:\r\n        image: redis:alpine\r\n        ports:\r\n            - \"6379:6379\"\r\n    rethinkdb:\r\n        image: rethinkdb\r\n        ports:\r\n            - \"28015:28015\"\r\n            - \"8080:8080\"\r\n        volumes:\r\n            - ./rethinkdb_data:/data/rethinkdb_data\r\n```\r\n\r\nThe compose file defines four services:\r\n\r\n- __deepstream__ - the deepstream server\r\n- __deepstream-search-provider__ - a deepstream client which handles the realtime queries of RethinkDB\r\n- __redis__ - Redis server which is used as both cache and message layer for deepstream\r\n- __rethinkdb__ - RethinkDB as the storage layer for deepstream\r\n\r\nThe services are accessible to each other via the service name as the hostname. We will need this value in\r\nthe deepstream configuration later.\r\n\r\nExplanation of properties of a service:\r\n  - __image__ URL of the image from the Docker Hub registry\r\n  - __ports__ port forwarding of the exposed ports on the container to the port on the host machine\r\n  - __volumes__ mount and share the directory from the host machine to the container\r\n  - __depends_on__ defines a dependency which affects the starting order of the containers\r\n  - __environment__ set environment variables in the container\r\n\r\n\r\nBy default deepstream will not use any connectors for the storage, cache and message layer.\r\nThis means you need to provide a configuration file and set the options for the connectors.\r\nThe easiest way to do this is by copying all the files from the [conf/](https://github.com/deepstreamIO/deepstream.io/tree/master/conf) directory into your _conf_ directory next to the compose file.\r\n\r\nThen change the plugins section to:\r\n\r\n```yaml\r\nplugins:\r\n  message:\r\n    name: redis\r\n    options:\r\n      host: redis\r\n      port: 6379\r\n  cache:\r\n    name: redis\r\n    options:\r\n      host: redis\r\n      port: 6379\r\n  storage:\r\n    name: rethinkdb\r\n    options:\r\n      host: rethinkdb\r\n      port: 28015\r\n      splitChar: /\r\n```\r\n\r\nNow you can run all containers with just one command:\r\n\r\n```bash\r\ndocker-compose up\r\n```\r\n\r\nIf you want to build the deepstream images from the Dockerfiles instead you can checkout\r\nour [Docker Repository on GitHub](https://github.com/deepstreamIO/docker)\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_docker-image_readme.md"}}
{"filePath":"install/docker-image/readme.md","title":"Docker Image","description":"Run a deepstream standalone container from an image. This tutorial will also explain how to build the image manually.","content":"\r\n\r\n### What is Docker?\r\n\r\n[Docker](https://www.docker.com/) is an open-source project that automates the deployment of applications inside software containers.\r\nDocker provides an additional layer of abstraction and automation of operating-system-level virtualization on Linux.\r\n\r\nAs actions are performed on a Docker base image, union file system layers are created and documented in such a way that each layer fully describes how to recreate the action. This strategy enables Docker's lightweight images as only layer updates need to be propagated (as opposed to entire VMs).\r\n\r\n![Docker Logo](docker.png)\r\n\r\n### Why use Docker with deepstream?\r\nDocker provides a lightweight solution to spin up fully functioning deepstream environments. It encapsulates all necessary settings and reduces complexity for large scale microservice deployments.\r\n\r\n### How to use Docker with deepstream?\r\nIn this tutorial we'll install a standalone Docker image and create a container. The image contains deepstream's source code as well as everything else necessary to run deepstream from source.\r\n\r\nIf you'd like a bit more than just a single deepstream node, head over to the [Docker Compose tutorial](../docker-compose/), describing how to set up multiple connected database, cache and search containers that act as the message and storage layer for deepstream.\r\n\r\nLet's start by installing the image from the DockerHub registry by running this command:\r\n\r\n```bash\r\ndocker pull deepstreamio/deepstream.io\r\n```\r\n\r\nNow create a container from this image and assign the container's name to `deepstream.io`:\r\n\r\n```bash\r\ndocker create -t -p 6020:6020 -p 6021:6021 \\\r\n  --name deepstream.io \\\r\n  -v $(pwd)/conf:/usr/local/deepstream/conf \\\r\n  -v $(pwd)/var:/usr/local/deepstream/var \\\r\n  deepstreamio/deepstream.io\r\n```\r\n\r\nNow you can start the container via\r\n\r\n```bash\r\ndocker start -ia deepstream.io\r\n```\r\n\r\nThis will start the container in the foreground. You can press <kbd>Ctrl</kbd>+<kbd>c</kbd> but\r\nthe container will still be alive. To stop the container you need to run\r\n\r\n```bash\r\ndocker stop deepstream.io\r\n```\r\n\r\nIn case you want to start it in the background just omit the `-ia` option.\r\n\r\nYou can show the logs with this command:\r\n\r\n```bash\r\ndocker logs -f deepstream.io\r\n```\r\n\r\nThe `-f` option will keep the process alive and follow output.\r\n\r\nIf you want to build the Docker image from the Dockerfile and use a docker-compose file instead you can checkout\r\nour [Docker Repository on GitHub](https://github.com/deepstreamIO/docker/tree/master/deepstream.io)\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_npm_readme.md"}}
{"filePath":"install/npm/readme.md","title":"NPM","description":"Installing deepstream via NPM and and Node.js","content":"\r\n\r\n![Node.js](nodejs.png)\r\n\r\ndeepstream can also be installed as an [NPM package](https://www.npmjs.com/package/deepstream.io) and offers a Node.js API to interact with it programmatically.\r\n\r\nThis can be useful to build custom authentication or permissioning logic or if you want to use the server [on top of a HTTP server like Express](/tutorials/integrations/other-http/). You can view the full Node.js API [here](/docs/server/node-api/).\r\n\r\nInstall the server via npm\r\n\r\n``` bash\r\nnpm install deepstream.io\r\n```\r\n\r\nCreate a js file, e.g. start.js with the following content\r\n\r\n```javascript\r\nconst DeepstreamServer = require('deepstream.io')\r\nconst C = DeepstreamServer.constants\r\n/*\r\nThe server can take\r\n1) a configuration file path\r\n2) null to explicitly use defaults to be overriden by server.set()\r\n3) left empty to load the base configuration from the config file located within the conf directory.\r\n4) pass some options, missing options will be merged with the base configuration\r\n*/\r\nconst server = new DeepstreamServer({\r\n  host: 'localhost',\r\n  port: 6020,\r\n  tcpPort: 6021\r\n})\r\n\r\n/*\r\nOptionally you can specify settings to set in complex objects, such as dataTransforms, a HTTPServer\r\nor a plugin which you want to reuse within your application\r\n*/\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.RPC,\r\n  action: C.ACTIONS.REQUEST,\r\n  transform: function(data, metaData) {}\r\n}])\r\n\r\n// start the server\r\nserver.start()\r\n```\r\n\r\nrun the file with node\r\n```bash\r\nnode start.js\r\n```\r\n\r\n#### Using the deepstream client in Node.js\r\nThe deepstream javascript client can be installed via [NPM](https://www.npmjs.com/package/deepstream.io-client-js) and used in Node.js.\r\n\r\n```bash\r\nnpm install deepstream.io-client-js\r\n```\r\n\r\nJust make sure to connect to deepstream's TCP port (6021 by default)!\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream('localhost:6021').login()\r\n```\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_osx_readme.md"}}
{"filePath":"install/osx/readme.md","title":"Installing on OSX","description":"Find out how to get deepstream running on OSX","content":"\r\n\r\n<a class=\"install-link\" data-os=\"mac\"></a>\r\n\r\n## Installing on Mac\r\nDownload the archive using the link above, extract it and open the extracted directory in your terminal.\r\n\r\n## Starting deepstream\r\n```bash\r\n./deepstream start\r\n```\r\n\r\nLearn more about deepstream's [command line interface](/docs/server/command-line-interface/) its [configuration file](/docs/server/configuration/).\r\n\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_ubuntu_readme.md"}}
{"filePath":"install/ubuntu/readme.md","title":"Installing on Ubuntu","description":"Installing Deepstream on Ubuntu Linux","content":"\r\n\r\nDeepstream is available via the APT package manager and currently supports Ubuntu `trusty`, `wily` and `xenial`.\r\n\r\nTo install, paste the following script into your terminal\r\n\r\n```bash\r\n# the source command makes the distro_name available as a variable\r\n# the echo command creates a source list entry string for the deepstream repo\r\n# the tee command appends it to APT's list of package sources\r\nsource /etc/lsb-release && echo \"deb http://dl.bintray.com/deepstreamio/deb ${DISTRIB_CODENAME} main\" | sudo tee -a /etc/apt/sources.list\r\n\r\n# downloads the key the distribution is signed with\r\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 379CE192D401AB61\r\n\r\n# updates APT's registry with the newly available packages\r\nsudo apt-get update\r\n\r\n# installs deepstream. -y skips \"are you sure?\" question\r\nsudo apt-get install -y deepstream.io\r\n```\r\n\r\n## Starting deepstream\r\ndeepstream can be started via its [command line interface](/docs/server/command-line-interface/).\r\n```bash\r\ndeepstream start\r\n```\r\n\r\n![Starting deepstream on linux](../linux-start.png)\r\n\r\n### Configuring deepstream\r\nYou can either change deepstream's [configuration file](../../docs/server/configuration/) directly in `/etc/deepstream` or create a copy and run deepstream with the `-c` flag. (Important, make sure to update all relative paths within the configuration after copying it).\r\n\r\n```bash\r\n$ cd ~\r\n$ cp /etc/deepstream/* .\r\n$ ls\r\nconfig.yml  permissions.yml  users.yml\r\n$ deepstream start -c config.yml\r\n```\r\n\r\n### Downloading deepstream's source code\r\nYou can also get the package and sources directly from [deepstream's release page](https://github.com/deepstreamIO/deepstream.io/releases)\r\n"}
{"index":{"_index":"pages","_type":"install","_id":"install_windows_readme.md"}}
{"filePath":"install/windows/readme.md","title":"Installing on Windows","description":"Find out how to get deepstream running on Windows","content":"\r\n\r\n<a class=\"install-link\" data-os=\"windows\"></a>\r\n\r\nDownload the server from the link above and unzip it.\r\n\r\n![Windows Executable](windows_install.png)\r\n\r\n## Starting deepstream\r\nYou can start the server by _double clicking_ the executable file or via `CMD`\r\n\r\n```bash\r\ndeepstream.exe start\r\n```\r\n\r\nLearn more about deepstream's [command line interface](/docs/server/command-line-interface/) and its [configuration file](/docs/server/configuration/).\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_client_readme.md"}}
{"filePath":"docs/client-js/client/readme.md","title":"Javascript Client","description":"The entry point API documentation for the deepstream.io js client","content":"\r\n\r\nThe deepstream JavaScript client can be used by both browsers and Node.js. You can get it via NPM or Bower as `deepstream.io-client-js` or browse the source on [Github](https://github.com/deepstreamIO/deepstream.io-client-js)\r\n\r\n### deepstream(url, options)\r\n\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: url\r\n  typ: String\r\n  opt: false\r\n  des: A url in the format <host>:<port>\r\n-\r\n  arg: options\r\n  typ: Object\r\n  opt: true\r\n  des: A map of options. Please find a list of available options [here](/docs/client-js/options/)\r\n{{/table}}\r\n\r\nCreates a client instance and initialises the connection to the deepstream server. The connection will be kept in a quarantine state and won't be fully usable until `login()` is called.\r\n\r\n```javascript\r\nvar deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream('localhost:6020').login()\r\n```\r\n\r\n## Constants\r\n* `deepstream.CONSTANTS` grants access to constants. See [constants](/docs/common/constants/) for a full list\r\n* `deepstream.MERGE_STRATEGIES` grants access to the default merge strategies used globally as `mergeStrategy` client option or per record with `record.mergeStrategy()`\r\n\r\n## Events\r\n\r\n### connectionStateChanged\r\nEmitted every time the connectionstate changes. The connectionState is passed to the callback and can also be retrieved using <a href=\"#getConnectionState()\">getConnectionState()</a>. A list of possible connection states is available [here](/docs/common/constants/#connection-states)\r\n\r\n### error\r\nAggregates all errors that are encountered. Some errors like `CONNECTION_ERROR` or `MESSAGE_PARSE_ERROR` are exlusively emitted by the client.\r\nOthers like `ACK_TIMEOUT` or `VERSION_EXISTS` that relate to a specific Record, Event or RPC are emitted first by the object they relate to and are then forwarded to the client. You can find a list of all errors [here](/docs/common/errors/).\r\n\r\n```javascript\r\nclient.on('error', ( error, event, topic ) =>\r\n  console.log(error, event, topic);\r\n)\r\n```\r\n\r\n## Methods\r\n\r\n### login(authParams, callback)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: authParams\r\n  typ: Object\r\n  opt: false\r\n  des: |\r\n    An object with authentication parameters, e.g <br><code>{ username: &#39;peter&#39;, password: '&#39;'sesame&#39; }</code>\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: true\r\n  des: A function that will be called once the response to the authentication request is received.\r\n{{/table}}\r\n```\r\n\r\nAuthenticates the client against the server. To learn more about how authentication works, please have a look at the [Security Overview](/tutorials/core/security-overview/).\r\n\r\nCallback will be called with: success (Boolean), data (Object).\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream('localhost:6020')\r\n// client.getConnectionState() will now return 'AWAITING_AUTHENTICATION'\r\n\r\nclient.login({username: 'peter', password: 'sesame'}, (success, data) => {\r\n  if (success) {\r\n    // start application\r\n    // client.getConnectionState() will now return 'OPEN'\r\n  } else {\r\n    // extra data can be optionaly sent from deepstream for\r\n    // both successful and unsuccesful logins\r\n    alert(data)\r\n\r\n    // client.getConnectionState() will now return\r\n    // 'AWAITING_AUTHENTICATION' or 'CLOSED'\r\n    // if the maximum number of authentication\r\n    // attempts has been exceeded.\r\n  }\r\n})\r\n\r\n// client.getConnectionState() will now return 'AUTHENTICATING'\r\n```\r\n\r\n### close()\r\nCloses the connection to the server.\r\n\r\n```javascript\r\nclient.on('connectionStateChanged', connectionState) => {\r\n  // will be called with 'CLOSED' once the connection is successfully closed.\r\n})\r\n\r\nclient.close()\r\n```\r\n\r\n### getConnectionState()\r\nReturns the current connectionState. Please find a list of available connectionStates [here](/docs/common/constants/#connection-state).\r\n\r\n### getUid()\r\nReturnes a unique id. The uid starts with a Base64 encoded timestamp to allow for semi-sequentual ordering and ends with a random string.\r\n\r\n```javascript\r\nclient.getUid() // 'i9i6db5q-1xak1s2sfzk'\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_datasync-anonymous-record_readme.md"}}
{"filePath":"docs/client-js/datasync-anonymous-record/readme.md","title":"Anonymous Record","description":"Learn how to use anonymous records to switch context without having to renew bindings","content":"\r\n\r\nAnonymousRecord is a record without a predefined name. It functions as a wrapper around an actual record that can be swapped out for another while keeping all bindings intact.\r\n\r\nAn anonymousRecord extends `Record` and contains all of its [API calls](/docs/client-js/datasync-record/).\r\n\r\nTo learn more about how they are used, have a look at the [AnonymousRecord Tutorial](/tutorials/core/datasync-anonymous-records/).\r\n\r\n## Creating an anonymousRecord\r\n\r\nAnonymousRecords are created and retrieved using `client.record.getAnonymousRecord('name')`\r\n\r\n```javascript\r\nconst anonymousRecord = client.record.getAnonymousRecord()\r\n```\r\n\r\n## Events\r\n\r\n### nameChanged\r\nThe new name of the underlying record which the anonymous record now represents.\r\n\r\n## Methods\r\n\r\n### setName(recordName)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: recordName\r\n  typ: String\r\n  opt: false\r\n  des: The name of the actual record the anonymousRecord should use. This can be called multiple times.\r\n{{/table}}\r\n```\r\nSets the underlying record the `anonymousRecord` wraps around. It takes care of cleaning up the previous record on your behalf.\r\n\r\n```javascript\r\nconst anonymousRecord = client.record.getAnonymousRecord()\r\nanonymousRecord.setName('user/john-snow')\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_datasync-client-record_readme.md"}}
{"filePath":"docs/client-js/datasync-client-record/readme.md","title":"Record Factory","description":"This class gives you access to all methods related to data-sync","content":"\r\n\r\n`client.record` gives you access to all methods related to data-sync.\r\n\r\n## Prerequisite\r\n\r\nYou need to connect to the deepstream server:\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream( 'localhost:6020')\r\nclient.login()\r\n```\r\n\r\n## Methods\r\n\r\n### client.record.getRecord(name)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  argument: name\r\n  type: String\r\n  optional: false\r\n  desc: The name of the record.\r\n{{/table}}\r\n```\r\n\r\nRetrieves or creates a [Record](/docs/client-js/datasync-record/) with the given name. Records are persistent data structures that are synced between clients. To learn more about what they are used for and how they work, head over to the [record tutorial](/tutorials/core/datasync-records/).\r\n\r\n{{#infobox \"info\"}}\r\nThe record will be loaded asynchronously. To ensure the record is loaded put your logic into the [whenReady](/tutorials/core/datasync-records/) callback.\r\n{{/infobox}}\r\n\r\n```javascript\r\nconst record = client.record.getRecord('user/johndoe')\r\n```\r\n\r\n### client.record.getList(name)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  argument: name\r\n  type: String\r\n  optional: false\r\n  desc: The name of the record.\r\n{{/table}}\r\n```\r\n\r\nRetrieves or creates a [List](../datasync-list/) with the given name. Lists are arrays of recordNames that clients can manipulate and observe. You can learn more about them in the [list tutorial](/tutorials/core/datasync-lists/).\r\n\r\n{{#infobox \"info\"}}\r\nThe list will be loaded asynchronously. To ensure the list is loaded put your\r\nlogic into the [whenReady](/tutorials/core/datasync-records/) callback.\r\n{{/infobox}}\r\n\r\n```javascript\r\nconst beatlesAlbums = client.record.getList('albums')\r\nbeatlesAlbums.whenReady(() => console.log(beatlesAlbums.getEntries()))\r\n\r\n/*\r\n  [\r\n    \"album/i9l0z34v-109vblpqddy\",\r\n    \"album/i9l0z3v4-ibrbp139rbr\",\r\n    \"album/i9l0z4d8-1w0p8xnk1sy\"\r\n  ]\r\n*/\r\n```\r\n\r\n### client.record.getAnonymousRecord()\r\n\r\nReturns an [AnonymousRecord](../datasync-anonymous-record/).\r\n\r\nAn AnonymousRecord is a record that can change its name. It\r\nacts as a wrapper around an actual record that can\r\nbe swapped out for another one whilst keeping all bindings intact.\r\nYou can learn more about anonymous records [here](/tutorials/core/datasync-anonymous-records/).\r\n\r\n```javascript\r\nconst record = client.record.getAnonymousRecord()\r\nrecord.setName('user/johndoe')\r\nrecord.setName('user/maxpower')\r\n```\r\n\r\n### client.record.has(name, callback)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  opt: false\r\n  des: The name of the record.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: Arguments are (String) error and (Boolean) hasRecord\r\n{{/table}}\r\n```\r\n\r\nThe callback contains an error argument and a boolean to indicate whether or not the record exists in deepstream. This is useful to avoid creating a record via `getRecord( name )` if you only want to edit the contents. The callback is invoked immediately if the record exists on the client.\r\n\r\n```javascript\r\nvar user = client.record.has('user/johndoe', (error, hasRecord) => {\r\n  // ...\r\n})\r\n```\r\n\r\n### client.record.snapshot(name, callback)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  opt: false\r\n  des: The name of the record.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: Arguments are (String) error and (Object) data\r\n{{/table}}\r\n```\r\n\r\nThe callback contains the record's content without subscribing to updates. This can be used to avoid scenarios where you would request the record and discard it immediately afterwards. The callback is invoked immediately if the record data is already loaded and ready.\r\n\r\n```javascript\r\nclient.record.snapshot('user/johndoe', (error, data) => {\r\n\t// ...\r\n})\r\n```\r\n\r\n### client.record.listen(pattern, callback)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: pattern\r\n  typ: String\r\n  opt: false\r\n  des: A RegExp as a string\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that will be called whenever a match is found. Arguments are (String) match and (Boolean) isSubscribed\r\n{{/table}}\r\n```\r\n\r\nAllows to listen for record subscriptions made by other clients. This is useful to create \"active\" data providers, e.g. providers that only provide data for records that users are actually interested in. You can find more about listening in the [record tutorial](/tutorials/core/datasync-records/).\r\n\r\n```javascript\r\nclient.record.listen('raceHorse/.*', (match, isSubscribed) => {\r\n  console.log(match) // 'raceHorse/fast-betty'\r\n  if (isSubscribed) {\r\n    // start publishing data\r\n  } else {\r\n    // stop publishing data\r\n  }\r\n})\r\n```\r\n\r\n<br/>\r\n{{#infobox \"info\"}}\r\n<br/>The listen callback will only be called once with `subscribed = true` for the first time a matching subscription is made and once with `subscribed = false` once all clients have unsubscribed from the record.\r\n<br/>The callback will be called for all matching subscriptions that already exist at the time its registered.\r\n{{/infobox}}\r\n\r\n### client.record.unlisten(pattern)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: pattern\r\n  typ: String\r\n  opt: false\r\n  def: '-'\r\n  des: A RegExp as a string\r\n{{/table}}\r\n```\r\n\r\n```javascript\r\nclient.record.unlisten('raceHorse/.*')\r\n```\r\n\r\nRemoves a listener that was previously registered using `listen()`.\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_datasync-list_readme.md"}}
{"filePath":"docs/client-js/datasync-list/readme.md","title":"List","description":"API docs for deepstream's list object, a manageable collection of record names","content":"\r\n\r\nLists are collections of record names. To learn more about how they are used, have a look at the [List Tutorial](../../../tutorials/core/datasync-lists/).\r\n\r\nLists and record names have an `n:m` relationship. A record name can be part of many lists and a list can contain many record names. A list can also contain the same record name multiple times.\r\n\r\n## Creating lists\r\nLists are created and retrieved using `client.record.getList( 'name' );`\r\n\r\n```javascript\r\nconst list = client.record.getList( 'cars' );\r\n```\r\n\r\n## Properties\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  des: The name of the list, as specified when calling `client.record.getList( 'name' );`\r\n-\r\n  arg: usages\r\n  typ: Number\r\n  des: The number of times `client.record.getList()` has been called for this record throughout the application\r\n-\r\n  arg: isReady\r\n  typ: Boolean\r\n  des: True once the list has received its current data and emitted the `'ready'` event\r\n{{/table}}\r\n```\r\n## Events\r\n\r\n### ready\r\nEmitted once the list has received its current data from the server.\r\n\r\n### entry-added\r\nEmitted whenever a new entry is added to the list. Passes the entry and its position within the list to the callback.\r\n\r\n### entry-moved\r\nEmitted whenever an entry is moved within the list. Passes the entry and its new position within the list to the callback.\r\n\r\n### entry-removed\r\nEmitted whenever an entry is removed from the list. Passes the entry and its last position within the list to the callback.\r\n\r\n### delete\r\nEmitted when the list was deleted, whether by this client or by another.\r\n\r\n### discard\r\nEmitted once the list was discarded.\r\n\r\n### error\r\nEmitted if the list encounters an error. The error message is passed to the event callback.\r\n\r\n## Methods\r\n\r\n### whenReady( callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that will be invoked as soon as the list is ready. Receives the list as an argument\r\n{{/table}}\r\n```\r\nInvokes `callback` once the list has been loaded. This might happen synchronously if the list is already available or asynchronously if the list still needs to be retrieved. Some methods, e.g. `addEntry()` or `setEntries()` or `subscribe()` can be used before the list is ready.\r\n\r\n```javascript\r\nlist.whenReady( ( list ) => {\r\n  // interact with the list\r\n});\r\n```\r\n\r\n### isEmpty()\r\n\r\nReturns `false` if the list has entries or `true` if it doesn't.\r\n\r\n```javascript\r\nif( list.isEmpty() ) {\r\n  // You don't have any entries\r\n}\r\n```\r\n\r\n### getEntries()\r\nReturns an array of the current entries in the list.\r\n\r\n```javascript\r\nvar entries = list.getEntries( entries )\r\nconsole.log( entries ) // [ 'car/1', 'car2' ]\r\n```\r\n\r\n### setEntries( entries )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: entries\r\n  typ: Array\r\n  opt: false\r\n  des: An array of record name strings\r\n{{/table}}\r\n```\r\nSets the contents of the list to the provided array of record names. To add or remove specific entries use `addEntry()` or `removeEntry()` respectively.\r\n\r\n```javascript\r\nlist.setEntries( [ 'car/1', 'car/2' ] );\r\n```\r\n\r\n### addEntry( entry, index )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: entry\r\n  typ: String\r\n  opt: false\r\n  des: A record name that should be added to the list\r\n-\r\n  arg: index\r\n  typ: Number\r\n  opt: true\r\n  des: An optional index that the new entry should be inserted at. If ommited, the new entry is appended to the end of the list.\r\n{{/table}}\r\n```\r\nAdds a new record name to the list.\r\n\r\n```javascript\r\nfunction addCar( number ) {\r\n  var id = 'car/' + client.getUid();\r\n  client.record.getRecord( id ).set( 'number', number );\r\n  list.addEntry( id );\r\n}\r\n```\r\n\r\n### removeEntry( entry, index )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: entry\r\n  typ: String\r\n  opt: false\r\n  des: A record name that should be removed from the list\r\n-\r\n  arg: index\r\n  typ: Number\r\n  opt: true\r\n  des: The index at which the record should be removed. If ommited, all entries of the given name will be removed\r\n{{/table}}\r\n```\r\nRemoves an entry from the list. `removeEntry` will not throw any error if the entry doesn't exist.\r\n\r\n```javascript\r\nfunction removeCar( carRecord ) {\r\n  list.removeEntry( carRecord.name );\r\n}\r\n```\r\n\r\n### subscribe( callback, triggerNow )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A callback function that will be called whenever the content of the list changes\r\n-\r\n  arg: triggerNow\r\n  typ: Boolean\r\n  opt: true\r\n  des: If true, the callback function will be called immediately with the current value\r\n{{/table}}\r\n```\r\nRegisters a function that will be invoked whenever any changes to the list's contents occur. Optionally you can also pass `true` to execute the callback function straight away with the list's current entries.\r\n\r\n```javascript\r\nfunction listChanged( entries ) {\r\n  // entries in list has changed\r\n}\r\nlist.subscribe( listChanged, false );\r\n```\r\n\r\n### unsubscribe( callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: true\r\n  des: The previously registered callback function. If ommited, all listeners will be unsubscribed.\r\n{{/table}}\r\n```\r\nRemoves a subscription that was previously made using `list.subscribe()`\r\n\r\nPlease Note: unsubscribe is purely a client side operation. To notify the server\r\nthat the app no longer requires updates for this list use `discard()`.\r\n\r\n```javascript\r\nlist.unsubscribe( listChanged );\r\n```\r\n\r\n### discard()\r\nRemoves all change listeners and notifies the server that the client is no longer interested in updates for this list.\r\n\r\n```javascript\r\nlist.discard();\r\n```\r\n<br/>\r\n{{#infobox \"info\"}}\r\nIt is important to make sure that `discard()` is called for any list that's no longer needed. If you only remove the listeners using `unsubscribe()` the server won't be notified and will continue to send updates to the client.\r\n{{/infobox}}\r\n\r\n### delete()\r\nDeletes the list on the server. This action deletes the list for all users from both cache and storage and is irreversible.\r\n\r\n```javascript\r\nlist.delete();\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_datasync-record_readme.md"}}
{"filePath":"docs/client-js/datasync-record/readme.md","title":"Record","description":"The API docs for deepstream records","content":"\r\n\r\nRecords are one of deepstream's core features. A Record is an arbitrary JSON data structure that can be created, retrieved, updated, deleted and listened to. Records are created and retrieved using `client.record.getRecord('name')`\r\n\r\nTo learn more about how they are used, have a look at the [Record Tutorial](/tutorials/core/datasync-records/).\r\n\r\n## Creating records\r\n\r\nRecords are created and retrieved using `client.record.getRecord( 'name' );`\r\n\r\n```javascript\r\nconst recordName = `user/${client.getUid()}` // \"user/iqaphzxy-2o1pnsvcnbo\"\r\nconst record = client.record.getRecord(recordName)\r\n```\r\n\r\n## Properties\r\n\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  desc: The name of the record, as specified when calling `client.record.getRecord( name )`\r\n-\r\n  arg: usages\r\n  typ: Number\r\n  desc: The number of active records throughout the application, meaning they have not yet been discarded via `discard()`\r\n-\r\n  arg: isReady\r\n  typ: Boolean\r\n  desc: True once the record has received its current data and emitted the `'ready'` event\r\n-\r\n  arg: isDestroyed\r\n  typ: Boolean\r\n  desc: True once the record has been discarded or deleted. The record would need to be retrieved again via `client.record.getRecord( name )\r\n{{/table}}\r\n```\r\n\r\n## Events\r\n\r\n### ready\r\nEmitted once the record has received its current data from the server.\r\n\r\n### delete\r\nEmitted when the record was deleted, whether by this client or by another.\r\n\r\n### discard\r\nEmitted once the record was discarded.\r\n\r\n### error\r\nEmitted if the record encounters an error. The error message is passed to the event callback.\r\n\r\n## Methods\r\n\r\n### whenReady(callback)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that should be invoked as soon as the record is ready.\r\n{{/table}}\r\n```\r\nImmediately executes the callback if the record is ready. Otherwise, it registers it as a callback for the `ready` event.\r\n\r\n```javascript\r\nrecord.whenReady(record => {\r\n  // data has now been loaded\r\n})\r\n```\r\n\r\n### set(path, value)\r\n\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: path\r\n  typ: String\r\n  opt: true\r\n  des: A particular path within the JSON structure that should be set.\r\n-\r\n  arg: value\r\n  typ: Various\r\n  opt: false\r\n  des: The value the record or path should be set to.\r\n{{/table}}\r\n```\r\nUsed to set the record's data and can be called with a value. A path can optionally be included.\r\n\r\n```javascript\r\n// Set the entire record's data\r\nrecord.set({\r\n  personalData: {\r\n    firstname: 'Homer',\r\n    lastname: 'Simpson',\r\n    status: 'married'\r\n  },\r\n  children: ['Bart', 'Maggie', 'Lisa']\r\n});\r\n\r\n// Update only firstname\r\nrecord.set('personalData.firstname', 'Marge')\r\n```\r\n\r\n### get(path)\r\n\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: path\r\n  typ: String\r\n  opt: true\r\n  des: A particular path within the JSON structure that should be retrieved.\r\n{{/table}}\r\n```\r\nUsed to return the record's data but if called without an argument, will return all the data. `get()` can also be used to retrive a specific part by defining a path string. If the part can not be found, `undefined` will be returned.\r\n\r\n```javascript\r\nrecord.get() // Returns entire object\r\nrecord.get('children[1]') // 'Maggie'\r\nrecord.get('personalData.firstname') // 'Homer'\r\n```\r\n\r\n### subscribe(path, callback, triggerNow)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: path\r\n  typ: String\r\n  opt: true\r\n  des: A path within the JSON structure that should be subscribed to.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that is called whenever the value changes and the data passed through.\r\n-\r\n  arg: triggerNow\r\n  typ: Boolean\r\n  opt: true\r\n  des: If true, the callback function will be called immediately with the current value.\r\n{{/table}}\r\n```\r\nRegisters that a function will be performed whenever the record's value changes. All of the record's data can be subscribed to by providing a callback function or when changes are performed to a specific path within the record.\r\n\r\nOptional: Passing `true` will execute the callback immediately with the record's current value.\r\n\r\nListening to any changes on the record:\r\n```javascript\r\n// Subscribe to any change of the record\r\nfunction userDataChanged(data) {\r\n  // do stuff...\r\n}\r\nuser.subscribe(userDataChanged)\r\n```\r\n\r\nListening to changes on a specific path within the record:\r\n```javascript\r\n// Only subscribe to the status of the user\r\nfunction statusChanged( status ) {\r\n  if (status !== 'married') {\r\n    // I want my childhood back!\r\n  }\r\n}\r\nuser.subscribe('status', statusChanged, true)\r\n```\r\n\r\n### unsubscribe(path, callback)\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: path\r\n  typ: String\r\n  opt: true\r\n  des: The path that was previously used for subscribe.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: The previously registered callback function.\r\n{{/table}}\r\n```\r\nRemoves a subscription previous made using `record.subscribe()`. Defining a path with `unsubscribe` removes that specific path, or with a callback, can remove it from generic subscriptions.\r\n\r\n{{#infobox \"info\"}}\r\n`unsubscribe` is entirely a client-side operation. To notify the server that the app would no longer interested in the record, use `discard()` instead.\r\n{{/infobox}}\r\n\r\nUnsubscribe all callbacks registered with the path `status`\r\n```javascript\r\nuser.unsubscribe('status')\r\n```\r\n\r\nUnsubscribe a specific callback registered for the path `status`\r\n```javascript\r\nuser.unsubscribe('status', statusChanged)\r\n```\r\n\r\nUnsubscribe a specific callback registered for the record\r\n```javascript\r\nuser.unsubscribe(userDataChanged)\r\n```\r\n\r\nUnsubscribe all callbacks not associated with a path\r\n```javascript\r\nuser.unsubscribe()\r\n```\r\n<br/>\r\n{{#infobox \"info\"}}\r\nIt is important to unsubscribe all callbacks that are registered when discarding a record. Just calling discard does **not** guarantee that callbacks will not be called.\r\n{{/infobox}}\r\n\r\n### discard()\r\nRemoves all change listerners and notifies the server that client no longer wants updates for this record if your application\r\nno longer requires the record.\r\n\r\n```javascript\r\nuser.discard()\r\n```\r\n\r\n<br/>\r\n{{#infobox \"info\"}}\r\n<br/>It is important to use this operation for records that are no longer needed. `unsubscribe()` only removes listeners and does not notify the server; in this case, the server will continue to send updates to the client.\r\n<br/>Records are only discarded when you have no record subscriptions left.\r\n{{/infobox}}\r\n\r\n### delete()\r\nThis permanently deletes the record on the server for all users.\r\n\r\n```javascript\r\nuser.delete()\r\n```\r\n\r\n<br/>\r\n{{#infobox \"info\"}}\r\n<br/>Since deleting a record means that it no longer exists, the resulting action will be a forced discard to all clients with that record.\r\n<br/>Creating a record directly after deleting it without waiting for the `delete` event can end up in a race condition. Try to ensure the record has been deleted succesfully to avoid edge cases.\r\n{{/infobox}}\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_options_readme.md"}}
{"filePath":"docs/client-js/options/readme.md","title":"Options","description":"The options that the deepstream javascript client can be initialized with","content":"\r\n\r\nOptions are passed to the client upon initialisation\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream( 'localhost:6020', {\r\n  // custom deepstream options\r\n  mergeStrategy: deepstream.LOCAL_WINS,\r\n  subscriptionTimeout: 500,\r\n  // custom engine.io options\r\n  rememberUpgrade: true\r\n})\r\n```\r\n\r\nYou can finely tune deepstream to meet your specific requirements, including reconnection behaviour and granular timeouts.\r\n\r\n## General Configuration\r\n\r\n### mergeStrategy\r\nA global merge strategy that is applied whenever two clients write to the same record at the same time. Can be overwritten on a per record level. Default merge strategies are exposed by the client constructor. It's also possible to write custom merge strategies as functions. You can find more on handling data conflicts [here](/tutorials/core/handling-data-conflicts/)<br>\r\n_Type_: Function<br>\r\n_Default_: `MERGE_STRATEGIES.REMOTE_WINS`\r\n\r\n### reconnectIntervalIncrement\r\nSpecifies the number of milliseconds by which the time until the next reconnection attempt will be incremented after every unsuccessful attempt.<br>\r\nE.g.for 1500: if the connection is lost,the client will attempt to reconnect immediately, if that fails it will try again after 1.5 seconds, if that fails it will try again after 3 seconds and so on...<br>\r\n_Type_: Number<br>\r\n_Default_: `4000`\r\n\r\n### maxReconnectAttempts\r\nThe number of reconnection attempts until the client gives up and declares the connection closed.<br>\r\n_Type_: Number<br>\r\n_Default_: `5`\r\n\r\n### rpcAckTimeout\r\nThe number of milliseconds after which a RPC will error if no Ack-message has been received.<br>\r\n_Type_: Number<br>\r\n_Default_: `6000`\r\n\r\n### rpcResponseTimeout\r\nThe number of milliseconds after which a RPC will error if no response-message has been received.<br>\r\n_Type_: Number<br>\r\n_Default_: `10000`\r\n\r\n### subscriptionTimeout\r\nThe number of milliseconds that can pass after providing/unproviding a RPC or subscribing/unsubscribing/listening to a record or event before an error is thrown.<br>\r\n_Type_: Number<br>\r\n_Default_: `2000`\r\n\r\n### maxMessagesPerPacket\r\nIf your app sends a large number of messages in quick succession, the deepstream client will try to split them into smaller packets and send these every <timeBetweenSendingQueuedPackages>ms. This parameter specifies the number of messages after which deepstream sends the packet and queues the remaining messages. Set to `Infinity` to turn the feature off.<br>\r\n_Type_: Number<br>\r\n_Default_: `100`\r\n\r\n### timeBetweenSendingQueuedPackages\r\nPlease see description for maxMessagesPerPacket. Sets the time in ms.<br>\r\n_Type_: Number<br>\r\n_Default_: `16`\r\n\r\n### recordReadAckTimeout\r\nThe number of milliseconds from the moment `client.record.getRecord()` is called until an error is thrown since no ack message has been received.<br>\r\n_Type_: Number<br>\r\n_Default_: `1000`\r\n\r\n### recordReadTimeout\r\nThe number of milliseconds from the moment `client.record.getRecord()` is called until an error is thrown since no data has been received.<br>\r\n_Type_: Number<br>\r\n_Default_: `3000`\r\n\r\n### recordDeleteTimeout\r\nThe number of milliseconds from the moment `record.delete()` is called until an error is thrown since no delete ack message has been received. Please take into account that the deletion is only complete after the record has been deleted from both cache and storage.<br>\r\n_Type_: Number<br>\r\n_Default_: `3000`\r\n\r\n## Browser connection specific\r\n\r\nThe following options do not apply to TCP connections and are specific to engine.io\r\n\r\n### upgrade\r\nWhether the client should try to upgrade the transport from long-polling to something better, e.g. WebSocket.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `true`\r\n\r\n### forceJSONP\r\nForces JSONP for polling transport.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `false`\r\n\r\n### jsonp\r\nDetermines whether to use JSONP when necessary for polling. If disabled (by settings to false) an error will be emitted (saying \"No transports available\") if no other transports are available. If another transport is available for opening a connection (e.g. WebSocket) that transport will be used instead.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `true`\r\n\r\n### forceBase64\r\nForces base 64 encoding for polling transport even when XHR2 responseType is available and WebSocket even if the used standard supports binary.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `false`\r\n\r\n### enablesXDR\r\nEnable Cross Domain Requests for IE8 to avoid loading the bar flashing click sounds. Default to false because Cross Domain Requests can't send cookies.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `false`\r\n\r\n### timestampRequests\r\nWhether to add the timestamp with each transport request. Note: this is ignored if the browser is IE or Android, in which case requests are always stamped.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `false`\r\n\r\n### timestampParam\r\nThe GET parameter key to use for the timestamp.<br>\r\n_Type_: String<br>\r\n_Default_: `t`\r\n\r\n### path\r\nThe path to connect to for browser connections.<br>\r\n_Type_: String<br>\r\n_Default_: `/deepstream`\r\n\r\n### transports\r\nA list of transports to try (in order). Engine.io always attempts to connect directly with the first one, provided the feature detection test for it passes.<br>\r\n_Type_: Array<br>\r\n_Default_: `['polling', 'websocket']`\r\n\r\n### rememberUpgrade\r\nIf true and if the previous websocket connection to the server succeeded, the connection attempt will bypass the normal upgrade process and will initially try websocket. A connection attempt following a transport error will use the normal upgrade process. It is recommended you turn this on only when using SSL/TLS connections, or if you know that your network does not block websockets.<br>\r\n_Type_: Boolean<br>\r\n_Default_: `false`\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_other-eventemitter_readme.md"}}
{"filePath":"docs/client-js/other-eventemitter/readme.md","title":"Event Emitter","description":"API docs for deepstream's event emitter","content":"\r\n\r\ndeepstream uses a `component-emitter` as an event emitter to implement a subset of the Node.js EventEmitter API.\r\n\r\n## Methods\r\n\r\n### on( event,callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: An eventname.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: The function that should be invoked when the event is emitted.\r\n{{/table}}\r\n```\r\nSubscribe to an event.\r\n\r\n```javascript\r\ndeepstream.on( 'error', ( error ) => {\r\n  // do something with error\r\n} )\r\n```\r\n\r\n### off( event,callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: true\r\n  des: An eventname.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: true\r\n  des: The previously-registered function.\r\n{{/table}}\r\n```\r\nUnsubscribes from an event by:\r\n* removing a specific callback when called with both arguments.\r\n```javascript\r\ndeepstream.off( 'error', errorCallback )\r\n```\r\n* removing all listeners for an event when only called with an event.\r\n```javascript\r\ndeepstream.off( 'error' )\r\n```\r\n* removing all listeners for all events if called without arguments.\r\n```javascript\r\ndeepstream.off()\r\n```\r\n\r\n### once( event,callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: An eventname.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: The function that should be invoked when the event is emitted.\r\n{{/table}}\r\n```\r\nRegister a one-time listener for an event. The listener will be removed immediately after its first execution.\r\n\r\n```javascript\r\ndeepstream.once( 'error', function( error ) {\r\n  // do something with error\r\n} )\r\n```\r\n\r\n### emit( event... )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: An eventname.\r\n-\r\n  arg: arguments\r\n  typ: Mixed\r\n  opt: true\r\n  des: The function that should be invoked when the event is emitted.\r\n{{/table}}\r\n```\r\nEmits an event.\r\n\r\n```javascript\r\ndeepstream.emit( 'error', 'An error of epic proportions occured!' )\r\n```\r\n\r\n### listeners( event )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: An eventname.\r\n{{/table}}\r\n```\r\nReturns an array of listeners that are registered for the event.\r\n\r\n```javascript\r\nconst listeners = deepstream.listeners() // [ Listener... ]\r\n```\r\n\r\n### hasListeners( event )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: An eventname.\r\n{{/table}}\r\n```\r\nReturns true if there are listeners registered for that event.\r\n\r\n```javascript\r\nconst hasListeners = deepstream.hasListener('error') // true\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_pubsub-client-event_readme.md"}}
{"filePath":"docs/client-js/pubsub-client-event/readme.md","title":"Event","description":"API docs for deepstream's events, the many to many broadcasting mechanism","content":"\r\n\r\nEvents are deepstream's implementation of the publish/subscribe pattern. You can find more about how they are used in the [events tutorial](/tutorials/core/pubsub-events/)\r\n\r\n## Methods\r\n\r\n### client.event.subscribe( event, callback )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: an eventname\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that will be called whenever the event is emitted\r\n{{/table}}\r\n\r\nSubscribes to an event. Callback will receive the data passed to `emit()`\r\n\r\n```javascript\r\n// client a\r\nclient.event.subscribe( 'news/sports', function( news ){\r\n  // show news\r\n});\r\n\r\n// client b\r\nclient.event.emit( 'news/sports', {\r\n  headline: 'New Olympics location announced',\r\n  content: 'In a surprising twist, the committee has chosen Antartica as the location of the next Olympics.'\r\n});\r\n```\r\n\r\n### client.event.unsubscribe( event, callback )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: an eventname\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: true\r\n  des: The callback that was previously registered with subscribe. If the callback is omitted, all listeners for the event will be removed\r\n{{/table}}\r\n\r\nUnsubscribes from an event that was previously registered with `subscribe()`. This stops a client from receiving the event.\r\n\r\n```javascript\r\n  client.event.unsubscribe( 'news/politics', callback );\r\n```\r\n\r\n### client.event.emit( event, data )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: event\r\n  typ: String\r\n  opt: false\r\n  des: an eventname\r\n-\r\n  arg: data\r\n  typ: Mixed\r\n  opt: true\r\n  des: Any serialisable data ( Objects, Strings, Numbers... ) that will be send with the event\r\n{{/table}}\r\n\r\nSends the event to all subscribed clients\r\n\r\n```javascript\r\n  client.event.emit( 'notifications', 'Maria just came online');\r\n```\r\n\r\n\r\n### client.event.listen( pattern, callback )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: pattern\r\n  typ: RegExp\r\n  opt: false\r\n  des: The pattern to match events which subscription status you want to be informed of\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that will be called whenever an event has been initially subscribed to or is no longer subscribed.\r\n{{/table}}\r\n\r\nRegisters the client as a listener for event subscriptions made by other clients. This is useful to create \"active\" data providers - processes that only send events if clients are actually interested in them. You can find more about listening in the [events tutorial](/tutorials/core/pubsub-events/#how-to-listen-for-event-subscriptions)\r\n\r\nThe callback is invoked with two arguments:\r\n- **eventName**: The name of the event that has been matched against the provided pattern\r\n- **isSubscribed**: A boolean indicating whether the event is subscribed or unsubscribed\r\n\r\n```javascript\r\n  client.event.listen( '^news/.*', function( eventName, isSubscribed ){\r\n    //start sending news for the relevant topic\r\n  });\r\n```\r\n\r\n### client.event.unlisten( pattern )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: pattern\r\n  typ: RegEx\r\n  opt: false\r\n  des: The previously registered pattern\r\n\r\n{{/table}}\r\n\r\nThis removes a previously registered listening pattern and the user will no longer be listening for active/inactive subscriptions.\r\n\r\n```javascript\r\n  client.event.unlisten( '^news/.*' );\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_reqres-client-rpc_readme.md"}}
{"filePath":"docs/client-js/reqres-client-rpc/readme.md","title":"RPC","description":"The API docs for remote procedure calls, deepstream's request-response mechanism","content":"\r\n\r\nRPCs (Remote Procedure Calls, sometimes also referred to as Remote Method Invocation) is deepstream's implementation of request-response workflows.\r\n\r\n## Methods\r\n\r\n### client.rpc.provide( name, callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  opt: false\r\n  des: The name of the RPC. Each client can only register as a provider for an RPC name once.\r\n-\r\n  arg: callback\r\n  typ: Function\r\n  opt: false\r\n  des: A function that handles incoming RPCs. Will be called with data and an [RPC response object](../reqres-response/).\r\n{{/table}}\r\n```\r\n\r\nRegisters the client as a provider for incoming RPCs of a specific name. The callback will be invoked when another client `client.rpc.make()`.\r\n\r\n```javascript\r\nclient.rpc.provide( 'add-two-numbers', function( data, response ){\r\n    response.send( data.numA + data.numB );\r\n});\r\n```\r\n<br/>\r\n{{#infobox \"info\"}}\r\n<br/>Only one callback per client can be registered for the same RPC\r\n<br/>Data can be any serializable object\r\n<br/>Documentation for the `response` object can be found [here](../reqres-response/)\r\n{{/infobox}}\r\n\r\n### client.rpc.unprovide( name )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  opt: false\r\n  des: The name of the RPC.\r\n{{/table}}\r\n```\r\n\r\nRemoves the client as a provider previously registered using `provide()`.\r\n\r\n```javascript\r\nclient.rpc.unprovide( 'add-two-numbers' );\r\n```\r\n\r\n### client.rpc.make( name, data, callback )\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: name\r\n  typ: String\r\n  opt: false\r\n  des: rpcname\r\n-\r\n  arg: data\r\n  typ: Mixed\r\n  opt: true\r\n  des: Any serialisable data ( Objects, Strings, Numbers... ) that will be send with the rpc\r\n{{/table}}\r\n```\r\n\r\nExecutes a remote procedure call. `callback` will be invoked with the returned result or with an error if the RPC failed.\r\n\r\n```javascript\r\nclient.rpc.make( 'add-two-numbers', { numA:4, numB: 7 }, ( error, result ) => {\r\n    // error = null, result = 11\r\n});\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_client-js_reqres-response_readme.md"}}
{"filePath":"docs/client-js/reqres-response/readme.md","title":"RPC Response","description":"The API docs for deepstream's RPC response object","content":"\r\n\r\nThe RPC response object is passed to the callback registered with `client.rpc.provide()`. It allows RPC providers to decide how to react to an incoming request.\r\n\r\n## Methods\r\n\r\n### response.send( data )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: data\r\n  typ: Mixed\r\n  opt: false\r\n  des: Any serializable response data\r\n{{/table}}\r\n\r\nSuccesfully complete a remote procedure call and sends data back to the requesting client.\r\n\r\n`data` can be any kind of serializable data, e.g. Objects, Numbers, Booleans or Strings\r\n\r\nIf `autoAck` is disabled and the response is sent before the ack message, the request will still be completed and the ack message will be ignored.\r\n\r\n```javascript\r\nclient.rpc.provide( 'add-two-numbers', ( data, response ) => {\r\n  response.send( data.numA + data.numB );\r\n});\r\n```\r\n\r\n### response.reject()\r\nRejects the request. Rejections are not errors, but merely a means of saying \"I'm busy at the moment, try another client\". Upon receiving a rejection deepstream will try to re-route the request to another provider for the same RPC. If there are no more providers left to try, deepstream will send a NO_RPC_PROVIDER error to the client.\r\n\r\n```javascript\r\nclient.rpc.provide( 'add-two-numbers', ( data, response ) => {\r\n  //reject the response so that it gets\r\n  //re-routed to another provider\r\n  response.reject();\r\n});\r\n```\r\n\r\n### error( errorMsg )\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: errorMsg\r\n  typ: String\r\n  opt: false\r\n  des: A string that will be passed as error to the RPC requestor\r\n{{/table}}\r\n\r\nSend an error to the client. `errorMsg` will be received as the first argument to the callback registered with `client.rpc.make()`. This will complete the RPC.\r\n\r\n```javascript\r\nclient.rpc.provide( 'count-vote', ( data, response ) => {\r\n  if( hasAlreadyVoted( data.user ) ) {\r\n   response.error( 'You can only vote once' );\r\n  }\r\n});\r\n```\r\n\r\n### response.ack()\r\nExplicitly acknowledges the receipt of a request.\r\n\r\nThis is usually done automatically, but can also be performed explicitly by setting `response.autoAck = false` and calling `ack()` later. This is useful when a client needs to perform an asynchronous operation to determine if it will accept or reject the request.\r\n\r\n{{#infobox \"info\"}}\r\nRequests count as completed once `send()` or `error()` was called. Calling `ack()` after that won't do anything.\r\n{{/infobox}}\r\n\r\n```javascript\r\nclient.rpc.provide( 'resize-image', ( data, response ) => {\r\n  // Turn of automatic acknowledgements. This needs to happen synchronously\r\n  response.autoAck = false;\r\n\r\n  // Acknowledge the request yourself at a later point\r\n  hasCapacities().then(() => {\r\n    response.ack();\r\n  });\r\n});\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_common_constants_readme.md"}}
{"filePath":"docs/common/constants/readme.md","title":"Constants","description":"A list of all constants deepstream uses","content":"\r\n\r\nConstants are used throughout deepstream. They can be accessed via the module.\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\ndeepstream.CONSTANTS\r\n```\r\n\r\n## General\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- MESSAGE_SEPERATOR | ASCII 30 | ✔ | ✔ |\r\n- MESSAGE_PART_SEPERATOR | ASCII 31 | ✔ | ✔ |\r\n- SOURCE_MESSAGE_CONNECTOR | SOURCE_MESSAGE_CONNECTOR | ✔ |  |\r\n{{/table}}\r\n```\r\n\r\n## Log Level\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- LOG_LEVEL.DEBUG | 0 | ✔ |  |\r\n- LOG_LEVEL.INFO | 1 | ✔ |  |\r\n- LOG_LEVEL.WARN | 2 | ✔ |  |\r\n- LOG_LEVEL.ERROR | 3 | ✔ |  |\r\n- LOG_LEVEL.OFF | 100 | ✔ |  |\r\n{{/table}}\r\n```\r\n\r\n## Server State\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- STATES.STARTING | starting | ✔ |  |\r\n- STATES.INITIALIZED | initialized | ✔ |  |\r\n- STATES.IS_RUNNING | is-running | ✔ |  |\r\n- STATES.CLOSING | closing | ✔ |  |\r\n- STATES.CLOSED | closed | ✔ |  |\r\n{{/table}}\r\n```\r\n\r\n## Connection State\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- CONNECTION_STATE.CLOSED | CLOSED |  | ✔ |\r\n- CONNECTION_STATE.AWAITING_CONNECTION | AWAITING_CONNECTION |  | ✔ |\r\n- CONNECTION_STATE.CHALLENGING | CHALLENGING |  | ✔ |\r\n- CONNECTION_STATE.AWAITING_AUTHENTICATION | AWAITING_AUTHENTICATION |  | ✔ |\r\n- CONNECTION_STATE.AUTHENTICATING | AUTHENTICATING |  | ✔ |\r\n- CONNECTION_STATE.OPEN | OPEN |  | ✔ |\r\n- CONNECTION_STATE.ERROR | ERROR |  | ✔ |\r\n- CONNECTION_STATE.RECONNECTING | RECONNECTING |  | ✔ |\r\n{{/table}}\r\n```\r\n\r\n## Event\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- EVENT.TRIGGER_EVENT | TRIGGER_EVENT | ✔ |  |\r\n- EVENT.INCOMING_CONNECTION | INCOMING_CONNECTION | ✔ |  |\r\n- EVENT.INFO | INFO | ✔ |  |\r\n- EVENT.SUBSCRIBE | SUBSCRIBE | ✔ |  |\r\n- EVENT.UNSUBSCRIBE | UNSUBSCRIBE | ✔ |  |\r\n- EVENT.RECORD_DELETION | RECORD_DELETION | ✔ |  |\r\n- EVENT.INVALID_AUTH_MSG | INVALID_AUTH_MSG | ✔ |  |\r\n- EVENT.INVALID_AUTH_DATA | INVALID_AUTH_DATA | ✔ |  |\r\n- EVENT.AUTH_ATTEMPT | AUTH_ATTEMPT | ✔ |  |\r\n- EVENT.AUTH_ERROR | AUTH_ERROR | ✔ |  |\r\n- EVENT.TOO_MANY_AUTH_ATTEMPTS | TOO_MANY_AUTH_ATTEMPTS | ✔ | ✔ |\r\n- EVENT.AUTH_SUCCESSFUL | AUTH_SUCCESSFUL | ✔ |  |\r\n- EVENT.NOT_AUTHENTICATED | NOT_AUTHENTICATED |  | ✔ |\r\n- EVENT.CONNECTION_ERROR | CONNECTION_ERROR | ✔ | ✔ |\r\n- EVENT.MESSAGE_PERMISSION_ERROR | MESSAGE_PERMISSION_ERROR | ✔ | ✔ |\r\n- EVENT.MESSAGE_PARSE_ERROR | MESSAGE_PARSE_ERROR | ✔ | ✔ |\r\n- EVENT.MAXIMUM_MESSAGE_SIZE_EXCEEDED | MAXIMUM_MESSAGE_SIZE_EXCEEDED | ✔ |  |\r\n- EVENT.MESSAGE_DENIED | MESSAGE_DENIED | ✔ | ✔ |\r\n- EVENT.INVALID_MESSAGE_DATA | INVALID_MESSAGE_DATA | ✔ |  |\r\n- EVENT.UNKNOWN_TOPIC | UNKNOWN_TOPIC | ✔ |  |\r\n- EVENT.UNKNOWN_ACTION | UNKNOWN_ACTION | ✔ |  |\r\n- EVENT.MULTIPLE_SUBSCRIPTIONS | MULTIPLE_SUBSCRIPTIONS | ✔ |  |\r\n- EVENT.NOT_SUBSCRIBED | NOT_SUBSCRIBED | ✔ |  |\r\n- EVENT.LISTENER_EXISTS | LISTENER_EXISTS |  | ✔\r\n- EVENT.NOT_LISTENING | NOT_LISTENING |  | ✔\r\n- EVENT.IS_CLOSED | IS_CLOSED |  | ✔\r\n- EVENT.ACK_TIMEOUT | ACK_TIMEOUT | ✔ | ✔ |\r\n- EVENT.RESPONSE_TIMEOUT | RESPONSE_TIMEOUT | ✔ | ✔ |\r\n- EVENT.DELETE_TIMEOUT | DELETE_TIMEOUT |  | ✔ |\r\n- EVENT.UNSOLICITED_MESSAGE | UNSOLICITED_MESSAGE |  | ✔ |\r\n- EVENT.MULTIPLE_ACK | MULTIPLE_ACK | ✔ |  |\r\n- EVENT.MULTIPLE_RESPONSE | MULTIPLE_RESPONSE | ✔ |  |\r\n- EVENT.NO_RPC_PROVIDER | NO_RPC_PROVIDER | ✔ |  |\r\n- EVENT.RECORD_LOAD_ERROR | RECORD_LOAD_ERROR | ✔ |  |\r\n- EVENT.RECORD_CREATE_ERROR | RECORD_CREATE_ERROR | ✔ |  |\r\n- EVENT.RECORD_UPDATE_ERROR | RECORD_UPDATE_ERROR | ✔ |  |\r\n- EVENT.RECORD_DELETE_ERROR | RECORD_DELETE_ERROR | ✔ |  |\r\n- EVENT.RECORD_SNAPSHOT_ERROR | RECORD_SNAPSHOT_ERROR | ✔ |  |\r\n- EVENT.RECORD_NOT_FOUND | RECORD_NOT_FOUND | ✔ | ✔ |\r\n- EVENT.CACHE_RETRIEVAL_TIMEOUT | CACHE_RETRIEVAL_TIMEOUT | ✔ |  |\r\n- EVENT.STORAGE_RETRIEVAL_TIMEOUT | STORAGE_RETRIEVAL_TIMEOUT | ✔ |  |\r\n- EVENT.CLOSED_SOCKET_INTERACTION | CLOSED_SOCKET_INTERACTION | ✔ |  |\r\n- EVENT.CLIENT_DISCONNECTED | CLIENT_DISCONNECTED | ✔ |  |\r\n- EVENT.INVALID_MESSAGE | INVALID_MESSAGE | ✔ |  |\r\n- EVENT.VERSION_EXISTS | VERSION_EXISTS | ✔ | ✔ |\r\n- EVENT.INVALID_VERSION | INVALID_VERSION | ✔ |  |\r\n- EVENT.PLUGIN_ERROR | PLUGIN_ERROR | ✔ |  |\r\n- EVENT.UNKNOWN_CALLEE | UNKNOWN_CALLEE | ✔ | ✔ |\r\n{{/table}}\r\n```\r\n\r\n## Topic\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- TOPIC.CONNECTION | C | ✔ | ✔ |\r\n- TOPIC.AUTH | A | ✔ | ✔ |\r\n- TOPIC.ERROR | X | ✔ | ✔ |\r\n- TOPIC.EVENT | E | ✔ | ✔ |\r\n- TOPIC.RECORD | R | ✔ | ✔ |\r\n- TOPIC.RPC | P | ✔ | ✔ |\r\n- TOPIC.PRIVATE | PRIVATE/ | ✔ | ✔ |\r\n{{/table}}\r\n```\r\n\r\n## Actions\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- ACTIONS.ACK | A | ✔ | ✔ |\r\n- ACTIONS.READ | R | ✔ | ✔ |\r\n- ACTIONS.REDIRECT | RED |  | ✔ |\r\n- ACTIONS.CHALLENGE | CH |  | ✔ |\r\n- ACTIONS.CHALLENGE_RESPONSE | CHR |  | ✔ |\r\n- ACTIONS.CREATE | C | ✔ | ✔ |\r\n- ACTIONS.UPDATE | U | ✔ | ✔ |\r\n- ACTIONS.PATCH | P | ✔ | ✔ |\r\n- ACTIONS.DELETE | D | ✔ | ✔ |\r\n- ACTIONS.SUBSCRIBE | S | ✔ | ✔ |\r\n- ACTIONS.UNSUBSCRIBE | US | ✔ | ✔ |\r\n- ACTIONS.HAS | H | ✔ | ✔ |\r\n- ACTIONS.SNAPSHOT | SN | ✔ | ✔ |\r\n- ACTIONS.LISTEN_SNAPSHOT | LSN | ✔ |  |\r\n- ACTIONS.LISTEN | L | ✔ | ✔ |\r\n- ACTIONS.UNLISTEN | UL | ✔ | ✔ |\r\n- ACTIONS.SUBSCRIPTIONS_FOR_PATTERN_FOUND | SF | ✔ |  |\r\n- ACTIONS.SUBSCRIPTION_FOR_PATTERN_FOUND | SP | ✔ |  |\r\n- ACTIONS.SUBSCRIPTION_FOR_PATTERN_REMOVED | SR | ✔ |  |\r\n- ACTIONS.PROVIDER_UPDATE | PU | ✔ | ✔ |\r\n- ACTIONS.QUERY | Q | ✔ | ✔ |\r\n- ACTIONS.CREATEORREAD | CR | ✔ | ✔ |\r\n- ACTIONS.EVENT | EVT | ✔ | ✔ |\r\n- ACTIONS.ERROR | E | ✔ | ✔ |\r\n- ACTIONS.REQUEST | REQ | ✔ | ✔|\r\n- ACTIONS.RESPONSE | RES | ✔ | ✔ |\r\n- ACTIONS.REJECTION | REJ | ✔ | ✔ |\r\n{{/table}}\r\n```\r\n\r\n## Data Types\r\n```\r\n{{#table mode=\"pipe\"}}\r\n_default_columns: &COLS [name, value, server, client]\r\nmeta:\r\n  options:\r\n    header: *COLS\r\ncolumns: *COLS\r\nlist:\r\n- TYPES.STRING | S | ✔ | ✔ |\r\n- TYPES.OBJECT | O | ✔ | ✔ |\r\n- TYPES.NUMBER | N | ✔ | ✔ |\r\n- TYPES.NULL | L | ✔ | ✔ |\r\n- TYPES.TRUE | T | ✔ | ✔ |\r\n- TYPES.FALSE | F | ✔ | ✔ |\r\n- TYPES.UNDEFINED | U | ✔ | ✔ |\r\n{{/table}}\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_common_errors_readme.md"}}
{"filePath":"docs/common/errors/readme.md","title":"Errors","description":"The API docs for deepstream's runtime errors","content":"\r\n\r\nErrors are used throughout deepstream. They can be accessed via the module.\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\ndeepstream.CONSTANTS.EVENTS\r\n```\r\n\r\n```\r\n{{#table}}\r\nmeta:\r\n  options:\r\n    header:\r\n      - name\r\n      - description\r\n      - server\r\n      - client\r\nlist:\r\n-\r\n  name: CONNECTION_ERROR\r\n  description: Either the browser or TCP connection has encountered an error.\r\n  server: ''\r\n  client: ✔\r\n\r\n-\r\n  name: UNSOLICITED_MESSAGE\r\n  description: A message was received that the client didn't expect, e.g. an update for a record that the client isn't subscribed to. This doesn't necessarily have to be an error, but can also be the result of messages crossing on the wire, e.g. when an outgoing record discard and an incoming message overlap.\r\n  server: ''\r\n  client: ✔\r\n\r\n-\r\n  name: MESSAGE_PARSE_ERROR\r\n  description: The client has received a syntactically incorrect message.\r\n  server: ✔\r\n  client: ✔\r\n\r\n-\r\n  name: IS_CLOSED\r\n  description: Emitted when the client tries to authenticate against an already closed connection.\r\n  server: ''\r\n  client: ✔\r\n\r\n-\r\n  name: VERSION_EXISTS\r\n  description: The client has tried to update a record to a version that the server already has. This might happen if multiple clients try to update the same record at the same time. This error will also be emitted by the affected Record. To mitigate this error, configure a [merge strategy](/tutorials/core/handling-data-conflicts/)\r\n  server: ✔\r\n  client: ✔\r\n\r\n-\r\n  name: NOT_AUTHENTICATED\r\n  description: Emitted if an operation is attempted before the client is authenticated (before <code>login()</code> has been called and a response was received).\r\n  server: ''\r\n  client: ✔\r\n\r\n-\r\n  name: ACK_TIMEOUT\r\n  description: The acknowledgement response for a record subscription, event subscription or rpc call hasn't been received in time. This error is also emitted by the object that encountered it, e.g. the Record or Rpc.\r\n  server: ✔\r\n  client: ✔\r\n\r\n-\r\n  name: LISTENER_EXISTS\r\n  description: Emitted when <code>client.record.listen( pattern, callback )</code> is called more than once for the same pattern.\r\n  server: ''\r\n  client: ✔\r\n\r\n-\r\n  name: NOT_LISTENING\r\n  description: Emitted when <code>client.record.unlisten( pattern )</code> is called for a pattern that no listener exists for.\r\n  server: ''\r\n  client: ✔\r\n\r\n-\r\n  name: TOO_MANY_AUTH_ATTEMPTS\r\n  description: Emitted when the client has made more invalid authentication attempts than the server accepts. This can be configured as maxAuthAttempts on the server.\r\n  client: ✔\r\n  server: ✔\r\n{{/table}}\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_server_command-line-interface_readme.md"}}
{"filePath":"docs/server/command-line-interface/readme.md","title":"Command Line Interface","description":"The options that can be passed to the deepstream server via the command line","content":"\r\n\r\ndeepstream comes with a comprehensive command line interface (CLI) that lets you start or stop the server, install connectors or override configuration options.\r\n\r\nMany of these options can also be set via the configuration file, read [config file documentation](/docs/server/configuration/). for a detailed list.\r\n\r\n## Usage\r\nIf you've [installed](/install/) deepstream on linux via a package manager, the `deepstream` command is already on your path. On Mac and Windows, you can access it through the executable, e.g. `./deepstream` or `deepstream.exe`\r\n\r\nFor the brave souls who've got deepstream via [Github](https://github.com/deepstreamIO/deepstream.io) or [NPM](https://www.npmjs.com/package/deepstream.io) instead, you can find the executable in `bin/deepstream`\r\n\r\nLet's start simple:\r\n\r\n```bash\r\ndeepstream --help\r\n```\r\n\r\nwill print out all available commands:\r\n\r\n```bash\r\n  Usage: deepstream [command]\r\n\r\n\r\n  Commands:\r\n\r\n    start [options]            start a deepstream server\r\n    stop                       stop a running deepstream server\r\n    status                     display if a deepstream server is running\r\n    install [options]          install connectors\r\n    info [options]             print meta information about build and runtime\r\n    hash [options] [password]  Generate a hash from a plain text password using file auth configuration settings\r\n\r\n  Options:\r\n\r\n    -h, --help     output usage information\r\n    -V, --version  output the version number\r\n```\r\n\r\nYou can pass the `--help` option to individual commands as well.\r\n\r\n\r\n### deepstream start\r\n\r\n```bash\r\ndeepstream start --help\r\n```\r\n\r\nwill print all the options you can specify for the deepstream server:\r\n\r\n```bash\r\n  Usage: start [options]\r\n\r\n  start a deepstream server\r\n\r\n  Options:\r\n\r\n    -h, --help                         output usage information\r\n    -c, --config [file]                configuration file, parent directory will be used as prefix for other config files\r\n    -l, --lib-dir [directory]          path where to lookup for plugins like connectors and logger\r\n    --server-name <name>               Each server within a cluster needs a unique name\r\n    --web-server-enabled [true|false]  Accept/Decline incoming HTTP connections\r\n    --tcp-server-enabled [true|false]  Accept/Decline incoming TCP connections\r\n    --host <host>                      host for the HTTP/websocket server\r\n    --port <port>                      port for the HTTP/websocket server\r\n    --tcp-host <host>                  host for the TCP server\r\n    --tcp-port <port>                  tcpHost\r\n    --disable-auth                     Force deepstream to use \"none\" auth type\r\n    --disable-permissions              Force deepstream to use \"none\" permissions\r\n    --log-level <level>                Log messages with this level and above\r\n    --colors [true|false]              Enable or disable logging with colors\r\n```\r\n\r\nYou can either use the shorthand syntax with one hyphen or the long syntax with two hyphens.\r\n\r\nAll the options starting from `--server-name` and below will overwrite the values within your config file if. If you want to specify more options, or see the default values provided please look at the [config file documentation](../configuration/).\r\n\r\n#### --config\r\n\r\nIf you installed it via a linux package manager it will default to _/etc/deepstream_.\r\nOtherwise the paths are relative to your current working directory.\r\n\r\n#### --lib-dir\r\n\r\nIf you installed it via a linux package manager it will default to _/var/lib/deepstream_.\r\nOtherwise the paths are relative to your current working directory.\r\n\r\n### deepstream stop\r\n\r\nIf you've started deepstream in another terminal or in the background this will stop the server.\r\n\r\n### deepstream status\r\n\r\nThis command will print uptime and status for a deepstream server previously started via the CLI.\r\n\r\n### deepstream install\r\n\r\nThis command allows you to install connectors for deepstream.io. Append `--help` to get some examples.\r\n\r\n```bash\r\n  Usage: install <type> <name>[:version]\r\n\r\n  install connectors\r\n\r\n  Options:\r\n\r\n    -h, --help                 output usage information\r\n    -l, --lib-dir [directory]  directory where to extract the connector, defaults to ./lib\r\n    -c, --config [file]        the configuration file containing\r\n    the lib-dir as an option\r\n    --verbose                  more debug output\r\n    --quiet                    no output\r\n\r\n  Examples:\r\n\r\n    $ deepstream install cache redis\r\n    $ deepstream install storage rethinkdb:0.1.0\r\n\r\n    list of available connectors: https://deepstream.io/download\r\n```\r\n\r\nThe version is optional. If omitted deepstream will fetch the latest release.\r\n\r\n### deepstream info\r\nDisplays information, about deepstream's version and system architecture.\r\nIf you'd like to raise an issue on Github we'd appreciate if you could paste attach it.\r\n\r\nOutput example\r\n\r\n```json\r\n{\r\n  \"deepstreamVersion\": \"1.0.0-rc.3\",\r\n  \"gitRef\": \"d638e19f6e081601add6b98270f64acde80243ca\",\r\n  \"buildTime\": \"Mon Jul 04 2016 11:12:31 GMT+0200 (CEST)\",\r\n  \"platform\": \"darwin\",\r\n  \"arch\": \"x64\",\r\n  \"nodeVersion\": \"v4.4.5\",\r\n  \"libs\": [\r\n    \"deepstream.io-logger-winston:1.1.0\",\r\n    \"uws:0.6.5\"\r\n  ]\r\n}\r\n```\r\n\r\n### deepstream hash\r\n\r\nThis command allows you generate a hash for a plaintext password. Show the usage by appending `--help` to that command:\r\n\r\n```bash\r\n  Usage: hash [options] [password]\r\n\r\n  Generate a hash from a plaintext password using file auth configuration settings\r\n\r\n  Options:\r\n\r\n    -h, --help           output usage information\r\n    -c, --config [file]  configuration file containing file auth and hash settings\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_server_configuration_readme.md"}}
{"filePath":"docs/server/configuration/readme.md","title":"Configuration","description":"The available configuration options to customise deepstream","content":"\r\n\r\nYou can make any configuration changes you need for your Deepstream setup in the *config.yml* file. You can find this in the */etc/deepstream/conf/* directory on Linux and in *${DEEPSTREAM_DIR}/conf/* on Mac and Windows.\r\n\r\n## General Configuration\r\n\r\nIn this section you can change general settings for each server in a cluster.\r\n\r\n### serverName\r\nEvery server in a cluster of servers needs a unique name. You can add your own or set it to `UUID` to let Deepstream auto-generate a unique ID.<br>\r\n_Default_: `UUID`\r\n\r\n### showLogo\r\nWhen starting, a server can show the Deepstream logo. This setting is best left enabled.<br>\r\n_Default_: `true`\r\n\r\n## Connectivity and Networking\r\n\r\nIn this section you can change, enable and disable networking services and details for them.\r\n\r\n### webServerEnabled\r\nSets if this server should accept incoming HTTP long-polling and websocket connections.<br>\r\n_Default_: `true`\r\n\r\n### tcpServerEnabled\r\nSets if this server should accept incoming TCP connections.<br>\r\n_Default_: `true`\r\n\r\n### port\r\nSets the port for the HTTP and Websocket server.<br>\r\n_Default_: `6020`\r\n\r\n### host\r\nSets the host for the HTTP and Websocket server.<br>\r\n_Default_: `0.0.0.0`\r\n\r\n### tcpPort\r\nSets the port of the TCP server.<br>\r\n_Default_: `6021`\r\n\r\n### tcpHost\r\nSets the host for the TCP server.<br>\r\n_Default_: `0.0.0.0`\r\n\r\n### urlPath\r\nSets which url path HTTP and Websocket connections should connect to.<br>\r\n_Default_: `/deepstream`\r\n\r\n## SSL Configuration\r\n\r\nIf you need secure connections to a server, configure SSL details here.\r\n\r\n### sslKey\r\nThe path to your SSL key file.<br>\r\n_Default_: `null`\r\n\r\n### sslCert\r\nThe path to your SSL certificate file.<br>\r\n_Default_: `null`\r\n\r\n### sslCa\r\nThe path to your SSL certificate authority file.<br>\r\n_Default_: `null`\r\n\r\n## Plugin Configuration\r\n\r\nYou can extend Deepstream with plugins for connectors to other services, these are for logging, storage engines, caching layers and message systems.\r\n\r\nTo enable a plugin, uncomment the relevant category key underneath the `plugins` key. Each plugin type has a path or name, and a set of options.\r\n\r\n### path\r\nSet a path to a JavaScript file, node module or a folder with an _index.js_ file which exports a constructor.\r\n\r\n### name\r\nIf you are using any of the official Deepstream connectors, add the name of what the plugin connects to here, for example `redis`.\r\n\r\n**Note**: You can set `path` **or** name, but not both.\r\n\r\n### options\r\nUnder this key, add sub key/value pairs to set the configuration options that are passed to the plugin. Each plugin should mention what configuration options you can set.\r\n\r\n## Logger\r\n\r\nDeepstream uses by default a logger which prints out to _stdout_ (errors and warnings to _stderr_). You can set these options for the default logger by using the same configuration style for the plugins:\r\n\r\n```yaml\r\nlogger:\r\n  name: default\r\n  options:\r\n    colors: true\r\n    logLevel: INFO\r\n```\r\n\r\n### colors\r\nSets whether the server's logs should output in color. This will look great in a console, but will leave color markers in log files if you redirect the output into a file.<br>\r\n_Default_: `true`\r\n\r\n### logLevel\r\nSets at what level and above the server should log messages. Valid levels are `DEBUG`, `INFO`, `WARN`, `ERROR`, and `OFF`.<br>\r\n_Default_: `INFO`\r\n\r\n## Storage Options\r\n\r\n### storageExclusion\r\nA regular expression that - if it matches a recordname - will prevent the record from being stored in the database.<br>\r\n_Default_: `null`\r\n\r\n## Security\r\n\r\nIn this section you can configure security settings for access and communication.\r\n\r\n### maxAuthAttempts\r\nThe number of invalid login attempts before the connection to the server is closed.<br>\r\n_Default_: `3`\r\n\r\n### logInvalidAuthData\r\nSets if logs should contain the cleartext usernames and passwords of invalid login attempts.<br>\r\n_Default_: `true`\r\n\r\n### maxMessageSize\r\nSets the maximum message size allowed to be sent to the server (in bytes).<br>\r\n_Default_: `1048576`\r\n\r\n## Authentication\r\n\r\nIn this section you can configure the authentication type the server uses.\r\n\r\nYou set the authentication type as a sub key the `auth` key. The authentication options are `none`, `file`, and `http` and come with their respective sets of `options`.\r\n\r\nEach of these requires reasonable explanation, so see the specific section of our docs for more details.\r\n\r\n```yaml\r\n#Authentication\r\nauth:\r\n  type: none\r\n  options: depends\r\n```\r\n\r\n_Default_: `none`\r\n\r\n## Permissioning\r\n\r\nIn this section you can configure the location of the permissions file that sets levels of access users and actions have to the server.\r\n\r\nAt the moment, you can create your own custom permission handler, or use a configuration file with the `config` option.\r\nIf you use the `config` option, then use the `options` sub key to configure it.\r\n\r\n### path\r\nSet the path to the file that declares permissions. This can be in json, js or yaml format. You can find more details on what's possible with this file [here](#).<br>\r\n_Default_: `./permissions.json`\r\n\r\n### maxRuleIterations\r\nThe Deepstream permissions model allows for some complex nested actions and queries. To prevent a performance hit you can limit the nesting level with this option.<br>\r\n_Default_: `3`\r\n\r\n### cacheEvacuationInterval\r\nThe results of permission checks are cached to improve performance. Use this setting to change the time interval (in milliseconds) that the cache is regenerated.<br>\r\n_Default_: `60000`\r\n\r\n## Timeouts (in milliseconds)\r\n\r\nIn this section you can configure timeout values for a variety of network calls.\r\n\r\n### rpcProviderQueryTimeout\r\nSets how long deepstream will wait for responses after querying for RPC provider<br>\r\n_Default_:`1000`\r\n\r\n### rpcProviderCacheTime\r\nSets how long deepstream remembers your registered RPC provider before asking for it again.<br>\r\n_Default_:`60000`\r\n\r\n### rpcAckTimeout\r\nSets how long Deepstream will wait for a RPC provider to acknowledge receipt of a request.<br>\r\n_Default_:`1000`\r\n\r\n### rpcTimeout\r\nSets how long deepstream will wait for RPCs to complete.<br>\r\n_Default_:`10000`\r\n\r\n### cacheRetrievalTimeout\r\nSets how long Deepstream will wait when retrieving values from the cache.<br>\r\n_Default_:`1000`\r\n\r\n### storageRetrievalTimeout\r\nSets how long Deepstream will wait when retrieving values from the database.<br>\r\n_Default_:`2000`\r\n\r\n### dependencyInitialisationTimeout\r\nSets how long Deepstream will wait for dependencies to initialize.<br>\r\n_Default_:`2000`\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_server_data-transforms_readme.md"}}
{"filePath":"docs/server/data-transforms/readme.md","title":"Transforming outgoing data","description":"Learn how to use transform functions to manipulate data before it leaves the server","content":"\r\n\r\nTransform-functions are registered using `server.set('dataTransforms', [...])`, for more details see the [Node.js api](../node-api/)\r\n\r\n{{#infobox \"important\" \"BUT ☝\"}}\r\nTransforming data slows deepstream down quite a bit. Usually, messages are constructed once and fanned out to all subscribed clients. If a transform function is registered however, messages are constructed for every receiver specifically which can add considerable overhead.\r\n\r\nSo: Use with caution and do as little as possible in your transform function.\r\n\r\nAlso, structure your data in a way that data is seperated by concern.\r\n\r\nFor example, if you have a user with admin and readonly data, have two seperate records called `user-admin/<id>` and `user/<id>` which you can then easily permission using [Valve](../valve-permissions/) instead.\r\n{{/infobox}}\r\n\r\n### RPC\r\n\r\n#### REQUEST\r\nMessage from server to RPC provider, result of `client.rpc.make(name, data, callback)`<br>\r\n\r\n- sender\r\n- reciever\r\n- rpcName\r\n\r\n```javascript\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.RPC,\r\n  action: C.ACTIONS.REQUEST,\r\n  transform: function(data, metaData) {\r\n    if (metaData.rpcName === 'get-price') {\r\n      data.discount = userDiscounts[metaData.sender]\r\n    }\r\n    return data\r\n  }\r\n}])\r\n```\r\n\r\n#### RESPONSE\r\nResponse from RPC provider on the way back to its requestor, result of `response.send(data)`\r\n\r\n- sender\r\n- reciever\r\n- rpcName\r\n\r\n```javascript\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.RPC,\r\n  action: C.ACTIONS.RESPONSE,\r\n  transform: function(data, metaData) {\r\n    if (metaData.rpcName === 'get-price') {\r\n      data.price = data.price * userDiscounts[metaData.sender]\r\n    }\r\n    return data\r\n  }\r\n}])\r\n```\r\n\r\n### EVENT\r\nData associated with an event send with `client.event.emit(name, data)`\r\n\r\n- sender\r\n- reciever\r\n- eventName\r\n\r\n```javascript\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.EVENT,\r\n  action: C.ACTIONS.EVENT,\r\n  transform: function(data, metaData) {\r\n    if (metaData.eventName === 'news') {\r\n      data.news = data.news.replace('insult', '#@#@#@')\r\n    }\r\n    return data\r\n  }\r\n}])\r\n```\r\n\r\n### RECORD\r\n\r\n#### READ\r\nResponse to a record subscription made with `client.record.getRecord()`\r\n\r\n- recordName\r\n- version\r\n- reciever\r\n\r\n```javascript\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.RECORD,\r\n  action: C.ACTIONS.READ,\r\n  transform: function(data, metaData) {\r\n    if (metaData.recordName.substr(0, 5) === 'item/') {\r\n      return data.price * userDiscounts[metaData.receiver]\r\n    }\r\n    return data\r\n  }\r\n}])\r\n```\r\n<br/>\r\n{{#infobox \"important\" \"Important!\"}}\r\nNote how read and update both have the same code path. This is because in both cases the entire data is sent, it is only very rarely that you would need different content between both actions.\r\n{{/infobox}}\r\n\r\n#### UPDATE\r\nFull record update, result of `client.record.set(data)`\r\n\r\n- recordName\r\n- version\r\n- reciever\r\n\r\n```javascript\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.RECORD,\r\n  action: C.ACTIONS.READ,\r\n  transform: function(data, metaData) {\r\n    if (metaData.recordName.substr(0, 5) === 'item/') {\r\n      return data.price * userDiscounts[metaData.receiver]\r\n    }\r\n    return data\r\n  }\r\n}])\r\n```\r\n\r\n#### PATCH\r\nPartial record update, result of `client.record.set(path, value)`\r\n\r\n- recordName\r\n- version\r\n- reciever\r\n- path\r\n\r\n```javascript\r\nserver.set('dataTransforms', [{\r\n  topic: C.TOPIC.RECORD,\r\n  action: C.ACTIONS.PATCH,\r\n  transform: function(data, metaData) {\r\n    if (\r\n      metaData.recordName.substr(0, 5) === 'item/' &&\r\n      metaData.path === 'price'\r\n    ) {\r\n      //data for PATCH is just the price\r\n      return data * userDiscounts[metaData.receiver]\r\n    } else {\r\n      return data\r\n    }\r\n  }\r\n}])\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_server_node-api_readme.md"}}
{"filePath":"docs/server/node-api/readme.md","title":"Node API","description":"API docs for using deepstream within your own node application","content":"\r\n\r\nAPI when using deepstream as a Node.js package via NPM.\r\n\r\n### `constructor(options)`\r\nInstantiate a new deepstream server instance. You can pass an optional object\r\nwhich contains the configuration or a filePath to the configuration file. Missing options will be merged with default values.\r\n\r\nIf you omit the argument, deepstream will use default values. Read more about\r\nthe [configuration and default values](/docs/server/configuration/).\r\n\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: options\r\n  typ: Object | String\r\n  opt: true\r\n  des: Either the configuration object or a filepath to the configuration file\r\n{{/table}}\r\n\r\n**Please note** calling `server = new Deepstream()` only creates the instance, to actually start the server, you still need to call `server.start();`\r\n\r\n```javascript\r\nconst Deepstream = require('deepstream.io')\r\nconst server = new Deepstream({port:8000})\r\n```\r\n\r\n## Events\r\n\r\n### `started`\r\nEmitted once `deepstream.start()` has been called and the startup procedure has completed succesfully.\r\n\r\n### `stopped`\r\nEmitted once `deepstream.stop()` has been called and the server has been completely shut down.\r\n\r\n---\r\n\r\n## Methods\r\n\r\n### `start()`\r\nStarts the server.\r\n\r\n### `stop()`\r\nStops the server.\r\n\r\n### `set(key, value)`\r\nThis method allows you to overwrite particular configuration options which were built via the\r\nconfiguration initialization step.\r\n\r\n💡 **NOTE:** If deepstream is initialized with a configuration object, `set()` will override the keys in your initial configuration. This is useful for passing in objects which are shared between deepstream and the rest of your application, such as a HTTPServer or cache connector. You can override any of the options using the same name within your [configuration](/docs/server/configuration/), except for the notable difference(s) below.\r\n\r\n```\r\n{{#table mode=\"api\"}}\r\n-\r\n  arg: key\r\n  typ: String\r\n  opt: false\r\n  des: The configuration option that should be set\r\n-\r\n  arg: value\r\n  typ: various\r\n  opt: false\r\n  des: The value that should be used\r\n{{/table}}\r\n```\r\n\r\n##### Differences when using `set(key, value)`\r\n\r\nIf you use an configuration object its properties will be treated as a file path.\r\nHere the `value` is treated as a string for these options:\r\n\r\n- `sslCert`\r\n- `sslKey`\r\n- `sslCA`\r\n\r\nActually these options can be passed by an configuration object, but if you use a file based configuration it only works with a `.js` file. YAML and JSON config files are not supporting these options.\r\n\r\n- `httpServer`\r\n- `dataTransforms`\r\n\r\nThese options might have a different name and location in the structure of the configuration object. If you use `set()` you also need to provide the instantiated instance as the `value`.\r\n\r\n- `authenticationHandler`\r\n- `permissionHandler`\r\n- `logger`\r\n- `message`\r\n- `cache`\r\n- `storage`\r\n\r\nMake sure you run `server.start()` after you set all your options.\r\n\r\nSome examples:\r\n\r\n```javascript\r\n\r\n/**\r\n* An existing http server to listen to rather\r\n* then letting deepstream create its own.\r\n*\r\n* Note: If webServerEnabled is false it will ignore\r\n* the passed in httpServer\r\n*\r\n* @type net.HttpServer || net.HttpsServer\r\n* @default true\r\n*/\r\nserver.set('httpServer', httpServer)\r\n\r\n/**\r\n* The public key to use if using ssl\r\n* Must have an associated sslKey set\r\n*\r\n* @type String\r\n*/\r\nserver.set('sslCert', fs.readFileSync('./keys/cert.pem', 'utf8'))\r\n\r\n\r\n/**\r\n* An object that exposes a isValidUser function.\r\n*\r\n* @type authenticationHandler\r\n* @default OpenPermissionHandler (same as `{auth:{type: none}}` in the default config)\r\n*/\r\nserver.set('authenticationHandler', new OAuthHandler())\r\n\r\n/**\r\n* An object that that exposes a canPerformAction function.\r\n*\r\n* @type permissionHandler\r\n* @default ConfigPermissionHandler (with arguments from the default config)\r\n*/\r\nserver.set('permissionHandler', new LdapPermissionHandler())\r\n\r\n\r\n/**\r\n* Transforms data before it leaves the server.\r\n* See //deepstream.io/tutorials/transforming-data.html for details\r\n*\r\n* @type Array\r\n* @default null\r\n*/\r\nserver.set('dataTransforms', [{\r\n\ttopic: C.TOPIC.RPC,\r\n\taction: C.ACTIONS.REQUEST,\r\n\ttransform: function(data, metaData) {}\r\n}])\r\n\r\n/**\r\n* A logger\r\n*\r\n* @type Logger\r\n* @default DeepstreamWinstonLogger\r\n*/\r\nserver.set('logger', new FileLogger())\r\n\r\n/**\r\n* MessageConnectors connect deepstream to a message bus\r\n* (e.g. AMQP, Redis, Kafka) thus allowing for clustering.\r\n*\r\n* See the \"Connectors\" section above for details\r\n*\r\n* @type MessageConnector\r\n* @default NoopMessageConnector\r\n*/\r\nserver.set('messageConnector', new RedisMessageConnector({\r\n  port: 6379,\r\n  host: 'localhost'\r\n}))\r\n\r\n\r\n/**\r\n* A regular expression that determines which records will not be\r\n* stored in the database.\r\n*\r\n* This is useful to improve performance for fast-updating records\r\n* that do not need to be stored in the long-term, e.g. stock prices\r\n*\r\n* Any record whose name matches the specified RegExp will be read / written\r\n* directly to cache\r\n*\r\n* @type RegExp\r\n* @default null\r\n*/\r\nserver.set('storageExclusion', /^dont-store\\/*./)\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_server_user-file_readme.md"}}
{"filePath":"docs/server/user-file/readme.md","title":"User File","description":"The API on how you can configure and use file based authentication","content":"\r\n\r\nThis file is used in conjunction with [file-based authentication](/tutorials/core/auth-file/). It is structured as a map of usernames and their associated passwords and optional auth data.\r\n\r\nThe userfile can be written in YAML or JSON.\r\n\r\n```yaml\r\n# Username as key\r\njohndoe:\r\n  # Password as hash if auth.options.has is configured or in cleartext\r\n  password: uY2zMQZXcFuWKeX/6eY43w==9wSp046KHAQfbvKcKgvwNA==\r\n  # Optional auth data that will be passed to permissioning as user.data\r\n  data:\r\n    role: admin\r\n\r\nsamjones:\r\n  password: 7KZrUQcnFUDNOQtqtKqhCA==ElDieSHdI2vtiws41JF/HQ==\r\n  data:\r\n    role: user\r\n```\r\n\r\nor in JSON\r\n\r\n```json\r\n{\r\n    \"johndoe\": {\r\n        \"password\": \"uY2zMQZXcFuWKeX/6eY43w==9wSp046KHAQfbvKcKgvwNA==\",\r\n        \"data\": { \"role\": \"admin\" }\r\n    },\r\n    \"samjones\": {\r\n        \"password\": \"7KZrUQcnFUDNOQtqtKqhCA==ElDieSHdI2vtiws41JF/HQ==\",\r\n        \"data\": { \"role\": \"user\" }\r\n    }\r\n}\r\n```\r\n\r\n\r\n### password\r\nCan either be the user's plaintext password or a hash of the password\r\n\r\nIf you've configured file based authentication as follows, use plaintext passwords:\r\n\r\n```yaml\r\n   type: file\r\n   options:\r\n     path: ./users.yml\r\n     hash: false\r\n```\r\n\r\nIf you've configured a hashing algorithm, use hashes as password:\r\n\r\n```yaml\r\n   type: file\r\n   options:\r\n     path: ./users.yml\r\n     hash: 'md5'\r\n     iterations: 50\r\n     keyLength: 16\r\n\r\n```\r\n<br/>\r\n{{#infobox \"hint\"}}\r\nYou can create hashes from passwords with the currently specified settings using deepstream's command line interface\r\n{{/infobox}}\r\n\r\n```bash\r\ndeepstream hash <password>\r\n```\r\n\r\ne.g.\r\n\r\n![deepstream hash console output](ds-hash-output.png)\r\n\r\n### data\r\nOptional authentication data, e.g. `role: admin` or `canCreatePosts: true`. This data will be available in your permission rules as `user.data`. Data stored within clientData will also be forwarded to the client as part of the login process.\r\n\r\n```json\r\n{\r\n    \"samjones\": {\r\n        \"password\": \"7KZrUQcnFUDNOQtqtKqhCA==ElDieSHdI2vtiws41JF/HQ==\",\r\n        \"data\": {\r\n          \"clientData\": { \"user-alias\": \"sammy\" },\r\n          \"role\": \"user\"\r\n        }\r\n    }\r\n}\r\n```\r\n"}
{"index":{"_index":"pages","_type":"docs","_id":"docs_server_valve-permissions_readme.md"}}
{"filePath":"docs/server/valve-permissions/readme.md","title":"Valve Permissions","description":"The API for Valve, deepstream's powerful permissioning mechanism","content":"\r\n\r\n## Rule Types\r\nYou can specify permission rules for the following interactions\r\n\r\n### record\r\n- `create` triggered when a record is requested for the first time\r\n- `write` operations that change a record's data. (PATCH & UPDATE)\r\n- `read` reading a record's data\r\n- `delete` deleting a record\r\n- `listen` listen for other clients subscribing to a record\r\n\r\n### event\r\n- `publish` sending events\r\n- `subscribe` subscribing for events\r\n- `listen` listen for other clients subscribing to events\r\n\r\n### rpc\r\n- `provide` registering a client as a RPC provider\r\n- `request` making a remote procedure call\r\n\r\n## Variables\r\nThese variables are available for use within a permission rule\r\n\r\n### user\r\nthe authentication data for the user attempting the read or write, containing the following keys:\r\n\r\n```javascript\r\n{\r\n    //Boolean, false if username === 'open'\r\n    isAuthenticated: true, //Boolean\r\n    //the userid / username as returned by auth the auth provider\r\n    id: 'johndoe', //String\r\n    //optional object, containing fields like e.g. role, access level etc\r\n    //returned by auth provider\r\n    data: { role: 'admin' } //Object\r\n}\r\n```\r\n\r\n**Usage Example:** write to record `user-profile` is only allowed for owner\r\n```yaml\r\nrecord:\r\n  user-profile/$username:\r\n    write: \"user.id === $username\"\r\n```\r\n\r\n### data\r\nthe incoming data for records, events and rpcs\r\n\r\n**Usage Example:** only allow publishing of event if it has more than 50 likes\r\n```yaml\r\nevent:\r\n  facebook-news:\r\n    publish: \"data.likes > 50\"\r\n```\r\n\r\n### oldData\r\nthe current data, only for records\r\n\r\n**Usage Example:** Only allow bids higher than the current price\r\n```yaml\r\nrecord:\r\n  item/*:\r\n    write: \"data.bid > oldData.bid\"\r\n```\r\n\r\n### now\r\ncurrent timestamp on the server in ms\r\n\r\n**Usage Example:** Only allow scheduling appointments in the future\r\n```yaml\r\nrpc:\r\n  schedule-appointment:\r\n    request: \"data.desiredDate > now\"\r\n```\r\n\r\n### action\r\nthe original action that triggered this rule (e.g. UPDATE / PATCH / LISTEN ) etc. Useful for more finegrained/low-level permissions. You can find a list of all available actions [here](/docs/common/constants/)\r\n\r\n**Usage Example:** Only allow patch updates\r\n```yaml\r\nrecord:\r\n  user-profile/:\r\n    write: \"data.action === 'PATCH'\"\r\n```\r\n\r\n### $variableName\r\nVariables that are extruded from the record / event / rpc name. Names can contain multiple variables. Variable names start with a dollar and are only allowed to contain uppercase letters, lowercase letters and numbers.\r\n\r\n**Usage Examples:**\r\n```yaml\r\nrecord:\r\n  user-profile/$userId:\r\n    # make sure users can only manipulate their own profile\r\n    write: \"$userId === user.id\"\r\nevent:\r\n  # Make sure the headline for `pet-news/pugs` contains the word pug\r\n  pet-news/$pet:\r\n    publish: \"data.headline.indexOf( $pet ) !== -1\"\r\n\r\n```\r\n\r\n## Cross reference\r\n\r\n### _(recordName)\r\n\r\nOnly for records. Cross-references another record and makes the other record's data available for the permission rule.\r\n\r\n**Usage Example:**\r\n```yaml\r\nrecord:\r\n  car-sale/$transactionId:\r\n    # when booking a new car sale, make sure that\r\n    # the car that's sold exists and that its price\r\n    # is the same or lower than what the customer is charged\r\n    write: \"_(data.carId) !== null && _(data.carId).price >= data.price\"\r\n\r\n```\r\n\r\n## String functions\r\nValve supports the following string functions\r\n- `startsWith`\r\n- `endsWith`\r\n- `indexOf`\r\n- `match`\r\n- `toUpperCase`\r\n- `toLowerCase`\r\n- `trim`\r\n\r\n**Usage Example:** make sure a postcode only contains numbers\r\n```yaml\r\nrecord:\r\n    address/*:\r\n      write: \"data.postcode && data.postcode.match( /^[0-9]*$/ )\"\r\n```\r\n"}
{"index":{"_index":"pages","_type":"info","_id":"info_community_contribution-guidelines_readme.md"}}
{"filePath":"info/community/contribution-guidelines/readme.md","title":"Open Source Contribution Guidelines","description":"Find out how you can contribute to deepstream","content":"\r\n\r\nWe love getting feedback and contributions from the opensource community. Please try to adhere to this guide when reporting any issues or making any changes.\r\n\r\n### PRs and Code Contributions\r\n\r\n* Tests must pass and not reduce coverage\r\n* Any changes, bug fixes or new features must include relevant tests\r\n* Pull requests should be made to the master branch.\r\n* Before we can accept your pull requests, you need to sign our [Contributor License Agreement](../cla/)\r\n\r\n## Ways to contribute\r\n\r\n### Logging Issues\r\n\r\nAll issues should include the deepstream.io and client language and version.\r\n\r\nFor issues related to the deepstream.io server, please include the results of running:\r\n\r\n```bash\r\ndeepstream info\r\n```\r\n\r\nWhich will print out something similar to this:\r\n\r\n```json\r\n{\r\n  \"deepstreamVersion\": \"1.0.0-rc.3\",\r\n  \"gitRef\": \"d638e19f6e081601add6b98270f64acde80243ca\",\r\n  \"buildTime\": \"Mon Jul 04 2016 11:12:31 GMT+0200 (CEST)\",\r\n  \"platform\": \"darwin\",\r\n  \"arch\": \"x64\",\r\n  \"nodeVersion\": \"v4.4.5\",\r\n  \"libs\": [\r\n    \"deepstream.io-logger-winston:1.1.0\",\r\n    \"uws:0.6.5\"\r\n  ]\r\n}\r\n```\r\n\r\n#### Website\r\nWe're always looking to improve documentation, weed out those typos that sneak past us and add tutorials on the plethora of things we can integrate. If you feel anything needs to be clarified, would like to write/see a guide on how to use it with your favourite framework or anything else just raise an issue [here](//github.com/deepstreamIO/deepstream.io-website)\r\n\r\n#### Clients\r\nDeepstream uses a [minimal text based protocol](../../protocol/message-structure-overview/), and as such can work with any language on an internet enabled device. Want to support your language of choice? Join the conversations [on github](//github.com/deepstreamIO/deepstream.io/issues?q=is%3Aopen+is%3Aissue+label%3Anew-client) or [write your own](/tutorials/core/writing-a-client/). There are quite a few being built by the community and I'm sure they would love more collaborators!\r\n\r\nFeel free to [contact us](../get-in-touch/) if you have any questions."}
{"index":{"_index":"pages","_type":"info","_id":"info_community_get-in-touch_readme.md"}}
{"filePath":"info/community/get-in-touch/readme.md","title":"Getting in touch","description":"Find out how to get in touch with other deepstream aficionados","content":"\r\n\r\nNeed help with a deepstream.io problem, found an issue or want to become a contributer? Join our lovely and helpful community:\r\n\r\n![Some peeps with macbooks](community.png)\r\n\r\n## Join us on Slack\r\nThere should always be someone around on the [deepstream.io slack channel](https://deepstream-slack.herokuapp.com/)\r\n\r\n## Drop by our Github\r\nWe use [Github](https://github.com/deepstreamIO) as the home for all of our open source code, issues and discussions. You should come by some time.\r\n\r\n## Other ways to get in touch\r\n* Meet us at an [event](../events/)\r\n* Follow [@deepstreamIO](https://twitter.com/deepstreamIO) on Twitter\r\n* Ask a question tagged with `deepstream.io` on [stack overflow](http://stackoverflow.com/questions/tagged/deepstream.io)\r\n* Send us an email at [info@deepstream.io](mailto:info@deepstream.io)\r\n* Keep it professional on [LinkedIn](https://www.linkedin.com/company/deepstreamhub)\r\n* Or stay casual on [Facebook](https://www.facebook.com/deepstreamIO/)"}
{"index":{"_index":"pages","_type":"info","_id":"info_community_t-shirt_readme.md"}}
{"filePath":"info/community/t-shirt/readme.md","title":"The 2016 t-shirt collection","description":"Find out how to apply for an awesome t-shirt, featuring Elton!","content":"\r\n\r\nAh, the classic story for t-shirt deal: You tell us what you've build with deepstream and you get your pick from the highly exclusive and coveted deepstream.io 2016 spring collection:\r\n\r\n![deepstream.io 2016 spring collection](tshirt.png)\r\n\r\n## Why?\r\nWe love to see what people are building with deepstream. It can be a demo-app, open source project or fully fledged business application. The only condition is - there has to be something to see or talk about, e.g. Github Repo, website, video etc...\r\n\r\n## How?\r\nJust send us a mail at [info@deepstream.io](mailto:info@deepstream.io). Don't forget to include your t-shirt size, pick of Elton in his standard or party attire and shipping address.\r\n\r\n\r\n\r\n"}
{"index":{"_index":"pages","_type":"info","_id":"info_performance_overview_readme.md"}}
{"filePath":"info/performance/overview/readme.md","title":"Performance Overview","description":"An overview on deepstream performance and running your tests","content":"\r\n\r\nRealtime web applications have a wide range of performance requirements. Apps with a focus on collaboration need to be able to sync changes between large numbers of clients, financial platforms must fan out prices with ultra-low latency and apps with a focus on communication require comparable up- and down-stream times.\r\n\r\nTo ensure deepstream can cater for all these use cases, we are regularly conducting performance tests and benchmarks.\r\n\r\nIn doing so, we have three main goals. We want to:\r\n\r\n* quantify deepstream’s performance to help choose the right hardware and deployment strategy\r\n* develop a suite of benchmarks that create reproducible results to help detect any decline in performance early on\r\n* make it easier to compare deepstream against other realtime servers to help you choose the right tool for your performance requirements\r\n\r\nPlease note: The following tests are non-conclusive and will be improved and extended as development progresses. If you have a particular performance test scenario in mind or noticed a bottleneck within deepstream, please write to us at [info@deepstream.io](mailto:info@deepstream.io).\r\n\r\n\r\n## Performance tests reduces risk by:\r\n\r\n* understanding how the system reacts under an expected load\r\n* ensuring the system can sustain the expected load for long periods of time\r\n* understanding how the system will react when reaching its full capacity\r\n* understanding how the system will react  when put under extreme load for a short period of time\r\n\r\n## What type of tests can take these into account?\r\n\r\n* load tests<br />\r\n    Make sure that the system works as expected under a set conditions. This covers *CPU* and *message latency* and its output can be used to determine what kind of deployment structure would suit you best.\r\n* soak tests<br />\r\n    Run tests for a long period of time with slightly higher traffic. This covers *memory* and *message latency* and is used to ensure the system can run in production for long periods of time without reducing performance or crashing.\r\n* stress tests<br />\r\n    Push the system into critical usage of CPU and/or network usage and/or Memory and determine how it reacts.\r\n* spike tests<br />\r\n    Generating large amounts of clients or traffic in a very small amount of time and ensuring that the system does not fail.\r\n\r\n## How can you improve results?\r\n\r\nDeepstream is designed as a distributed system. Individual nodes can multiplex updates amongst each other. This means that it’s possible to keep message latency and throughput rates steady whilst catering for an ever increasing number of simultaneously connected clients – simply by spinning up additional deepstream nodes as needed.\r\n\r\nTo put this statement into numbers, please see the performance test results for [a single node in comparison to a three instance cluster](../single-node-vs-cluster/).\r\n\r\n## How to run the tests yourself\r\n\r\nWhen running these tests, we're using a [performance test harness that's available on github](//github.com/deepstreamIO/deepstream.io-performance). It makes it easy to quickly spin up large numbers of deepstream servers and clients.\r\n\r\nClients are created in pairs that send messages back and forth on a unique record in order to calculate latency and keep track of updates.\r\nEach client increments the same field of a record in turns ( even and odd ) until the configured number of messages for the test case is reached.\r\n\r\nUsing a combination of the following environment variables you can adjust the test harness to either run high throughput, high concurrency or long duration tests.\r\n\r\n\r\n| Enviroment Variable       |  Example Value | Description                                                        |\r\n| ------------------------- | -------------- | ------------------------------------------------------------------ |\r\n| DEEPSTREAMS               | 6021,7021      | The deepstream ports to create or connect to ( via tcp )           |\r\n| HOST                      | localhost      | The deepstream host to connect to                                  |\r\n| SERVER_SPAWNING_SPEED     | 1000           | The interval at which servers are spawned ( in ms )                |\r\n| TEST_TIME                 | 100000         | The time for server to run ( in ms )                               |\r\n| LOG_LEVEL                 | 3              | The server log level                                               |\r\n| CLIENT_PAIRS              | 125            | Amount of client pairs to create                                   |\r\n| MESSAGE_FREQUENCY         | 25             | How often to send messages ( in ms )                               |\r\n| MESSAGE_LIMIT             | 5000           | Limit of messages per pair                                         |\r\n| CLIENT_SPAWNING_SPEED     | 100            | Speed of generating clients ( in ms )                              |\r\n| LOG_LATENCY               | true           | Print client latency ( requires CALCULATE_LATENCY to be true )     |\r\n| CALCULATE_LATENCY         | true           | Store client latency during test                                   |\r\n"}
{"index":{"_index":"pages","_type":"info","_id":"info_performance_single-node-vs-cluster_readme.md"}}
{"filePath":"info/performance/single-node-vs-cluster/readme.md","title":"Single Node vs Three Instance Cluster","description":"Performance results from a single and cluster deepstream setup","content":"\r\n\r\nThis test aims to verify horizontal scalability by establishing message latency and cpu consumption under high traffic for a single deepstream node in comparison to a cluster of three nodes.\r\n\r\n### Test Setup\r\nAll tests were run on Amazon Web Services EC2 instances within the same region, running AWS Linux. For the cluster tests, Redis was used as a message bus.\r\n\r\n<table class=\"mini\">\r\n    <thead>\r\n        <tr>\r\n            <th>&nbsp;</th>\r\n            <th>Single Node</th>\r\n            <th>Cluster</th>\r\n    </thead>\r\n    <tbody>\r\n        <tr>\r\n            <td>machine for deepstream servers</td>\r\n            <td>1 EC2 c4.large</td>\r\n            <td>1 EC2 c4.2xlarge</td>\r\n        </tr>\r\n        <tr>\r\n            <td>deepstream servers</td>\r\n            <td>1</td>\r\n            <td>3</td>\r\n        </tr>\r\n        <tr>\r\n            <td>client pairs</td>\r\n            <td>250</td>\r\n            <td>750</td>\r\n        </tr>\r\n        <tr>\r\n            <td>ec2 t2.micro instances clients were distributed across</td>\r\n            <td>2</td>\r\n            <td>6</td>\r\n        </tr>\r\n        <tr>\r\n            <td>message frequency per client</td>\r\n            <td>~25ms</td>\r\n            <td>~25ms</td>\r\n        </tr>\r\n        <tr>\r\n            <td>duration of full load</td>\r\n            <td>~3 min</td>\r\n            <td>~12 min</td>\r\n        </tr>\r\n        <tr>\r\n            <td>messages per second</td>\r\n            <td>~10,000</td>\r\n            <td>~30,000</td>\r\n        </tr>\r\n    </tbody>\r\n</table>\r\n\r\n## Single Deepstream Node\r\n\r\n#### Latency Distribution\r\nAverage latency was 2.065ms ( machines being within same data centre )\r\n\r\n![Single deepstream latency](one-ds-latency.png)\r\n\r\n#### CPU usage\r\nCPU reached an average of 80% with approximately 10,000 messages a second\r\n\r\n![Single deepstream cpu](one-ds-cpu.png)\r\n\r\n## Three deepstream node cluster\r\n\r\n##### Latency Distribution\r\nAverage latency was 0.999ms ( machines being within same data centre )\r\n![Deepstream cluster latency](three-ds-latency.png)\r\n\r\n#### CPU usage\r\nCPU reached an average of 70% on all three processes to deal with approximately 30,000 messages a second\r\n\r\n![Deepstream Cluster CPU usage](three-ds-cpu.png)"}
{"index":{"_index":"pages","_type":"info","_id":"info_protocol_message-structure-overview_readme.md"}}
{"filePath":"info/protocol/message-structure-overview/readme.md","title":"Message Structure Overview","description":"An overview over deepstream's internal message structure","content":"\r\n\r\nDeepstream messages are transmitted using a proprietary, minimal, string-based protocol. Every message follows the same structure:\r\n\r\n<div class=\"message-structure\">\r\n&lt;topic&gt;|&lt;action&gt;|&lt;data[0]&gt;|...|&lt;data[n]&gt;+\r\n</div>\r\n\r\n| and + are used in these examples as placeholders, messages are actually separated by invisible Ascii control characters (\"record separator\" (30) and \"group seperator\" (31))\r\n\r\nEvery message has a topic (e.g. RECORD, EVENT, AUTH etc.) and an action ( CREATE, DELETE, SUBSCRIBE etc.). For a full list of available topics and actions please see the list of constants [here](/docs/common/constants/).\r\n\r\n### Example\r\nHere's an example: creating or reading a record `user/Lisa`\r\n\r\n```javascript\r\nuserLisa = ds.record.getRecord( 'user/Lisa' );\r\n```\r\n\r\nwould prompt the client to send this message to the server\r\n\r\n![Message Structure Overview](message-structure-record-create.png)\r\n\r\nMessages always start with `topic` and `action`, but can contain an arbitrary amount of data fields afterwards.\r\n\r\nSetting the value of a path within the record for example\r\n\r\n```javascript\r\nuserLisa.set( 'lastname', 'Owen' );\r\n```\r\n\r\nwould result in this outgoing message\r\n\r\n![Message Structure Path](message-structure-record-patch.png)\r\n\r\nPlease note the additional S before `Owen`. This indicates that the remaining part of the message should be treated as a string. Please find a list of available types [here](/docs/common/#data-types).\r\n\r\nBoth client and server use a message-parser to validate these messages and to convert them into objects looking like this:\r\n\r\n```javascript\r\n{\r\n\traw: 'R\\u001fP\\u001fuser/Lisa\\u001f1\\u001flastname\\u001fSOwen',\r\n\ttopic: 'R',\r\n\taction: 'P',\r\n\tdata: [ 'user/Lisa', '1', 'lastname', 'SOwen' ]\r\n}\r\n```\r\n\r\nThe actual conversion of `SOwen` into `Owen` happens further down the line by the part of the application that handles this specific message and knows which fields contain typed data."}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_active-data-providers_readme.md"}}
{"filePath":"tutorials/core/active-data-providers/readme.md","title":"Active Data Providers","description":"How to boost your application performance by supplying on demand data","content":"\r\n\r\nWhat are Data Providers?\r\n\r\nData Providers are processes that feed data into deepstream. Technically, they are just normal deepstream clients that run on the backend and write to records, send events or provide RPCs.\r\n\r\n#### Example\r\nImagine you're building an application that shows stock prices from various exchanges from around the world. For each exchange, you'd build a process that receives data and forwards it to deepstream.\r\n\r\n![Data Providers](data-providers.png)\r\n\r\n#### The Problem\r\nNasdaq alone can send out tens of millions of price updates every day and its not much different for other stock exchanges. This can put an unsustainable load on your infrastructure and can lead to high bandwith costs.\r\n\r\nEven worse: Most updates might be for stocks that no client is subscribed to and won't be forwarded at all.\r\n\r\n#### The Solution: Active Data Providers\r\nOnly write to records / send events that clients are actually interested in. Deepstream supports a feature called `listening` that lets clients listen for event or record subscriptions made by other clients. First, the listener registers for a pattern, e.g. `nasdaq/.*`. From thereon, it will be notified whenever a subscription is made or removed that matches said pattern.\r\n\r\n```javascript\r\nclient.record.listen('nasdaq/.*', (match, isSubscribed) => {\r\n  /*\r\n    match = 'nasdaq/msft'\r\n    isSubscribed = true\r\n  */\r\n});\r\n```\r\n\r\nThis allows you to create efficient providers that only send out the data that's currently needed.\r\n\r\n![Active Data Providers](active-data-providers.png)\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_auth-file_readme.md"}}
{"filePath":"tutorials/core/auth-file/readme.md","title":"File based authentication","description":"A deepstream authentication mechanism that reads credentials and userdata from a file","content":"\r\n\r\nFile based authentication allows you to store usernames, passwords or password hashes and optional meta-data in a file that will be used to authenticate incoming connections.\r\n\r\nFilebased authentication is a good choice for scenarios with smaller amounts of connections that need authenticating, e.g. publicly readable realtime dashboards with a small number of provider processes delivering the data.\r\n\r\n## Using file based authentication\r\n\r\nTo enable authentication to a Deepstream server with user credentials stored in a local file, set the `type` key to `file` in the `auth` section of the server's [configuration file](/docs/server/configuration/).\r\n\r\n```yaml\r\nauth:\r\n  type: file\r\n  options:\r\n    path: ./users.json\r\n    hash: 'md5'\r\n    iterations: 100\r\n    keyLength: 32\r\n```\r\n\r\nThe `path` key contains a path to the file relative to the config file, that defines your users and passwords. By default this is the _users.yml_ file that comes with deepstream, but the name or location is up to you.\r\n\r\nIn the `hash` key add the hashing algorithm to hash the passwords, for example, using `md5` (or any other algorithm supported by your operating system). The `iterations` key sets how many times the algorithm should was applied to the user's password, and `keyLength` the length of the key generated. These should match how you hashed the passwords.\r\n\r\nIn the _users_ file, create a list of your users, their hashed passwords (you can create hashes with your setting using deepstreams [hash command](/docs/server/command-line-interface/#deepstream-hash)), and any other user data that you would like to pass to permissioning about that user.\r\n\r\n```yaml\r\nchris:\r\n  password: tsA+ks76hGGSGHF8**/JHGusy78=75KQ2Mzm\r\n  data:\r\n    department: admin\r\n\r\nfred:\r\n  password: jhdGHJ7&0-9)GGSGHF8**/JHGusy78sjHJ&78\r\n  data:\r\n    department: finance\r\n```\r\n\r\nStart the Deepstream server and you should see the authentication type confirmed.\r\n\r\n![Deepstream starting with file authentication](ds-auth-file-start.png)\r\n\r\nIn your application code you can now connect to the Deepstream server and try to login a user.\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream('localhost:6021')\r\n\r\nclient.login({\r\n  username: 'chris',\r\n  password: 'password' // NEEDS TO BE REAL\r\n})\r\n```\r\n\r\nIf a success, the Deepstream console will show:\r\n\r\n![Authentication success](ds-auth-file-success.png)\r\n\r\nAnd if a failure:\r\n\r\n![Authentication failure](ds-auth-file-failure.png)\r\n\r\nYou can then handle the outcome of the login request in your JavaScript code, for example:\r\n\r\n```javascript\r\nclient.login({\r\n  username: 'chris',\r\n  password: 'password' // NEEDS TO BE REAL\r\n}, (success, data) => {\r\n  if (success === true) {\r\n    // Handle a successful login\r\n  } else {\r\n    // Handle an incorrect login, the errorCode is available\r\n    console.log(errorCode)\r\n  }\r\n})\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_auth-http-webhook_readme.md"}}
{"filePath":"tutorials/core/auth-http-webhook/readme.md","title":"HTTP Authentication","description":"How to register your own HTTP server as a Webhook for user authentication","content":"\r\n\r\nHttp authentication lets you register your own HTTP server's URL as a Webhook. Every time a user tries to login, deepstream will send their credentials via POST request to your server. Depending on your server's response, the user's login will be denied or granted.\r\n\r\n![Webhook Authentication Flow](webhook-flow.png)\r\n\r\nHttp authentication is the most flexible authentication type as it's completely up to your server to implement an authentication mechanism. You can query a database, contact an oAuth provider, validate a WebToken or whatever else your heart desires.\r\n\r\n## Using HTTP Authentication\r\nTo enable HTTP authentication, set the `type` to `http` in the `auth` section of the server's [configuration file](/docs/server/configuration/).\r\n\r\n```yaml\r\ntype: http\r\noptions:\r\n  endpointUrl: https://someurl.com/validateLogin\r\n  permittedStatusCodes: [ 200 ]\r\n  requestTimeout: 2000\r\n```\r\n\r\nIn the `options` key, set an `endpointUrl` for an authentication service that deepstream will send a `POST` request to, `permittedStatusCodes` to the list of accepted http codes for successful authentication, and `requestTimeout`is the timeout value (in milliseconds).\r\n\r\n{{#infobox \"important\"}}\r\nUnless your deepstream and authentication servers are within the same private network, you should use a secure connection (https).\r\n{{/infobox}}\r\n\r\nThe following payload is sent with the `POST` request for your authentication service to work with.\r\n\r\n```json\r\n{\r\n  \"connectionData\": {...},\r\n  \"authData\": {...}\r\n}\r\n```\r\n\r\nHow you construct the service behind the url endpoint is up to you and your application stack, but it should return the relevant http response code and either a `username` string, or a JSON:\r\n\r\n```json\r\n{\r\n    \"username\":\"chris\",\r\n    \"clientData\": {},\r\n    \"serverData\": {}\r\n}\r\n```\r\n\r\nThe content of `clientData` and `serverData` are up to you, but useful for sending data back to deepstream, with `clientData` available in the `client.login()` callback and `serverData` sent to the permissions handler.\r\n\r\nStart the deepstream server and you should see the authentication type confirmed.\r\n\r\n![deepstream starting with http authentication](ds-auth-http-start.png)\r\n\r\nThis simple node server returns an http code of '200' when a certain username is passed, and '404' if it's any other username:\r\n\r\n```javascript\r\nconst express = require('express')\r\nconst bodyParser = require('body-parser')\r\nconst app = express()\r\n\r\napp.use(bodyParser.json())\r\n\r\napp.post('/auth-user', (req, res) => {\r\n  if (req.body.authData.username === 'chris') {\r\n    res.json({\r\n      username: 'chris',\r\n      clientData: { themeColor: 'pink' },\r\n      serverData: { role: 'admin' }\r\n    })\r\n  } else {\r\n    res.status(403).send('Invalid Credentials')\r\n  }\r\n})\r\n\r\napp.listen(3000)\r\n```\r\n\r\nIn your application code you can now connect to the deepstream server and try to login a user. Try changing the value of username to something aside from 'chris' to see what happens.\r\n\r\n```javascript\r\n// from Node.js\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst client = deepstream('localhost:6021'); //Change port to 6020 for browsers\r\n\r\nclient.login({\r\n  username: 'chris',\r\n  password: 'password' // NEEDS TO BE REAL\r\n}, function(success, data) {\r\n  //success == true\r\n  //data == { themeColor: 'pink' }\r\n})\r\n```\r\n\r\nIf a success, the deepstream console will show:\r\n\r\n![Authentication success](ds-auth-http-success.png)\r\n\r\nAnd if a failure:\r\n\r\n![Authentication failure](ds-auth-http-fail.png)\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_auth-none_readme.md"}}
{"filePath":"tutorials/core/auth-none/readme.md","title":"Auth None","description":"How to disable user authentication for simple applications and development","content":"\r\n\r\nTo disable authentication against a deepstream server altogether, either set the `auth` type to `none` in the server's [configuration file](../../../docs/server/configuration/).\r\n\r\n```yaml\r\n#Authentication\r\nauth:\r\n  type: none\r\n```\r\n\r\nOr use the `--disable-auth` command line argument.\r\n\r\n```bash\r\n./deepstream start --disable-auth\r\n```\r\n\r\nThe Deepstream startup log should confirm that authentication is disabled.\r\n\r\n![Deepstream starts with no authentication](ds-start-auth-none.png)\r\n\r\n**Please note** Even with authentication type `none`, users can still provide an (unverified) username by sending `{username: 'johndoe'}` at login.\r\n\r\n```javascript\r\nclient = deepstream('localhost:6020').login({username: 'johndoe'})\r\n```\r\n\r\nIf no username is provided, deepstream will default to `OPEN`.\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_cluster-messaging_readme.md"}}
{"filePath":"tutorials/core/cluster-messaging/readme.md","title":"Cluster Messaging","description":"Learn how to connect multiple deepstream instances in a cluster to scale with your needs","content":"\r\n\r\n![Deepstream Internals](./internal-workings.svg)\r\n\r\nDeepstream nodes can scale horizontally by syncing their state via a messaging layer. This can be done with almost with any message-broker. Deepstream offers plugins for a number of popular systems and protocols, e.g. [AMQP](/tutorials/integrations/msg-amqp/), [Apache Kafka](/tutorials/integrations/msg-kafka/) or [Redis Pub/Sub](/tutorials/integrations/cache-redis/)\r\n\r\n## Connecting deepstream nodes directly\r\nFor smaller clusters it used to be possible to connect deepstream nodes directly in a full-mesh configuration (everyone-to-everyone). This feature has been deprecated in its current incarnation, but will soon be replaced by a more scalable (and hopefully slightly smarter) direct-message-connector plugin based on the [Small World Network Paradigm](https://en.wikipedia.org/wiki/Small-world_network).\r\n\r\n## Choosing a messaging system\r\nDeepstream nodes use a simple publish/subscribe pattern with high-level topics to communicate with each other. Especially when used in conjunction with a cache this eliminates the need for persistent messaging or complex routing algorithms.\r\nSince state is kept within the persistence layer, deepstream nodes can even life without guaranteed message delivery as subsequent messages will reconcile inconsistent states.\r\n\r\nThis means that deepstream only utilises the most common subset of messaging functionality. When it comes to choosing a message broker therefor, the main criteria should be speed, scalability and ease of use.\r\n\r\nLet's look at what that means for some popular choices to be used together with deepstream. For a detailed analysis of the performance characteristics of various message brokers, have a look [at this comprehensive article](http://bravenewgeek.com/tag/kafka/).\r\n\r\n#### Redis\r\nRedis is a great fit for deepstream as it offers both caching (even disk persistence) and pub-sub capabilities. It's fast, simple and has all the features deepstream needs for production deployments.\r\n\r\n#### AMQP\r\nThe \"Advanced Message Queueing Protocol\" is implemented by a large number of brokers, e.g. [RabbitMQ](https://www.rabbitmq.com/), [Qpid](https://qpid.apache.org/), [HornetQ](http://hornetq.jboss.org/) or [ActiveMQ](http://activemq.apache.org/) to name just a few. AMQP supports powerful routing patterns and achieves high reliability through features such as persistent queues or guaranteed message delivery, but is quite heavy and doesn't exactly excel when it comes to speed.\r\n\r\n#### Kafka\r\nOriginally developed to power messaging at LinkedIn, Kafka has been open sourced by the Apache Foundation and has since found its way into many large enterprises. It offers queued, persistent and distributed pub/sub messaging that can be scaled to almost biblical dimensions. While faster than AMQP it still needs to save every message to disk, resulting in higher latency and can also be quite a hassle to set up and run.\r\n\r\n## Messaging on Cloud Platforms\r\nIf you're running deepstream on a PaaS infrastructure it will make sense to interface with its messaging capabilities.\r\n\r\nFor __Amazon Webservices__, don't bother with SQS, SNS, Kinesis or SWF. Simply launch a small to medium sized [Elasticache](https://aws.amazon.com/elasticache/) instance with Redis as a caching engine and use its pub/sub capabilities (without being charged for message throughput).\r\n\r\n__Google Cloud Platform__ offers a [build-in pub/sub service](https://cloud.google.com/pubsub/). It scales perfectly and is highly reliable, but not too fast and produces a bit of overhead by persisting messages and requiring ACKs. If your application requires low-latency messaging or only operates a small-to-medium sized deepstream cluster, running a Redis-instance on Google's [Compute Engine](https://cloud.google.com/compute/) or [Container Engine](https://cloud.google.com/container-engine/) might be a better choice.\r\n\r\n__Microsoft Azure__ offers an AMQP based service bus, but it's quite slow and takes a lot of adjustments to get to work as the backbone of a pub/sub architecture. Similar to AWS it is a lot easier to just use Azure's [hosted Redis offering](https://azure.microsoft.com/en-us/services/cache/) instead.\r\n\r\n#### Hosters / Messaging as a service providers\r\nThere is also a large offering of messaging-as-a-service offerings. [CloudAMQP](https://www.cloudamqp.com/), [CloudKafka](http://www.cloudkafka.com/), [StormMQ](http://stormmq.com/) or [RedisLabs](https://redislabs.com/) to name just a few. Whilst they offer great peace of mind through their managed services and high uptime guarantees, we strongly recommend to go with a message-broker running within your own data-center instead. Deepstream uses its message-bus extensively and every millisecond network latency between it and your cluster will make your application notably slower.\r\n\r\n## Connecting to a message broker\r\nDeepstream connectors are available for a number of message brokers and we're constantly looking to expand this selection. You can find an overview of available connectors on the [install page](/install/). Connectors can be installed via deepstream's commandline interface, using the `msg` keyword, e.g.\r\n\r\n```bash\r\ndeepstream install msg redis\r\ndeepstream install msg amqp\r\ndeepstream install msg kafka\r\n\r\n# or on windows using deepstream.exe\r\ndeepstream.exe install msg redis\r\n```\r\n\r\nEach connector requires specific configuration parameters. These can be configured in deepstream's config.yml file (found either in your deepstream's `conf` directory or on linux in `/etc/deepstream/`). When installing a connector, it usually prints an example of the most common settings.\r\n\r\n![Example Console Output](connector-output.png)\r\n\r\nMessage connectors are configured in the config's `plugins - message` section.\r\n\r\nIf you're using deepstream from Node, it's also possible to download connectors from NPM. All connectors follow the naming convention `deepstream.io-type-name`, e.g. `deepstream.io-msg-redis`.\r\n\r\n## Writing your own connector\r\nIf you can't find a connector for your system of choice, you can also write your own quite easily in C++ with Node bindings or in Node.js. If you're happy with the way your connector turned out, please consider contributing it. To do so, have a look at deepstream's [contribution guidelines](/info/community/contribution-guidelines/)\r\n\r\nTo get started, just clone or fork the [msg-connector-template](//github.com/deepstreamIO/deepstream.io-msg-connector-template) and fill in the blanks. To see if it works, update the `settings` variable on line 9 of the [test file](https://github.com/deepstreamIO/deepstream.io-msg-connector-template/blob/master/test/message-connector-messagingSpec.js) and run the tests with `npm test`. Please note: The tests are very high level and only cover basic functionality. It will make sense to add additional tests that are specific to your connector.\r\n\r\n#### Some considerations when implementing a message connector\r\n\r\n**initialisation**\r\nIf your connector requires an initialisation step, e.g. establishing a connection to the messaging, its `isReady` property should initially be set to `false`. Once the connection is established, set `isReady` to `true` and emit a `ready` event by calling `this.emit( 'ready' );`.\r\n\r\n**general errors**\r\nWhenever a generic error occurs (e.g. a connectivity error or any other error that's not directly related to a `publish`, `subscribe` or `unsubscribe` operation, your connector should emit an `error` event and send the error message as a parameter, e.g. `this.emit( 'error', 'connection lost' );`\r\n\r\n**operation-specific errors**\r\nWhenever an error occurs as a consequence of a `publish`, `subscribe` or `unsubscribe` operation, pass it to the callback as the first argument, otherwise pass `null`.\r\n\r\n**topics are very coarse by default**\r\nA lot of message brokers take a long time to create new topics, but are very quick at sending messages.\r\nDeepstream therefor only uses four high level topics (RECORD, RPC, EVENT plus a private topic), but will send and receive large numbers of messages on each of them. If this leads to performance problems it might make sense for the message connector to do some custom sub-routing, e.g. based on record namespaces etc.\r\n\r\n**serialisation/deserialisation**\r\nMessages are passed to the `publish()` method as javascript objects and are expected to be returned by the receiver as such. So it's up to the message connector to serialize and deserialize them, e.g. as JSON or MsgPack\r\n\r\n**make sure not to receive your own messages**\r\nThe message connector acts as both publisher and subscriber for each topic. It should however not receive its own messages. Some messaging middleware supports this, but for others it might be necessary to add an unique sender-id to outgoing messages and filter out incoming messages that have the same id\r\n\r\n**and finally**\r\nMessaging is the backbone of deepstream's scaling / clustering capabilites. So this needs to be reliable... and fast!\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_datasync-anonymous-records_readme.md"}}
{"filePath":"tutorials/core/datasync-anonymous-records/readme.md","title":"Anonymous Records","description":"Learn how to use anonymous records to simplify working with dynamic selections","content":"\r\n\r\nIf you've read about [records](/tutorials/core/datasync-records/), you know that they are small data objects that can be observed and manipulated. A record lets you store values using `.set();`, retrieve them via `.get()` and listen for changes with `subscribe()`.\r\n\r\nAnonymous records do exactly the same.\r\n\r\n## What are anonymous records?\r\nThe only difference between a record and an anonymous record is that records have a unique name - but anonymous records don't. Instead, they have a `setName( name )` method that lets you change their name.\r\n\r\nConceptionally, an anonymous record is like a shell that wraps around other records. Listeners can be bound to that shell and stay intact while the underlying record changes.\r\n\r\nIf you ever had to work at a place that believes in hot-desking (I hope you haven't), you probably know these laptop docks:\r\n\r\n![Laptop Dock](./laptop-dock.jpg)\r\n\r\nThey stay connected to the screen, keyboard and power plug etc., but let you switch the laptop that drives them. An anonymous record works pretty much the same way.\r\n\r\n## What are anonymous records used for?\r\nAnonymous records come in handy if a section of an interface can be used to manipulate different records of a similar type. Take this example app for instance:\r\n\r\n![Switching Users](./simple-app-anim.gif)\r\n\r\nHere, each of the Simpsons is a record. The names of all three are stored in a list. The section with the input-fields on the right is powered by a single anonymous record. All input-fields are two-way bound to paths within it.\r\n\r\n![Simple App Structure](simple-app-structure.png)\r\n\r\nNow, whenever the user selects one of the Simpsons, the anonymous record's `setName( id )` method is called with the `recordId` of that Simpson. It internally removes all subscriptions to the old record, switches to the new one, re-subscribes, all change-listener fire and the UI is up to date.\r\n\r\n## How do I create an anonymous record?\r\nBy calling `client.record.getAnonymousRecord()`. The method doesn't take any arguments.\r\n\r\nA few more things worth mentioning:\r\n\r\n- All method calls to the anonymous-record, e.g. `delete()`, `discard()`, `get()` or `set()` are proxied to the currently underlying record.\r\n\r\n- The anonymous record emits a `ready` event whenever `setName()` is called and the new record is ready\r\n\r\n- The anonymous record also emits a `nameChanged` event immediately after every call to `setName()`\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_datasync-lists_readme.md"}}
{"filePath":"tutorials/core/datasync-lists/readme.md","title":"Lists","description":"Learn how you can use lists to create collections out of records with attributes in common","content":"\r\n\r\nLists are observable arrays of recordnames (not their data!).\r\n\r\nLists and recordnames have what a computer science teacher would call an n:m-relationship - a list can contain many recordnames and a recordname can be part of many lists.\r\n\r\nTo make life easier, lists come with all sorts of convenience methods. You can add an entry at a specific index using `addEntry( recordName, index )`, remove entries from anywhere within the list using `removeEntry( recordName )`, set or get all entries or check if the list `isEmpty()`.\r\n\r\nApart from that, lists are quite similar to [records](../datasync-records/). They notify you `whenReady()`, can be `subscribe()` to and need to be `discard()` after usage.\r\n\r\n## What are lists useful for?\r\nLists are used whenever records need to be combined into collections. Let's take the infamous [TodoMVC](http://todomvc.com/) for example. When built with deepstream, each task would be a record, containing the `title` and a `completed` flag.\r\n\r\n![records in todo list](todolist-record.png)\r\n\r\nEach record is identified by a unique name, e.g. `todo/ikfndiqx-43jdj23bsdf`.\r\n\r\nThe `todo/` part of the name identifies the category the record belongs to and specifies the table within the database it will be stored in. It does however NOT automatically add the record to a list of todos.\r\n\r\nTo organize our tasks in a list called `todos`, we would need to explicitly create it using\r\n\r\n```javascript\r\nvar todos = ds.record.getList( 'todos' );\r\n```\r\n\r\nand add our recordnames as entries\r\n\r\n```javascript\r\ntodos.setEntries([\r\n    'todo/ikfndidw-1973pnhmyk7',\r\n    'todo/ikfndiqx-43jdj23bsdf',\r\n    'todo/ikfndidt-5sdk3zag354'\r\n]);\r\n```\r\n\r\n![todolist with list](todolist-list.png)\r\n\r\n## Taking it a step further\r\ndeepstream is all about combining simple building blocks into powerful apps - and lists are no exception. Nesting references to lists within records and references to records within lists allows you to model your applications data-layer as a fully synced and observable tree structure.\r\n\r\n![List - Record Tree Structure](tree-structure.png)\r\n\r\n## Using lists with anonymous records\r\nLists are often used to power selection panels for [anonymous records](../datasync-anonymous-records/).\r\n\r\n![simple app with anonymous record](simple-app-structure.png)\r\n\r\n## Why do lists not contain or subscribe to the actual record data?\r\nRecords are more than just their data. They have their own subscribe/discard lifecycle which tends to be closely associated with the component that renders them. Naturally, this component is the best place to request the record and manage its lifecycle.\r\n\r\nRecordnames are lightweight strings that can easily be passed around, e.g. as `props` to React components or as `data-model` to an Android listview.\r\n\r\nThis also addresses one of the major challenges of developing realtime apps:  Using  bandwidth efficiently and minimizing the amount of data that's send over the wire. One of the best ways to achieve this is by limiting subscriptions to the records that are currently in view. Lists help with that by providing the necessary structure to create infinite grids or panels that automatically load and discard data while the user scrolls."}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_datasync-records_readme.md"}}
{"filePath":"tutorials/core/datasync-records/readme.md","title":"Records","description":"Learn how you can use records and access the powerful benefits of datasync","content":"\r\n\r\nRecords are the main building blocks of deepstream's data-sync capabilities. They are atomic bits of JSON data that can be manipulated and observed. Any change to a record is instantly synced across all connected clients.\r\n\r\n## Using records\r\nRecords are requested using `client.record.getRecord(name)`. If a record with the specified name doesn't exist yet, it will be created on the fly. If you just want to check if a record exists without creating it, you can use `client.record.has(name, cb)`.\r\n\r\nRecords have `set()` and `get()` methods to interact with their data and `subscribe()` to inform you about updates. Here's what that would look like in action:\r\n\r\n```javascript\r\n// Client A: Hungry customer sees pizza delivery approach on map\r\nconst driver = client.record.getRecord('driver/jack')\r\ndriver.subscribe('coords', updateMapPointer)\r\n```\r\n\r\n```javascript\r\n// ClientB: Delivery driver's smartphone feeds position into record\r\nconst driver = client.record.getRecord('driver/jack')\r\nnavigator.geolocation.watchPosition(position => {\r\n  driver.set('coords', position.coords)\r\n})\r\n```\r\n\r\n## Paths\r\n`get()`, `set()` and `subscribe()` can be used to get the entire record's data, but also support \"paths\". Paths let you access sub-parts of your record's data using JSON notation, e.g. `pets[1].fur.color`. If a value for a path that doesn't exist yet is set, the path will be created on the fly.\r\n\r\n## Record lifecycle\r\nWhen calling `client.record.getRecord`, one of three things can happen:\r\n\r\n- If the record doesn't exist on the client or the server yet, it will be created. The initial data of a newly created record is an empty object `{}`.\r\n\r\n- If the record exists on the server, but hasn't been loaded on the client yet, it will be retrieved.\r\n\r\n- If the record is already loaded on the client, its instance will be returned.\r\n\r\nIndependent of whether the record has been loaded yet, `getRecord` will return a record instance straight away. You can already start setting values or subscribing to updates at this point, however `get()` calls might return `null`.\r\n\r\nTo ensure a record is fully loaded, use the `whenReady()` method. Please note: This method will execute synchronously when the record is already available or asynchronously if its still being loaded.\r\n\r\n#### Discarding Records\r\nTo inform the server that you're no longer interested in updates for a record, call `discard()`. Discard calls are aggregated, meaning that if a record was requested in three different places using `getRecord()`, it also needs to be discarded three times in order for the server to be notified.\r\n\r\n#### Deleting Records\r\nRecords can be deleted using `delete()`. Deleting also discards the record. Whenever a record is deleted by one client, the same record on all other clients will emit a `delete` event.\r\n\r\n#### Unsubscribe, Discard and Delete - what's the difference?\r\n`unsubscribe()` removes an existing subscription to updates to a record or path. This is purely a client side operation and doesn't notify the server.\r\n\r\n`discard()` tells the server that you're no longer interested in receiving updates for the record.\r\n\r\n`delete()` irreversibly deletes the record from the database and cache and notifies all servers and clients within the cluster of the deletion.\r\n\r\n## Getting a snapshot\r\nIf you just require a static one-off view into a record's data, but not bother with the entire lifecycle you can also use `client.record.snapshot(name, cb)`\r\n\r\n## Naming records\r\nEach record is identified by a name that needs to be unique across the entire system. So what does a great recordname look like? Something like this:\r\n\r\n```javascript\r\nconst book = client.record.getRecord('book/iq6auu7d-p9i1vz3q0yi')\r\n```\r\n\r\nThis name consists of different parts: `book` is the category of the record. `/` is used as a split-character by many database connectors to sort records into tables. `iq6auu7d-p9i1vz3q0yi` is a unique id. Unique ids can be generated on the client using `client.getUid()`. They consist of a base64 encoded millisecond timestamp and a random string. UIDs are a common concept in distributed systems as they eliminate the need for a centralized, incremental id.\r\n\r\n#### Can't unique id's clash?\r\nTheoretically: yes. But the chances for two ids that were generated within the exact same millisecond to be the same are 1:1x10^16 - or one to 10 quadrillion - which is considered an acceptable risk.\r\n\r\n#### Can I use more descriptive record names?\r\nAbsolutely, any string can be used as a record name. But you need to be certain that the string never changes. This is the case for many institutional usecases that are already based on unique ids. If you're building a stock trading platform, it's perfectly fine to name your record for Microsoft's stock `stock/msft`.\r\n\r\n#### Making the username part of the record name\r\nMany permissioning strategies in deepstream are based on record, event or rpc-names and the data they contain.\r\nTo make sure that only `johndoe` can change his settings, you would call your record `settings/johndoe` and specify an associated [valve-rule](../permission-conf-simple/):\r\n\r\n```yaml\r\nrecord:\r\n  settings/$username:\r\n    write: \"$username === user.id\"\r\n```\r\n\r\n## Listening\r\nRecords also support a concept called \"listening\". Every client can register as a listener for record name patterns, e.g. `^settings/.*`. Whenever other clients start subscribing to records that match said pattern, the listener will be notified.\r\n\r\n```javascript\r\nclient.record.listen('^settings/.*', (match, isSubscribed) => {\r\n  //start sending data to this record\r\n})\r\n```\r\n\r\nThis is useful to create \"active\" data providers - backend processes that only send out data that's actually requested. A few things worth mentioning about listening:\r\n\r\n- The listen-callback is called with `subscribed=true` once the first client subscribes to a matching record and with `subscribed=false` once the last subscriber for a matching record unsubscribes.\r\n\r\n- Listening also keeps state. Registering as a listener for a pattern that already has matching subscriptions will call the callback multiple times straight away, once for every matching subscription.\r\n\r\n**Important:** At the moment, listening is limited to subscriptions made on the same deepstream server. Subscriptions made on other servers within a cluster are not propagated. This is something that will be added in the future.\r\n\r\n{{#infobox \"important\"}}\r\nThe listen callback with `isSubscribed=false` is only triggered once the last subscriber has disconnected or discarded the record. If your active data provider is subscribed to the record as well in order to write to it, it counts as a subscriber and the callback won't be invoked.\r\n\r\nThis is a known limitation and will be addressed in future releases that will also introduce load-balancing for listeners, only-one rules etc. To create a high availability cluster of active data providers straight away, we recommend building a listener-orchestration server that tackles these tasks for now.\r\n{{/infobox}}\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_getting-started-config_readme.md"}}
{"filePath":"tutorials/core/getting-started-config/readme.md","title":"Starting the server","description":"Find out how to start the deepstream server","content":"\r\n\r\nDeepstream comes with a configuration file found at _conf/config.yml_ on windows/mac or in '/etc/deepstream/config.yml on Linux'. You can either edit it directly or create a copy and pass it to deepstream using the `-c` flag, e.g.\r\n\r\n```bash\r\ndeepstream start -c ~/path/to/config.yml\r\n```\r\n\r\nThe file is fully documented and there are a lot of configuration options that you can change or add. If you want to change port or host details, add plugins or options for permissions and authentication, the configuration file is the place to be.\r\n\r\n## A few hints\r\nDeepstream's configuration file can be written in both YAML or JSON. Deepstream will automatically choose the right parser, based on the file-extension.\r\n\r\nSome core configuration options can be overridden via commandline parameters, e.g. `--host`, `--port` or `--disable-auth`. For a full list, just run\r\n\r\n```bash\r\ndeepstream start --help\r\n```\r\n\r\nThe configuration file contains relative paths, e.g. for `./permissions.yml` or `users.yml`. If you run the file from another location, make sure to update them.\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_getting-started-quickstart_readme.md"}}
{"filePath":"tutorials/core/getting-started-quickstart/readme.md","title":"Getting started","description":"Learn how to start a server and connect a simple client","content":"\r\n\r\nTime to get started with deepstream. This tutorial takes you through the initial steps of starting a server and connecting to it from a simple webpage using the JS Client.\r\n\r\n![Getting Started Endresult](getting-started.gif)\r\n\r\n## Start the server\r\n\r\nLet's start by installing the server. Just pick the [right version for your operating system](/install/) and follow its steps. Once the server is installed, you can start it with:\r\n\r\n```bash\r\ndeepstream start\r\n```\r\n\r\n## Getting the client\r\n\r\nFor this tutorial we'll simply get the client from a [CDN](https://cdn.rawgit.com/deepstreamIO/deepstream.io-client-js/master/dist/deepstream.min.js), but you can also get it as `deepstream.io-client-js` via NPM or Bower:\r\n\r\n```bash\r\nbower install deepstream.io-client-js\r\nnpm install deepstream.io-client-js\r\n```\r\n\r\nCreate an _index.html_ file and add the following to it, making sure to point to your client library:\r\n\r\n```html\r\n<!DOCTYPE html>\r\n<html>\r\n  <head>\r\n    <script src=\"https://cdn.rawgit.com/deepstreamIO/deepstream.io-client-js/master/dist/deepstream.min.js\"></script>\r\n  </head>\r\n  <body>\r\n    <input type=\"text\" />\r\n    <script type=\"text/javascript\">\r\n      //js goes here\r\n    </script>\r\n  </body>\r\n</html>\r\n```\r\n\r\nThis page consists of one text field ready for user input. Inside the `script` tag, add the following JavaScript to login to your deepstream server:\r\n\r\n```javascript\r\nconst client = deepstream('localhost:6020').login()\r\n```\r\n\r\nNext up, we request a \"record\". Records are small bits of data that are synced\r\nacross all connected client.\r\n\r\n```javascript\r\nrecord = client.record.getRecord('some-name')\r\n```\r\n\r\nFinally, let's wire it up to our input field. The goal is to open the same page in multiple browser windows and see the input stay in sync\r\n\r\n```javascript\r\n\r\nconst input = document.querySelector('input')\r\n\r\ninput.onkeyup = () => {\r\n  record.set('firstname', input.value)\r\n};\r\n\r\nrecord.subscribe('firstname', value => {\r\n  input.value = value\r\n})\r\n```\r\n\r\nOpen the web page in two browser windows and type text into either of the text fields and the other browser window will reflect changes instantly.\r\n\r\nAnd that's it. There's of course a lot more to deepstream than that. If you'd like to learn more about records and what they can be used for, head over to the [record tutorial](/tutorials/core/datasync-records/). Or start reading about deepstream's [Request/Response](/tutorials/core/request-response-rpc/) or [Pub/Sub](/tutorials/core/pubsub-events/) features.\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_handling-data-conflicts_readme.md"}}
{"filePath":"tutorials/core/handling-data-conflicts/readme.md","title":"Handling Data Conflicts","description":"This tutorial explains how to handle merge conflicts in deepstream","content":"\r\n\r\nMerge conflicts can occur when two or more clients write to the same record at the exact same time.\r\n\r\n## How does deepstream keep track of data consistency?\r\nDeepstream uses incrementing version numbers to make sure changes to records happen in sequence and no intermediary update gets lost. Each message created as a result of a `set()` call contains the version number that the client expects to set the record to.\r\n\r\nThe server will ensure that the version of an incoming update is exactly one higher than the current version. If it is, the update is applied and propagated to all other subscribed clients. If it's not though, one of two things will happen:\r\n\r\n#### If the incoming version is the same as the existing version\r\nIf the version of an incoming update is the **same** as the current version,\r\ndeepstream assumes a write conflict. It will keep the current version and send a `VERSION_EXISTS` error to the client that tried to update the record to the existing version. On the client, this will invoke a `MERGE_STRATEGY` function. More about merge strategies below.\r\n\r\n#### If the incoming version is lower than or more than 1 higher than the current version\r\nIf versions have gotten out of sync, the server will attempt a reconciliation. Currently only simple resets are supported, but the feature is being actively improved and extended.\r\n\r\n## Handling merge conflicts\r\nMerge conflicts are handled by `MERGE_STRATEGY` functions. These are exposed by the `deepstream` object and can be set globally when the client is initialized or on a per record basis.\r\n\r\n```javascript\r\n// Set merge strategy globally when initialising the client\r\nds = deepstream( 'localhost', {\r\n    mergeStrategy: deepstream.MERGE_STRATEGIES.LOCAL_WINS\r\n});\r\n\r\n// Set merge strategy on a per record basis\r\nrec = ds.record.getRecord( 'some-record' );\r\nrec.setMergeStrategy( deepstream.MERGE_STRATEGIES.REMOTE_WINS );\r\n```\r\n\r\nBy default, `LOCAL_WINS` and `REMOTE_WINS` are available. It is also possible to implement custom merge strategies, e.g.\r\n\r\n```javascript\r\n// Accept remote title, but keep local content\r\nrec.setMergeStrategy(( record, remoteData, remoteVersion, callback ) => {\r\n    callback( null, {\r\n        title: remoteData.title,\r\n        content: record.get( 'content' )\r\n    });\r\n});\r\n```\r\n\r\n## Avoiding merge conflicts\r\nThe more granular you structure your records, the rarer merge conflicts will be. Generally, deepstream is better at coping with a large number of small records than with a few very large ones.\r\nEspecially when it comes to records with high upstream and downstream rates, e.g. am item on an auction site with quickly updating prices, it might make sense to make the upstream (e.g. the bids a client submits) a separate record or model them as events or RPCs"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_permission-conf-advanced_readme.md"}}
{"filePath":"tutorials/core/permission-conf-advanced/readme.md","title":"Valve Permissioning Advanced","description":"Learn how to unlock the full potential of Valve","content":"\r\n\r\nOk, time for some advanced valve rules. To get up to speed with the basics, head over to the [simple permission rules example](/tutorials/core/permission-conf-simple/).\r\n\r\n## Variables\r\nValve automatically injects a set of variables into its permission rules.\r\n\r\n#### `data` & `oldData`\r\n`data` contains the data for events and RPCs. It can be used to validate the payload.\r\n\r\n```yaml\r\n# make sure a tweet contains max 140 characters\r\n    publish: \"data.content.length < 140\"\r\n# make sure firstname is a string\r\n    write: \"typeof data.firstname === 'string'\"\r\n```\r\n\r\nFor records, `data` is the INCOMING data - the current data is available as `oldData`. This is helpful for comparisons:\r\n\r\n```yaml\r\n# make sure bids at an auction can only go up\r\n    write: \"data.price > oldData.price\"\r\n# make sure that `owner` can't be changed once written\r\n    write: \"!data.owner || data.owner == oldData.owner\"\r\n```\r\n\r\n#### `user`\r\n`user` is an object containing information about the user attempting the action. It offers `user.id` - the username that was provided at login and `user.data`.\r\n\r\n`user.data` is the meta-data that was provided when the user logged in. This could be the data returned by the [http webhook](/tutorials/core/auth-http-webhook/) as `serverData` or the `data` field from the user file if you're using [file based authentication](/tutorials/core/auth-file/). Data is a great place to store authentication data like roles (e.g. `{role: 'admin'}`), access-levels or flags like `{ canDeletePosts: false }`.\r\n\r\nIt is also useful to perform age based validation in conjunction with `now`, e.g. if you're running a liquor store and need to make sure your user's are 21+:\r\n\r\n```yaml\r\nrecord:\r\n    \"*\":\r\n        # 662256000000 = 21 years in milliseconds\r\n        read: \"( user.data.birthdayTimestamp + 662256000000 ) > now\"\r\n```\r\n\r\n## String Functions\r\nValve supports a number of built-in string functions, namely `startsWith`, `endsWith`, `indexOf`, `match`, `toUpperCase`, `toLowerCase` and `trim`. They can be useful to normalize and compare values, e.g.\r\n\r\n```yaml\r\nrpc:\r\n    book-purchase:\r\n        request: \"data.card.issuer.toLowerCase() == 'visa' && data.card.number.match(/^4[0-9]{12}(?:[0-9]{3})?$/)\"\r\n```\r\n\r\n## Cross References\r\nCross references allow you to reference data from other records within deepstream. This is an incredibly versatile feature, allowing you to check states or user-data, make sure that an item that's being purchased is still in stock or verify pre-conditions, e.g. making sure a user can only vote once. Cross references are written as `_(recordName)`.\r\n\r\n```yaml\r\nmake-call:\r\n    request: _('shop-status').isOpen == true\r\n```\r\n\r\nRecordnames referrenced by the `_()` function can also be dynamically created, e.g. from strings and path-variables.\r\n\r\n```yaml\r\n# Make sure an item is still in stock\r\npurchase/$itemId:\r\n    request: _('item/' + $itemId ).inStock > 0\r\n```\r\n\r\n#### Nesting cross-references\r\nIt's even possible to nest cross references. Say we are running an online pharmacy and can only sell certain categories of drugs in countries they have clearance for. Here's the data we have to work with:\r\n\r\n```javascript\r\n// record drug/iqbxxluu-2lc9bl30t18\r\n{\r\n    name: 'Aprotinin',\r\n    categoryId: 'iqbxyw8u-1e686wg77xk'\r\n}\r\n\r\n// record category/iqbxyw8u-1e686wg77xk\r\n{\r\n    name: 'general Antifibrinolytics'\r\n    allowedCountries: [ 'FRA', 'SPA' ]\r\n}\r\n\r\n// user.data\r\n{\r\n    country: 'USA'\r\n}\r\n```\r\n\r\nthe rule below would now perform the following steps:\r\n\r\n- load information about the drug the user is trying to purchase\r\n- use the drug's `categoryId` to load the category it belongs to\r\n- check if the user's country is in the list of countries this drug's category can be sold in\r\n\r\n```yaml\r\n# Make sure a drug's category is cleared for sale in the user's country\r\npurchase/$drugId:\r\n    request: _('category/' + _( 'drug/' + $drugId ).categoryId ).allowedCountries.indexOf( user.data.country ) > -1\r\n```\r\n\r\nIn this case the purchase attempt would be declined as the drug's category is only cleared for sale in France and Spain, but the user is from the US.\r\n\r\n#### Performance implications of cross-references\r\nEvery cross-reference executes an additional query against deepstream's cache for every permissioning step - which will slow message transactions down. Nested cross references in particular are loaded one after another, so can lead to noticeable delays. You can specify a maximal depth for nested cross references as `maxRuleIterations: 3` in deepstream's config file.\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_permission-conf-simple_readme.md"}}
{"filePath":"tutorials/core/permission-conf-simple/readme.md","title":"Valve Permissioning Simple","description":"Learn the basics of Valve and permissioning in deepstream","content":"\r\n\r\nPermissions give you granular control over which records, events or RPCs can be used in which way by which user.\r\n\r\n## What can be permissioned?\r\nPermissions allow you to specify if a user can create, write, read or delete a record, publish or subscribe to events, provide and make RPCs or listen to other client's subscriptions.\r\n\r\n## How do permissions work?\r\nDeepstream uses a proprietary permission syntax called \"Valve\". Valve rules are simple javascript strings with a reduced feature-set. Each valve rule defines access to a specific action related to a specific concept, e.g. a \"write\" to a \"record\".\r\n\r\nHere's what they look like:\r\n\r\n```yaml\r\nrecord:\r\n    #an auctioned item\r\n    auction/item/$sellerId/$itemId:\r\n\r\n        #everyone can see the item and its price\r\n        read: true\r\n\r\n        #only users with canBid flag in their authData can bid\r\n        #and bids can only be higher than the current price\r\n        write: \"user.data.canBid && data.price > oldData.price\"\r\n\r\n        #only the seller can delete the item\r\n        delete: \"user.id == $sellerId\"\r\n```\r\n\r\n## How to use Valve rules?\r\nValve rules can be written in YAML or JSON in the `permissions` file (path in `permissions/options/path` in `config.yml`).\r\n\r\nThe file has three levels of nesting:\r\nType (record, event or rpc)\r\n    - Name (record, event or rpc name)\r\n        - Action( read, write, delete, publish etc.)\r\n\r\n## Let's start with a simple example\r\nLet's say we're building a notification system that allows users to send their status as event (a mini Twitter if you like). Every user can subscribe to another user's status, but users can only publish their own status updates.\r\n\r\nFirst, let's define default actions for all events. Our example platform is open, so generally everyone can publish and subscribe. Listening is not part of the platform, so let's just turn it off altogether:\r\n\r\n```yaml\r\nevent:\r\n    \"*\":\r\n        publish: true\r\n        subscribe: true\r\n        listen: false\r\n```\r\n\r\nNext, let's define rules for our status events. Our event names will follow the schema `user-status/<username>`, e.g. `user-status/lisa`:\r\n\r\n```yaml\r\nevent:\r\n    \"*\":\r\n        publish: true\r\n        subscribe: true\r\n        listen: false\r\n    user-status/$userId:\r\n        publish: \"user.id === $userId\" #users can only share their own status\r\n```\r\n\r\nThis example introduced two basic concepts:\r\n\r\n* **specificity** Valve uses a simple specificity concept to find out which rule to apply: The longest (e.g. most detailed) rule wins. `user-status/$userId` is longer than the general rule `\"*\"`, so for publish operations it will be applied. For subscribe and listen though, the permission will fall back to the general rule `\"*\"`.\r\n\r\n* **path variables** dollar prefixes identify parts of a record, RPC or eventname as variables, e.g. `$userId` in `user-status/$userId`. These variables will be made available within the permission rule. Using e.g. the username as part of the record name for records with access restrictions is a central concept in valve.\r\n\r\n## What's next?\r\nThis was a quick introduction to what valve can do, but there are more powerful concepts, such as build-in variables, string-functions or cross-references to explore. To learn about these, let's move on to the [advanced permission tutorial](/tutorials/core/permission-conf-advanced/)\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_pubsub-events_readme.md"}}
{"filePath":"tutorials/core/pubsub-events/readme.md","title":"Events","description":"Learn how you can use events to implement  publish/subscribe and decouple your logic","content":"\r\n\r\nEvents are deepstream's implementation of the \"publish/subscribe\" or \"observer\" pattern. If you're familiar with e.g. JavaScript event-emitters or Java events, you know how they work. Zero or more interested clients subscribe to an event (sometimes also called a \"topic\", \"channel\" or \"namespace\") and zero or more other clients publish to it.\r\n\r\n![Pub/Sub diagram](pubsub-diagram.png)\r\n\r\nThe emphasis on \"zero or more\" underlines one of the main characteristics of pub/sub: Publishers and subscribers are completely decoupled. It's a bit like a newspaper - journalists write articles assuming, but not knowing that they will be read and readers open the sports section expecting, but not knowing that something will be written on it.\r\n\r\nThis decoupling makes pub/sub scalable and fault tolerant, but sometimes you want to know if there's someone out there waiting for your messages. For that, deepstream's events come with a feature called \"listening\".\r\n\r\n## Pub/Sub and its limitations\r\nDue to its simplicity and versatility, pub/sub is the most widely adopted pattern for realtime systems with many hosted (e.g. [Pusher](https://pusher.com/), [PubNub](https://www.pubnub.com/)), server-to-client (e.g. [socket.io](http://socket.io/), [SocketCluster](http://socketcluster.io/#!/)) or server-to-server (e.g. [Redis](http://redis.io/topics/pubsub), [Kafka](http://kafka.apache.org/)) solutions available.\r\n\r\nHowever, pub/sub is purely a lightweight way of messaging, but doesn't have any concept of persistence or state. It's therefore often used to notify clients of changes which in turn trigger a separate HTTP request to retrieve the actual data.\r\n\r\nThis comes with significant overhead and is increasingly abandoned in favour of \"data-sync\", an approach where the actual data is distributed and kept in sync across all subscribed clients. Data-sync is one of deepstream's core features and can be used in the form of [records](/tutorials/core/datasync-records/).\r\n\r\nHaving said that, pub/sub vs data-sync doesn't need to be an either/or decision. Both complement each other well and can be used together for many use cases.\r\n\r\n## How to use events\r\n\r\nSubscription to events can be established with `client.event.subscribe` and removed with `client.event.unsubscribe`.\r\n\r\n```javascript\r\nfunction eventCallback( data ) {\r\n\t//callback for incoming events\r\n}\r\n\r\n//Subscribing to an event\r\nclient.event.subscribe( 'news/sports', eventCallback );\r\n\r\n//Removing specific callback to the event\r\nclient.event.unsubscribe( 'news/sports', eventCallback );\r\n\r\n//Removing all subscriptions to the event\r\nclient.event.unsubscribe( 'news/sports' );\r\n```\r\n\r\nEvents can be published using `client.event.emit`. It's possible to send any kind of serializable data along with the event, e.g. Strings, JSON objects, Numbers, Booleans etc.\r\n\r\n```javascript\r\nclient.event.emit( 'news/sports', 'football is happening' );\r\n```\r\n\r\n## How to listen for event subscriptions\r\ndeepstream allows clients to \"listen\" for other clients' event subscriptions. This is useful to create \"active\" data providers that only emit events if they are actually needed.\r\n\r\nListeners can register for a pattern described by a regular expression, e.g. `'^news/.*'`.\r\n\r\n```javascript\r\nclient.event.listen( '^news/.*', ( match, subscribed ) => {\r\n    //match = 'news/sports'\r\n    //subscribed = true\r\n})\r\n```\r\n\r\nThe listen-callback is called with `subscribed=true` once a matching event is subscribed to for the first time and with `subscribed=false` once the last subscriber for a matching event unsubscribes.\r\n\r\nListening also keeps state. Registering as a listener for a pattern that already has matching subscriptions will call the callback multiple times straight away, once for every matching subscription.\r\n\r\n{{#infobox \"important\"}}\r\nAt the moment, listening is limited to subscriptions made on the same deepstream server. Subscriptions made on other servers within a cluster are not propagated. This is something that will be added in the future.\r\n{{/infobox}}\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_request-response-rpc_readme.md"}}
{"filePath":"tutorials/core/request-response-rpc/readme.md","title":"Remote Procedure Calls","description":"Learn how you can use RPCs for your request/response requirements","content":"\r\n\r\nRemote Procedure Calls (RPC) are deepstream's mechanism for request/response communication (think Ajax Request, but with added load balancing and rerouting etc).\r\n\r\nRPCs are helpful on their own as a substitute for classic HTTP workflows, but are particularly useful when combined with other deepstream concepts like pub/sub or data-sync.\r\n\r\n## some great uses for RPC\r\n\r\n* **Querying your database** If you’re using a database or search engine like [ElasticSearch](/tutorials/integrations/db-elasticsearch/) with deepstream, RPCs can be used to query data from it.\r\n* **Interfacing with REST APIs** Need to retrieve a forecast from OpenWeatherMap, get a Github Repo’s commit history or query data from YQL? Create a process that forwards incoming RPCs as HTTP requests and returns the result.\r\n* **Securely combining multi step record transactions** If you're building a realtime voting system, you might want to increase the vote count AND flag the user as having voted at the same time. Use an RPC for that.\r\n* **Distributing computational load** Running expensive image processing tasks? Break your image into parts and let deepstream distribute them between RPC providers.\r\n* **Asking users for input** Need to ask one of your users for input? RPCs can be used for question-answer workflows on the client as well.\r\n\r\n## Using RPCs\r\nLet's look at an example: adding two numbers (granted, not something you would strictly need to do on the backend, but lets keep things simple).\r\n\r\nEvery RPC is identified by a unique name. For our example, we'll choose `'add-two-numbers'`. First, a process needs to register as a \"provider\" - something that's capable of fulfilling a request. This is done using `client.rpc.provide()`. Each client can only provide the rpc once.\r\n\r\n```javascript\r\nclient.rpc.provide( 'add-two-numbers', ( data, response ) => {\r\n    response.send( data.numA + data.numB );\r\n});\r\n```\r\n\r\nNow any client can invoke the remote method. This is done using `client.rpc.make()`.\r\n\r\n```javascript\r\nclient.rpc.make( 'add-two-numbers', { numA: 7, numB: 13 }, ( err, result ) => {\r\n    // result == 20;\r\n});\r\n```\r\n\r\nIf you want to switch off your provider, you can pass the rpc name to unprovide.\r\n\r\n```javascript\r\nclient.rpc.unprovide( 'add-two-numbers' );\r\n```\r\n\r\n## RPC routing\r\nProcesses can register as providers for multiple RPCs and many processes can provide the same RPC. Deepstream will try to route a client's request as efficiently as possible as well as load-balance incoming requests between the available providers.\r\n\r\n![RPC rerouting](rpc-rerouting.png)\r\n\r\nProviders themselves are also able to reject requests (e.g. because they're under heavy load) using `response.reject()` which will prompt deepstream to re-route the request to another available provider.\r\n\r\n```javascript\r\n//Limiting to 50 simultanious image resize tasks at a time\r\nvar inProgress = 0;\r\nclient.rpc.provide( 'resize-image', ( url, response ) => {\r\n    inProgress++;\r\n\r\n    if( inProgress > 50 ) {\r\n        response.reject();\r\n    } else {\r\n        resizeImage( url ).then(() => {\r\n            inProgress--;\r\n            response.send( 'done' );\r\n        });\r\n    }\r\n});\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_searching-and-querying_readme.md"}}
{"filePath":"tutorials/core/searching-and-querying/readme.md","title":"Searching and Querying","description":"How to search through deepstream's data","content":"\r\n\r\ndeepstream is a fast transactional realtime server, but doesn't store any data itself (except for an internal cache for faster access). It does however work with a wide array of databases and makes it easy to expose the underlying db's querying features.\r\n\r\nLet's look at some examples:\r\n\r\n## RethinkDB\r\ndeepstream comes with a pre-build connector to expose RethinkDB's realtime search capabilities as a dynamic [List](/tutorials/core/datasync-lists/).\r\n\r\nTo use it, [install RethinkDB as a storage option](/tutorials/integrations/db-rethinkdb/) and then use [our connector](https://github.com/deepstreamIO/deepstream.io-provider-search-rethinkdb) to use the database seamlessly in your applications.\r\n\r\nOnce you have followed the installation and setup instructions for the connector, you can start searching records.\r\n\r\nFirst create some records to experiment with, this will be written to RethinkDB automatically:\r\n\r\n```javascript\r\nconst newItem1 = client.record.getRecord('shoes/air')\r\nnewItem1.set({color: 'red', brand: 'Nike', price: 100})\r\n\r\nconst newItem2 = client.record.getRecord('shoes/classics')\r\nnewItem2.set({color: 'white', brand: 'Reebok', price: 90})\r\n\r\nconst newItem3 = client.record.getRecord('shoes/liga')\r\nnewItem3.set({color: 'green', brand: 'Puma', price: 110})\r\n```\r\n\r\nTo search for all shoes that are less than 100, create a dynamic list name:\r\n\r\n```javascript\r\nconst priceString = JSON.stringify({\r\n  table: 'shoes',\r\n  query: [\r\n    [ 'price', 'lt', 100 ]\r\n  ]\r\n})\r\n```\r\n\r\nThe `table` value matches the path set when creating the records, in this case, `shoes`.\r\n\r\nAnd search the records:\r\n\r\n```javascript\r\nclient.record.getList('search?' + priceString)\r\n```\r\n\r\nTry searching for all shoes that are green:\r\n\r\n```javascript\r\nconst colorString = JSON.stringify({\r\n  table: 'shoes',\r\n  query: [\r\n    [ 'color', 'match', '^green*' ]\r\n  ]\r\n})\r\nconst results = client.record.getList('search?' + colorString)\r\n```\r\n\r\nYou can combine one or more conditions into the query. To search for shoes that are green **and** less than 100, change your query to the following:\r\n\r\n```javascript\r\nconst colorPriceString = JSON.stringify({\r\n  table: 'shoes',\r\n  query: [\r\n    [ 'color', 'match', '^green*' ],\r\n    [ 'price', 'lt', 100 ]\r\n  ]\r\n})\r\nconst results = client.record.getList('search?' + colorPriceString)\r\n```\r\n\r\nEach condition is an array of `[ field, operator, value ]`, the supported operators are:\r\n\r\n- `eq`: Equal to.\r\n- `gt`: Greater than.\r\n- `lt`: Less than.\r\n- `ne`: Not equal to.\r\n- `match`: A RegEx match.\r\n\r\nIt's a good idea to delete your queries when you're finished with them. Do this with:\r\n\r\n```javascript\r\ncolorPriceString.delete()\r\n```\r\n\r\n## Elasticsearch\r\n\r\nElasticSearch can be accessed by creating a small provider process that makes it accessible as an RPC.\r\n\r\nStart by [installing elasticsearch as a storage option](/tutorials/integrations/db-elasticsearch/).\r\n\r\nOnce you have followed the installation and setup instructions for the connector, you can start searching recorclient.\r\n\r\nFirst create some records to experiment with, this will be written to Elasticsearch automatically:\r\n\r\n```javascript\r\nconst newItem1 = client.record.getRecord('shoes/air')\r\nnewItem1.set({color: 'red', brand: 'Nike', price: 100})\r\n\r\nconst newItem2 = client.record.getRecord('shoes/classics')\r\nnewItem2.set({color: 'white', brand: 'Reebok', price: 90})\r\n\r\nconst newItem3 = client.record.getRecord('shoes/liga')\r\nnewItem3.set({color: 'green', brand: 'Puma', price: 110})\r\n```\r\n\r\nThen send the RPC call to the client:\r\n\r\n```javascript\r\nclient.rpc.provide('search-color-for-brand', (brand, response) => {\r\n  // Query ElasticSearch\r\n  es.search({\r\n    index: 'deepstream',\r\n    q: 'brand=' + brand\r\n }).then((body) => {\r\n    // Return result\r\n    response.send(body.hits.hits.color)\r\n  }, (error) => {\r\n    //Return Error\r\n    response.error(error.toString())\r\n })\r\n})\r\n```\r\n\r\nElasticsearch allows for complex search parameters, and you should read their [query-building documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html) for more details. You add all search parameters to the `q` parameter in the `es.search` method above. To search for shoes that are a particular color **and** less than 100, change your query to the following:\r\n\r\n```javascript\r\nq: 'color=' + color + '&price:[* to 100]'\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_security-overview_readme.md"}}
{"filePath":"tutorials/core/security-overview/readme.md","title":"Security Overview","description":"How encryption, authentication and permissioning work together","content":"\r\n\r\nSecurity in deepstream is based on three interrelated concepts:\r\n\r\n    - encrypted connections\r\n    - user authentication\r\n    - granular permissions\r\n\r\nHere's how they work together:\r\n\r\n## Encrypted Connections\r\nDeepstream supports transport layer security for web-facing connections using HTTPS and WSS. To setup SSL on deepstream, you need to provide the following configuration keys:\r\n\r\n```yaml\r\nsslKey: ./my-key.key\r\nsslCert: ./my-cert.crt\r\nsslCa: ./my-ca.crt\r\n```\r\n\r\nOften it's easier and more performant though to leave SSL termination to a load balancer (e.g. Nginx or HA Proxy). To learn more about this, head over to the [Nginx Tutorial](/tutorials/integrations/other-nginx/).\r\n\r\n## Authentication\r\nEvery incoming connection needs to pass an authentication step. This happens when the client calls `login( data, callback )`.\r\nDeepstream comes with three built-in authentication mechanisms:\r\n\r\n- [none](#) allows every connection. Choose this option for public sites that don't require access controls.\r\n- [file](#) reads authentication data from a static file. This is a good choice for public read / private write use cases, e.g. sports result pages that let every user visit, but only a few backend processes update the result.\r\n- [http](#) contacts a configurable HTTP webhook to ask if a user is allowed to connect. This is the most flexible option as it allows you to write a tiny http server in any language that can connect to databases, active directories, oAuth providers or whatever else your heart desires.\r\n\r\nApart from just accepting / denying incoming connections, the authentication step can also provide two extra bits of information:\r\n\r\n- `clientData` is returned to the client upon login. This is useful to provide user specific settings upon login, e.g. `{ \"view\": \"author\" }` for a blog\r\n-  `serverData` is kept privately on the server and passed to the permission handler whenever a client performs an action. This makes certain security concepts possible, e.g. role based authentication.\r\n\r\n```javascript\r\n\"system-settings\": {\r\n    //publicly readable\r\n    \"read\": true,\r\n    //writable only by admin\r\n    \"write\": \"user.data.role === 'admin'\"\r\n}\r\n```\r\n\r\n### Authentication FAQ\r\n- **When exactly does authentication happen?** When a deepstream client is created, it establishes a connection straight away. The connection however remains in a quarantine state until `login()` is called. This makes sure that auth-data is sent over an encrypted connection and helps to get the at times time-consuming connection establishment out of the way while the user is busy typing passwords.\r\n\r\n- **Can I read usernames for auth purposes out of a deepstream record?**\r\nThere's no built-in support for this at the moment, but it's easy to use the http auth-type and write a server that reads from the same database or cache deepstream connects to.\r\n\r\n- **Can a user connect more than once at the same time**\r\nYes. The same user can connect multiple times from separate browser windows or devices.\r\n\r\n## Permissioning\r\nPermissioning is the act of deciding whether a specific action, e.g. writing to a record or subscribing to an event is allowed.\r\nTo help with this, deepstream uses an expressive, JSON-based permissioning language called Valve. There's a lot to say about Valve. Here's a small Valve File that should give you a first impression, to learn more though head over to the [Valve Permissions Tutorial](/tutorials/core/permission-conf-simple/)\r\n\r\n```yaml\r\nrecord:\r\n\r\n  \"*\":\r\n    create: true\r\n    delete: true\r\n    write: true\r\n    read: true\r\n    listen: true\r\n\r\n  public-read-private-write/$userid:\r\n    read: true\r\n    create: \"user.id === $userid\"\r\n    write: \"user.id === $userid\"\r\n\r\n  only-increment:\r\n    write: \"!oldData.value || data.value > oldData.value\"\r\n    create: true\r\n    read: true\r\n\r\n  only-delete-egon-miller/$firstname/$lastname:\r\n    delete: \"$firstname.toLowerCase() === 'egon' && $lastname.toLowerCase() === 'miller'\"\r\n\r\n  only-allows-purchase-of-products-in-stock/$purchaseId:\r\n    create: true\r\n    write: \"_('item/' + data.itemId ).stock > 0\"\r\n\r\nevent:\r\n\r\n  \"*\":\r\n    listen: true\r\n    publish: true\r\n    subscribe: true\r\n\r\n  open/\"*\":\r\n    listen: true\r\n    publish: true\r\n    subscribe: true\r\n\r\n  forbidden/\"*\":\r\n    publish: false\r\n    subscribe: false\r\n\r\n  a-to-b/\"*\":\r\n    publish: \"user.id === 'client-a'\"\r\n    subscribe: \"user.id === 'client-b'\"\r\n\r\n  news/$topic:\r\n    publish: \"$topic === 'tea-cup-pigs'\"\r\n\r\n  number:\r\n    publish: \"typeof data === 'number' && data > 10\"\r\n\r\n  place/$city:\r\n    publish: \"$city.toLowerCase() === data.address.city.toLowerCase()\"\r\n\r\nrpc:\r\n\r\n  \"*\":\r\n    provide: true\r\n    request: true\r\n\r\n  a-provide-b-request:\r\n    provide: \"user.id === 'client-a'\"\r\n    request: \"user.id === 'client-b'\"\r\n\r\n  only-full-user-data:\r\n    request: \"typeof data.firstname === 'string' && typeof data.lastname === 'string'\"\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_storing-data_readme.md"}}
{"filePath":"tutorials/core/storing-data/readme.md","title":"Storing Data","description":"Find out how deepstream uses cache and storage systems to store your data","content":"\r\n\r\n## How deepstream stores data\r\nAs a standalone server, deepstream keeps all its data in internal memory. In a production cluster however, deepstream servers won't hold any data themselves. Instead, data is stored in a combination of storage and cache layers that are accessible to all deepstream nodes within the cluster. This allows the individual servers to remain stateless and go down / fail over without causing any data-loss, but it also allows for the data to be distributed across multiple nodes.\r\n\r\nDeepstream can connect to both a cache and a database at the same time.\r\nWhenever a value needs to be stored, it is written to the cache in a blocking fashion and simultaneously written to storage in a non-blocking way.\r\n\r\nSimilarly, whenever an entry needs to be retrieved, deepstream looks for it in the cache first and in storage second. If the value can only be found in storage, deepstream will load it into the cache for faster access.\r\n\r\n## So why have this distinction between cache and storage at all?\r\n\r\nBecause they complement each other quite well!\r\n\r\n- Caches need to make relatively small amounts of data accessible at high speeds. They usually achieve this by storing data in memory, rather than on disk (although some, e.g. Redis, can write to disk as well). This means that all data is lost when the process exists. Caches also usually don't offer support for complex queries (although e.g. [Hazelcast](/tutorials/integrations/cache-hazelcast/) has some).\r\n\r\n- Databases (storage) are slower to read or write to, but offer efficient long-term storage for larger amounts of data and allow for more elaborate ways of querying (e.g. full-text search, SQL joins etc.)\r\n\r\n## Why doesn't deepstream store data itself?\r\nDeepstream is sometimes compared to projects like [Firebase](https://firebase.google.com/) (now part of Google Cloud Platform) or [RethinkDB](http://rethinkdb.com/) with [Horizon](https://horizon.io/) - realtime databases that let users create streaming queries.\r\n\r\nWhile deepstream is a great fit for similar use-cases, it's conceptually quite different: deepstream's design-philosophy is inspired by the way multiplayer-game servers or financial streaming works, rather than by datastores.\r\n\r\nIt can be used as a blazingly fast standalone server without any data-layer, but can also cater for large-scale collaboration apps with demanding storage requirements when connected to both a cache and a database.\r\n\r\nFinally, we've taken the lessons learned from fixed stack frameworks like [Meteor](https://www.meteor.com/) to heart. Deepstream is a fast and versatile realtime server - but it doesn't try to be anything more than that.\r\n\r\nWe believe that web technology is moving away from the monolithic enterprise stacks of the past towards a vibrant ecosystem of highly specialized microservices - and our goal is to make deepstream thrive within this ecosystem by making it usable with as many programming languages, frontend frameworks, databases, caches, message-busses, loggers, identity management systems or deployment environments possible.\r\n\r\n## Connecting to a cache or database\r\ndeepstream uses different types of \"connectors\" - plugins that enable it to interface with other systems.\r\n\r\n***Cache connectors*** are plugins that connect deepstream to an in-memory cache, e.g. Redis, Memcached, IronCache, Hazelcast or Aerospike.\r\n\r\n***Storage connectors*** are plugins that connect deepstream to a database, e.g. MongoDB, CouchDB, Cassandra or Amazon's DynamoDB.\r\n\r\n## Choosing a database and cache\r\nWhen it comes to choosing a database and cache, there are a few things to take into account:\r\n\r\n- **Choose systems that scale in the cloud** deepstream can scale horizontally across many computers or instances in the cloud. But that won't help much if your database or cache doesn't do the same. Fortunately, most popular solutions offer some form of sharding, clustering or cloud-replication, but not all cloud services support that. If you're using AWS for instance and consider ElastiCache with Redis as a caching engine, your deployment is limited to a single machine.\r\n\r\n- **Choose systems that complement each other** Some caches like Redis store data to disk and can be used without a database. Some systems like Hazelcast or Redis offer both caching and pub/sub messaging, eliminating the need for a separate message-bus. Some in-memory only caches like Memcached are extremely fast, but need to be backed by a database for persistent storage. Some databases offer very fast read performance and a build-in caching layer, so could be used by themselves (make sure to register them as a cache though since otherwise deepstream might fall back to its internal cache and use the potentially stale data within).\r\n\r\n- **Object/Document/NoSQL databases make more sense than relational ones** deepstream's data-structures are small, independent chunks of JSON, identified by a unique key that can be organized in collections. This makes them a natural fit for object or document oriented databases like Mongo, Rethink or Couch. Deepstream also works with relational databases like MySQL, PostGre or Oracle, but doesn't take advantage of their data-modelling capabilities.\r\n\r\n- **Be careful when using external x-as-a-service providers**\r\nIt can be tempting to use a fully managed cache from an infrastructure-as-a-service provider, but be aware if its hosted outside of your datacenter: deepstream constantly interacts with its cache and every millisecond in network latency will slow down your application considerably.\r\nEqually, a lot of cache / database protocols are designed for use within a trusted environment and therefor unencrypted. If your database lives outside of your network, make sure to use TLS or choose a service that offers a VPN.\r\n\r\n## Downloading and installing connectors\r\nDeepstream connectors are available for many popular databases and caches and we're constantly looking to expand the selection. You can find an overview of available connectors on the [download page](/install/). Connectors can be installed via deepstream's commandline interface, using the `cache` or `storage` keyword, e.g.\r\n\r\n```bash\r\ndeepstream install cache redis\r\ndeepstream install cache memcached\r\ndeepstream install storage mongodb\r\ndeepstream install storage rethinkdb\r\n\r\n# or on windows using deepstream.exe\r\ndeepstream.exe install storage rethinkdb\r\n```\r\n\r\nEach connector requires specific configuration parameters. These can be configured in deepstream's config.yml file (found either in your deepstream's `conf` directory or on linux in `/etc/deepstream/`). When installing a connector, it usually prints an example of the most common settings.\r\n\r\n![Example Console Output](console-output-elasticsearch-install.png)\r\n\r\nCache connectors are configured in the config's `plugins - cache` section, database connectors in `plugins - storage`\r\n\r\nIf you're using deepstream from Node, it's also possible to download connectors from NPM. All connectors follow the naming convention `deepstream.io-type-name`, e.g. `deepstream.io-storage-rethinkdb`.\r\n\r\n## Writing your own connector\r\nIf you can't find a connector for your system of choice, you can also write your own quite easily in C++ with Node bindings or in Node.js. If you're happy with the way your connector turned out, please consider contributing it. To do so, have a look at deepstream's [contribution guidelines](/info/community/contribution-guidelines/)\r\n\r\nTo get started, just clone or fork the [cache-and-storage-connector-template](//github.com/deepstreamIO/deepstream.io-cache-and-storage-connector-template) and fill in the blanks. To see if it works, update the `settings` variable on line 7 of the [test file](//github.com/deepstreamIO/deepstream.io-cache-and-storage-connector-template/blob/master/test/cache-connectorSpec.js) and run the tests with `npm test`. Please note: The tests are very high level and only cover basic functionality. It will make sense to add additional tests that are specific to your connector.\r\n\r\n#### Some considerations when implementing a cache/storage connector\r\n\r\n**initialisation**\r\nIf your connector requires an initialisation step, e.g. establishing a connection to the db/cache, its `isReady` property should initially be set to `false`. Once the connection to the cache / database is established, set `isReady` to `true` and emit a `ready` event by calling `this.emit( 'ready' );`.\r\n\r\n**general errors**\r\nWhenever a generic error occurs (e.g. a connectivity error or any other error that's not directly related to a `get`, `set` or `delete` operation, your connector should emit an `error` event and send the error message as a parameter, e.g. this.emit( 'error', 'connection lost' );\r\n\r\n**operation-specific errors**\r\nWhenever an error occurs as part of a `get`, `set` or `delete` operation, pass it to the callback as the first argument, otherwise pass `null`.\r\n\r\n**serialization**\r\nThe values that are passed to `set()` will be raw JavaScript objects and are expected to be returned by `get()` as such. It's therefor up to your connector to handle serialisation / de-serialisation, e.g. as Binary Data, JSON or message-pack. Some systems (e.g. MongoDB) can also handle raw JSON directly.\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_transforming-data_readme.md"}}
{"filePath":"tutorials/core/transforming-data/readme.md","title":"Transforming outgoing data","description":"Learn how to use transform functions to manipulate data before it leaves the server","content":"\r\n\r\nDeepstream allows to manipulate data before it leaves the server. This can be useful for a number of usecases\r\n\r\n* Adding trusted data like usernames to outgoing RPCs\r\n* Removing confidential information from record data before it is send to clients\r\n* Modifying values based on the user they are sent to, e.g. applying user specific discounts to prices\r\n\r\n## How to manipulate data\r\nData transformation functions can be registered for all TOPIC / ACTION combinations that send out data.\r\n\r\n```javascript\r\nvar Deepstream = require( 'deepstream.io' );\r\nvar server = new Deepstream();\r\n\r\nserver.set( 'dataTransforms', [{\r\n\ttopic: C.TOPIC.RPC,\r\n\taction: C.ACTIONS.REQUEST,\r\n\ttransform: function( data, metaData ) {\r\n\t\tif( metaData.rpcName === 'get-price' ) {\r\n\t\t\tdata.discount = userDiscounts[ metaData.sender ];\r\n\t\t}\r\n\t\treturn data;\r\n\t}\r\n},{\r\n\ttopic: C.TOPIC.RECORD,\r\n\taction: C.ACTIONS.READ,\r\n\ttransform: function( data, metaData ) {\r\n\t\tdelete data.confidentialInfo;\r\n\t\treturn data;\r\n\t}\r\n}]);\r\n\r\nserver.start();\r\n```\r\n\r\n## Transforming record data\r\n\r\nTransforming outgoing record data can be a bit complex, due to the different ways that updates are sent out.\r\n* When a client calls `client.record.getRecord( recordName )` the server responds with a READ action.\r\n* When a record is completely updated, using `record.set({})` with a single argument, an UPDATE action is sent out.\r\n* In case of partial updates, using `record.set(path, value)`, a PATCH message is sent out, containing the path and only the updated value.\r\n\r\nWhen manipulating outgoing record-data, all three cases need to be taken into account.\r\n\r\nThe following example shows how a user-discount would be applied to the price of an item, assuming:\r\n* Record names are structured as `item/<id>`\r\n* Record data looks like `{ price: <id>, other: <id> }`\r\n\r\n```javascript\r\nvar userDiscounts = {\r\n\t'anne': 0.97,\r\n\t'max': 0.90,\r\n\t'lisa': 0.85\r\n};\r\n\r\n// READ and UPDATE have the same signature,\r\n// so we can use the same function for both\r\nvar transformReadAndUpdate = function( data, metaData ) {\r\n\tif( metaData.recordName.substr( 0, 5 ) === 'item/' ) {\r\n\t\tdata.price *= userDiscounts[ metaData.receiver ];\r\n\t}\r\n\treturn data;\r\n};\r\n\r\nserver.set( 'dataTransforms', [{\r\n\ttopic: C.TOPIC.RECORD,\r\n\taction: C.ACTIONS.READ,\r\n\ttransform: transformReadAndUpdate\r\n}, {\r\n\ttopic: C.TOPIC.RECORD,\r\n\taction: C.ACTIONS.UPDATE,\r\n\ttransform: transformReadAndUpdate\r\n},{\r\n\ttopic: C.TOPIC.RECORD,\r\n\taction: C.ACTIONS.PATCH,\r\n\ttransform: function( data, metaData ) {\r\n\t\tif(\r\n\t\t\tmetaData.recordName.substr( 0, 5 ) === 'item/' &&\r\n\t\t\tmetaData.path === 'price'\r\n\t\t) {\r\n\t\t\t//data for PATCH is just the price\r\n\t\t\treturn data * userDiscounts[ metaData.receiver ];\r\n\t\t} else {\r\n\t\t\treturn data;\r\n\t\t}\r\n\t}\r\n}]);\r\n```\r\n<br/>\r\n\r\n{{#infobox \"important\" \"BUT!!!\"}}\r\nTransforming data slows deepstream down quite a bit. Usually, messages are constructed once and fanned out to all subscribed clients. If a transform function is registered however, messages are constructed for every receiver specifically which can add considerable overhead.\r\n\r\nSo: Use with caution and do as little as possible in your transform function.\r\n\r\nAlso, structure your data in a way that is seperated by concern. For example, if you have a user with admin and readonly data, have two seperate records called `user-admin/<id>` and `user/<id>` which you can then permission using [Valve Permissions](/tutorials/core/permission-conf-simple/) instead.\r\n{{/infobox}}\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_core_writing-a-client_readme.md"}}
{"filePath":"tutorials/core/writing-a-client/readme.md","title":"Writing a client","description":"An introduction into how to write a deepstream.io client","content":"\r\n\r\nDeepstream is an open platform that uses a minimal, text-based protocol to communicate between server and client. Here’s an example of what this looks like.\r\n\r\n```gherkin\r\n# Connection established\r\n# Clients sends authentication request\r\n> A|REQ|{\"username\":\"XXX\",\"password\":\"YYY\"}+\r\n\r\n# Server acknowledges authentication\r\n< A|A+\r\n\r\n# Client subscribes to some event\r\n> E|S|someEvent+\r\n\r\n# Server acknowledges the subscription\r\n< E|A|S|someEvent+\r\n\r\n# Server sends a message for this event\r\n# (the S in SmoreDetails indicates type:String)\r\n< E|EVT|someEvent|Smore details+\r\n```\r\n\r\nDeepstream communicates via [engine.io](https://github.com/socketio/engine.io) with browsers and via TCP for all other programming languages. Using a low level protocol like TCP means that pretty much everything, from basic Arduinos to enterprise Java servers can communicate with deepstream.\r\n\r\nIf you'd like to write a client for your language of choice, have a look if there's [already an issue for it](https://github.com/deepstreamIO/deepstream.io/labels/new-client) and get in touch! We'd be more than happy to help.\r\n\r\n## Where to start?\r\nHere's how to tackle writing a deepstream client:\r\n- Have a look at the [page on message structure](/info/protocol/message-structure-overview/)\r\n- Establish a TCP connection to a deepstream server\r\n- Send an [auth message and parse the response](/info/specs/connectivity/)\r\n- Start by implementing [events](/info/specs/events/) as they are the simplest feature\r\n- Add Records, RPCs\r\n\r\nHere are some more ressources and considerations that will prove useful in writing new clients.\r\n\r\n## Message Specifications\r\n\r\nYou can now view all the different message structures [here](/info/protocol/all-messages/) as a reference for all the different messages on the system. If your not sure what a message you recieved means, want to find out if it has an associated ack or want a quick introduction into the low level protocol it's the perfect place to bookmark.\r\n\r\n## Features\r\n\r\nMaking sure your client works in the exact way the server expects it to can be quite challenging when running against a real server. Because of this, we covered all the features offered using [cucumber](https://cucumber.io/), a format that is supported by most of the popular programming languages.\r\n\r\nWhat this means is you can test your client continuously while developing it using prewritten integration tests. These tests are continuously run against current clients and when you get them all passing provides the guarantee you're using the procotol correctly.\r\n\r\nBefore reading on, take a quick peek at [the connectivity feature](/info/specs/connectivity/) to see how they look like.\r\n\r\n{{#infobox \"hint\"}}\r\nYou can hover over messages in features and spec pages to get a breakdown of what they get parsed into.\r\n{{/infobox}}\r\n\r\nSince the tests will be run in the language the client is being written in you would also need to setup a very simple TCP server.\r\n\r\nThe best place to start would be looking at the [server step definitions](//raw.githubusercontent.com/deepstreamIO/deepstream.io-client-specs/master/step-definitions-server/step-definition-server.js) and its [TCP server](//raw.githubusercontent.com/deepstreamIO/deepstream.io-client-specs/master/step-definitions-server/tcp-server.js) and applying the same logic in your language of choice.\r\n\r\n{{#infobox \"hint\" \"Remember to catch errors\"}}\r\nIn order to guarantee no errors are being ignored you can add a [cucumber hook](//github.com/cucumber/cucumber/wiki/Hooks) to run after each feature and ensure no unexpected errors were thrown.\r\n{{/infobox}}\r\n\r\n# Connection States\r\n\r\nNext on is the [connection states](/docs/common/constants/#connection-states). The connection starts off in an AWAITING_AUTHENTICATION mode, in which you are required to login in order to be able to send and recieve messages through the server. Once you do the client should be in AUTHENTICATING, and if the login is successful the connection will end up in OPEN, which means everything is working fine.\r\n\r\nIf the connection does drop, clients are expected to go into reconnecting mode to allow them to try and reestablish the connection, in which it can either go back to AWAITING_AUTHENTICATION or result in the client closing after too many failed attempts.\r\n\r\n![Connection state diagram](connection-state-diagram.png)\r\n\r\n## TCP Buffering\r\n\r\nSince clients are expected to have a very high throughput it's good practice to buffer messages within the program and then send it out once at the end of a CPU cycle.\r\n\r\nFor example, let us say I have a loop that generates lots of events:\r\n\r\n```javascript\r\nwhile( i < 10000 ) {\r\n    deepstreamClient.event.emit( 'value', Math.random() );\r\n    i++;\r\n}\r\n```\r\n\r\nIf I was to send the message through directly to the TCP socket for every iteration it would create an overhead having to interact with the socket directly so often. Instead, we can concatenate the messages within the client and then send them in one go. Because of the use of the message seperation character the server can then split the package up and process them in the same order.\r\n\r\n```javascript\r\nconnection.prototype.send = function( message ) {\r\n    bufferedSendMessage += message;\r\n    if( !flushTimeout ) {\r\n        flushTimeout = setTimeout( flushMessage );\r\n    }\r\n}\r\n```\r\n\r\nThe same situation happens when you recieve a message. If the traffic is slow you might only be getting one message per packet, but when traffic volumes pickup you will need to split the data recieved and process each message individually.\r\n\r\n```javascript\r\nconnection.prototype.recieve = function( message ) {\r\n    var lastMessageSeperator, messages;\r\n\r\n    lastMessageSeperator = recievedMessages.lastIndexOf( MESSAGE_SEPERATOR );\r\n    messages = recievedMessages.substring( 0, lastMessageSeperator);\r\n    processMessages( messages );\r\n}\r\n```\r\n\r\n## Acks\r\n\r\nUnfortunately timeouts can always occur when internet connections become so slow they might as well not work. Because of this deepstream has ack messages to let the client know the server has recieved the messages sent.\r\n\r\nAck timeouts are the clients responsibility to keep track of. When a message that can get an associated ack is sent out, you need to set a timer which can be removed once the server replies. If not the application will be notified which depending on the situation can allow you to try the same action again, or notify the user their desired behaviour might not have gone occurred successfully.\r\n\r\n## Unsoliciated Messages\r\n\r\nMessages recieved that are unexpected should throw an UNSOLICITATED_MESSAGE error. If it occurs often it's usually a useful indication something might be leaking or not have unsubscribed properly.\r\n\r\n{{#infobox \"hint\" \"Edge Case\"}}\r\nIn the case of race conditions this could occur if the server sends a message to the client the exact same time it unsubscribed from the message. This can't be avoided, but should only happen very rarely.\r\n{{/infobox}}\r\n\r\n## Errors\r\n\r\nThe last major thing to keep in consideration are error scenarios. If you look at the [event constants](/docs/common/constants/#Event) you'll notice that there are quite a few different unhappy scenarios that may occur. Many of these are expected behaviour, such as MESSAGE_PERMISSION_ERROR or TOO_MANY_AUTH_ATTEMPTS. Others are errors returned by deepsteam incase the message protocol was not correctly used, such as INVALID_MESSAGE_DATA, MESSAGE_PARSE_ERROR or UNKNOWN_TOPIC and some are to expose internal issues, such as CACHE_RETRIEVAL_TIMEOUT or STORAGE_RETRIEVAL_TIMEOUT.\r\n\r\nIt's important to:\r\n* Not let the application die when an error occurs\r\n\r\nAlways recover from them as gracefully as possible.\r\n* Not swallow errors\r\n\r\nErrors occur because something went wrong. Having an empty catch statement\r\nis only a stop gap solution and you should always make sure to log the issues that occur\r\n* Prevent users from doing things they are not permissioned to\r\n\r\nIf a user is not permissioned to do a certain action it is advisable to stop them from attempting to. Otherwise it could result in inconsistencies on the server where the data was ignored and on the client where the changes either have to be reverted once the MESSAGE_DENIED is recieved or remain out of sync otherwise."}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_cache-hazelcast_readme.md"}}
{"filePath":"tutorials/integrations/cache-hazelcast/readme.md","title":"Hazelcast Cache Connector","description":"Learn how to use Hazelcast with deepstream","content":"\r\n\r\n#### What is Hazelcast?\r\nHazelcast is a distributed caching layer, organized as a grid of independent nodes that sync their state. On top of that, Hazelcast allows to perform computations based on the stored data and even supports basic server side messaging.\r\n\r\n#### Why use Hazelcast with deepstream?\r\nHazelcast can be a good choice as a fast and scalable caching layer for deepstream. It can outperform other caches like Redis in cluster-mode, but is a bit trickier to set up.\r\nWhere Hazelcast really comes into its own is the additional Map-Reduce functionality built on top of its caching capabilities. This makes it possible to perform simple, distributed computations based on record data and feed the results back to deepstream.\r\n\r\n![Hazelcast Diagram](hazelcast-diagram.png)\r\n\r\n#### Using Hazelcast with deepstream\r\ndeepstream comes with a cache connector for Hazelcast. Simply install it via the command line\r\n\r\n```bash\r\ndeepstream install cache hazelcast\r\n```\r\n\r\n![Hazelcast install console output](console-output.png)\r\n\r\nor, if you're using deepstream in Node, [get it from NPM](https://www.npmjs.com/package/deepstream.io-cache-hazelcast)\r\n\r\n#### Configuring the Hazelcast connector\r\n\r\nYou can configure the Hazelcast cache connector in the plugins section of deepstream's config.yml file. Please find a full list of configuration options [here](http://hazelcast.github.io/hazelcast-nodejs-client/api/0.3/docs/modules/_config_.html)\r\n```yaml\r\nplugins:\r\n    cache:\r\n      name: hazelcast\r\n      options:\r\n        networkConfig:\r\n          addresses:\r\n            - host: hostname\r\n              port: 1234\r\n\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_cache-memcached_readme.md"}}
{"filePath":"tutorials/integrations/cache-memcached/readme.md","title":"Memcached Cache Connector","description":"Learn how to use Memcached with deepstream","content":"\r\n\r\n#### What is Memcached?\r\n[Memcached](https://memcached.org/) is a distributed caching system. All its data is purely kept in memory which means two things:\r\n\r\n- its very fast\r\n- everything is gone if you pull the plug.That's not necessarily a bad thing, but it means you need some sort of persistent database layer as well if you choose to go with Memcached.\r\n\r\n#### Why use Memcached with deepstream?\r\nFor two reasons: Speed and scale. Memcached can power large clusters and writes and reads data very quickly. The memcached protocol has also seen some wider adoption, e.g. by [Hazelcast](../cache-hazelcast/), so if you do decide to change caches at some point, migration might be quite easy.\r\n\r\n#### I want to use AWS Elasticache with deepstream. Should I choose Memcached or Redis as a caching-engine?\r\nRedis! Memcached is a great cache, but Redis also saves data to disk and can act as a message-bus for smaller deepstream clusters, giving you all the functionality you need.\r\n\r\n#### How to use Memcached with deepstream?\r\nDeepstream comes with an official connector for Memcached. You can install it via the commandline:\r\n\r\n```bash\r\ndeepstream install cache memcached\r\n```\r\n\r\nresulting in the following output\r\n\r\n![Memcached Console Output](console-output.png)\r\n\r\nIf you are using deepstream from Node, you can also install it via [NPM](https://www.npmjs.com/package/deepstream.io-cache-memcached)\r\n\r\n#### Configuring the Memcached connector\r\nYou can configure memcached in the plugin section of deepstream's config.yml file\r\n\r\n```yaml\r\nplugins:\r\n  cache:\r\n    name: memcached\r\n    options:\r\n      serverLocation: [ 'localhost:11211' ] # One or more endpoint URLs\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_cache-redis_readme.md"}}
{"filePath":"tutorials/integrations/cache-redis/readme.md","title":"Redis Cache/Message Connector","description":"Learn how to use Redis with deepstream","content":"\r\n\r\n#### What is Redis?\r\nA lot of things. In fact, so many that its often referred to as the \"Swiss Army Knife of the web\". Redis is first and foremost a cache. It's fast, simple, single threaded with non-blocking I/O and scales well in distributed deployments (should sound familiar to deepstream fans).\r\n\r\nBut it also persists its data to disk, making it a good alternative to full-size databases for simpler usecases and can act as a publish/subscribe server for message distribution.\r\n\r\n![Redis](redis.png)\r\n\r\nYou can easily install Redis yourself or use it as a service from your cloud hosting provider, e.g. via [AWS ElastiCache](https://aws.amazon.com/elasticache/), [Microsoft Azure](https://azure.microsoft.com/en-us/services/cache/) or [RackSpace's Object Rocket](http://objectrocket.com/). Due to its popularity there are also a number of specialized Redis hosting companies, e.g. [RedisLabs](https://redislabs.com/), [RedisGreen](http://www.redisgreen.net/), [Compose](https://www.compose.io/) or [ScaleGrid](https://scalegrid.io/), but be careful: Deepstream constantly interacts with its cache, so every millisecond network latency between its server and your RedisHoster makes deepstream notably slower. We strongly recommend choosing a cache that runs in close network proximity to your deepstream servers, e.g. within the same data-center.\r\n\r\n#### Why use Redis with deepstream?\r\nRedis is a great fit for use with deepstream. It can be used as a cache, persists data and re-distributes messages for smaller to medium sized deepstream clusters. It won't be much help when it comes to executing complex queries, but if you can live without, Redis might be all you need for your production deployment.\r\n\r\n#### How to use Redis with deepstream?\r\ndeepstream offers two separate plugins for Redis: a cache connector and a message connector. Both can connect to the same Redis endpoint and both offer support for standalone Redis deployments and clusters.\r\n\r\nInstall via commandline or if you're using deepstream in Node.js via NPM ([Cache](https://www.npmjs.com/package/deepstream.io-cache-redis), [Message](https://www.npmjs.com/package/deepstream.io-msg-redis))\r\n\r\n```bash\r\n    deepstream install msg redis\r\n    deepstream install cache redis\r\n```\r\n\r\nThe same configuration options for both can be set in deepstream's config.yml file\r\n\r\n```yaml\r\nplugins:\r\n  cache:\r\n    name: redis\r\n    options:\r\n      host: localhost\r\n      port: 6379\r\n  message:\r\n    name: redis\r\n    options:\r\n      host: localhost\r\n      port: 6379\r\n```\r\n\r\n\r\nBoth connectors work with Redis clusters as well. Just adjust your options as follows:\r\n\r\n```yaml\r\nplugins:\r\n   cache:\r\n     name: redis\r\n     options:\r\n       nodes:\r\n         - host: <String>\r\n           port: <Number>\r\n           password: <String>\r\n         - host: <String>\r\n           port: <Number>\r\n       maxRedirections: 16\r\n       redisOptions:\r\n         password: 'fallback-password'\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_db-elasticsearch_readme.md"}}
{"filePath":"tutorials/integrations/db-elasticsearch/readme.md","title":"ElasticSearch DataBase Connector","description":"Learn how to use ElasticSearch with deepstream","content":"\r\n\r\n#### What is ElasticSearch\r\n[ElasticSearch](https://www.elastic.co/products/elasticsearch) is a standalone search engine that let's you add powerful full-text search capabilities to your application. You might be thinking right now: __what's wrong with a bit of MySQL and `WHERE content like %word%`?__...well, ElasticSearch is an entirely different league. It offers word stemming (search \"fisher\" also searches fish, fishing etc.), synonyms and related words ( \"UK\" also searches for United Kingdom, Great Britain etc.) and a number of other crucial concepts for proper search capabilities.\r\nYou can run ElasticSearch yourself as a single server or a cluster or use it as a service, e.g. via [AWS ElasticSearch](https://aws.amazon.com/elasticsearch-service/).\r\n\r\n#### Using ElasticSearch with deepstream.io\r\nElasticSearch's powerful search capabilities can be a great addition for many deepstream apps, but there are some things to be aware of:\r\n\r\n**Use it with a persisting cache** ElasticSearch can be used just as a normal data-base to store deepstream records, but it is first and foremost a search engine. When used with deepstream.io, we'd recommend also using a cache that saves data to disk, e.g. [Redis](../cache-redis/) as a faster way of storing and accessing data.\r\n\r\n**ElasticSearch's 'realtime search' means something different** [There's a lot of discussion about what __realtime__ exactly means](https://deepstream.io/blog/what-is-realtime/). For deepstream and many others, it means that data is sent to you as it becomes available.\r\n'Realtime search' in this context means that new results are streamed out as soon as they match an existing query. This interpretation us shared by others, e.g. [RethinkDB](../db-rethinkdb/) as well.\r\nElasticSearch also mentions its \"realtime search capabilities\". Here it means though that results can be searched for immediately after a write to the database (as opposed to e.g. a Hadoop cluster where data only becomes available after a significant delay). To search for the newly added data however, you still have to execute a new query.\r\n\r\n#### Storing deepstream's data in ElasticSearch\r\nStoring data in ElasticSearch is easy. Just use deepstream's storage connector for ElasticSearch.\r\n\r\n![deepstream elasticsearch install console output](elasticsearch-install-console-output.png)\r\n\r\n#### Querying ElasticSearch from deepstream\r\nYou have two choices: ElasticSearch offers a well structured REST API that can be accessed directly via HTTP (e.g. via an Ajax call from the Browser). This can be a simple and very solid choice, especially when used with a managed ElasticSearch deployment as a service.\r\n\r\nElasticSearch can also be accessed from within deepstream through a remote procedure call (RPC). For this, you'll need to create a backend process that sits in between deepstream and the search engine.\r\n\r\n![deepstream elasticsearch provider diagram](deepstream-elasticsearch-provider-diagram.png)\r\n\r\nSay we create a (admittably nonsensical) set of records like\r\n\r\n```javascript\r\nclient.record.getRecord('cars/ferrari').set({\r\n  brand: 'Ferrari',\r\n  color: 'Red'\r\n})\r\n```\r\n\r\nand our greatest desire in life would be to execute a RPC that searches for a brand and returns its signature color\r\n\r\n```javascript\r\nclient.rpc.make('search-color-for-brand', 'Ferrari', (err, color) => {\r\n  //color === red\r\n});\r\n```\r\n\r\nour search provider would look as follows:\r\n\r\n```javascript\r\nconst elasticsearch = require('elasticsearch')\r\nconst deepstream = require( './ds-client' )\r\n\r\n// Create ElasticSearch Client\r\nconst es = new elasticsearch.Client({\r\n  host: 'elasticsearch-host',\r\n  log: 'trace'\r\n});\r\n\r\n// Create Deepstream Client\r\nconst client = deepstream('deepstream-host').login()\r\n\r\n// Register as provider for search-color-for-brand\r\nclient.rpc.provide('search-color-for-brand', (brand, response) => {\r\n  // Query ElasticSearch\r\n  es.search({\r\n    index: 'deepstream',\r\n    q: 'brand=' + brand\r\n  }).then((body) => {\r\n    // Return result\r\n    response.send( body.hits.hits.color )\r\n  }, (error) => {\r\n    //Return Error\r\n    response.error(error.toString())\r\n  })\r\n})\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_db-mongodb_readme.md"}}
{"filePath":"tutorials/integrations/db-mongodb/readme.md","title":"MongoDB DataBase Connector","description":"Learn how to use MongoDB with deepstream","content":"\r\n\r\n#### What is MongoDB?\r\nMongoDB is one of the most widely adopted NoSQL databases. It stores documents in a binary version of JSON, scales via clustering and boosts a lot of features designed for speed, such as a low-level TCP API and a built-in, intermediate caching layer.\r\n MongoDB is free and open-source under GNU GPL and Apache License, and hosts a variety of features that gears itself towards good consistency and partition tolerance.\r\n\r\n#### Why use MongoDB with deepstream\r\nMongoDB can be used with deepstream as a storage solution for record data. Since deepstream is designed to structure data in JSON blobs, identified by a primary key, a NoSQL database like MongoDB is more suitable compared to relational databases like MySQL. MongoDB excels at performance and is appropriate for dynamic queries and data that changes on a frequent basis.\r\n\r\n#### Downsides?\r\nMongoDB is very established, being the most popular object-oriented database model and the 4th most used after Oracle, MySQL and Microsoft SQL. It does however not come with any build in support for realtime queries or output streams (although some approaches of hooking into it's [Operational Log](https://docs.mongodb.com/manual/core/replica-set-oplog/) exist)\r\n\r\n#### Using MongoDB with deepstream.io\r\ndeepstream can connect to MongoDB using the \"MongoDB storage connector\", a plugin that connects to the database and automatically syncs incoming and outgoing record updates.\r\n\r\n**Installing the MongoDB storage connector**\r\n\r\nYou can install the mongodb connector via deepstream's commandline interface, using:\r\n\r\n```bash\r\ndeepstream install storage mongodb\r\n```\r\n\r\nor in windows\r\n```bash\r\ndeepstream.exe install storage mongodb\r\n```\r\n\r\nresulting in deepstream MongoDB connector install command line output.\r\n\r\nIf you're using deepstream's Node.js interface, you can also install it as an [NPM module](https://www.npmjs.com/package/deepstream.io-storage-mongodb)\r\n\r\n**Configuring the MongoDB storage connector**\r\nYou can configure the storage connector plugin in deepstream with the following options:\r\n\r\n```yaml\r\nstorage:\r\n  name: mongodb\r\n  options:\r\n    connectionString: ${MONGODB_CONNECTION_STRING}\r\n    # optional database name, defaults to `deepstream`\r\n    database: 'someDb'\r\n    # optional table name for records without a splitChar\r\n    # defaults to deepstream_records\r\n    defaultTable: 'someTable'\r\n    # optional character that's used as part of the\r\n    # record names to split it into a tabel and an id part\r\n    splitChar: '/'\r\n```\r\n\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_db-rethinkdb_readme.md"}}
{"filePath":"tutorials/integrations/db-rethinkdb/readme.md","title":"RethinkDB DataBase Connector","description":"Learn how to use RethinkDB with deepstream","content":"\r\n\r\n#### What is RethinkDB?\r\nRethinkDB is a distributed, document-oriented database. It implements a proprietary, function based query language, called ReQL to interact with its schemaless JSON data collections.\r\n\r\nWhat makes RethinkDB stand out is its ability to perform “realtime queries”. Rather than just retrieving query results as snapshots of the current state, RethinkDB allows to keep search result cursors open and stream continuous updates as new documents match or unmatch the query.\r\n\r\n#### Why use RethinkDB with deepstream?\r\n![deepstream.io and rethinkdb](deepstream-rethinkdb.png)\r\nRethinkDB’s realtime search makes it a great fit as a datastore within a deepstream architecture. Combining its search capabilities with deepstream’s data-sync, pub/sub and rpc can be a very powerful combination.\r\n\r\n#### Any downsides?\r\nNot really. We’ve used RethinkDB extensively within internal architectures and can very much recommend it. As database lifecycles go, it’s still very young and unestablished, but seeing increasing adoption. At the moment, the realtime querying capabilities aren’t compatible with all query types (e.g. aggregate queries like averages etc. aren’t supported), sharding is limited to 64 nodes and load balancing / shard accessing can require connection redirects.\r\n\r\n#### What about Horizon.io?\r\nRethinkDB offers a Node.js module called [horizon](https://horizon.io/). If you’re looking to access RethinkDB’s querying capabilities via ReQL directly from the browser, this will be a better choice.\r\nDeepstream in comparison is a full, data-base agnostic realtime server. It provides higher level structures like data-sync, pub/sub and request/response, making it more flexible. Deepstream is horizontally scalable via clustering and is highly tuned for performance and speed, using an intermediate caching layer for faster data-access.\r\n\r\n#### How to use RethinkDB with deepstream.io\r\nDeepstream offers a database connector plugin for RethinkDB and optionally also a search provider that creates realtime queries based on dynamic list names. RethinkDB and the search provider are also part of the [Compose file for Docker](/install/docker-compose/).\r\n\r\n**Installing the RethinkDB storage connector**\r\n\r\nYou can install the rethinkdb connector via deepstream's commandline interface, using:\r\n\r\n```bash\r\ndeepstream install storage rethinkdb\r\n```\r\n\r\nor in windows\r\n```bash\r\ndeepstream.exe install storage rethinkdb\r\n```\r\n\r\nresulting in\r\n![deepstream RethinkDB connector install command line output](rethinkdb-deepstream-install-console-output.png)\r\n\r\nIf you're using deepstream's Node.js interface, you can also install it as an [NPM module](https://www.npmjs.com/package/deepstream.io-storage-rethinkdb)\r\n\r\n**Configuring the RethinkDB storage connector**\r\nYou can configure the storage connector plugin in deepstream with the following options:\r\n\r\n```yaml\r\nstorage:\r\n  path: rethinkdb\r\n  options:\r\n\r\n    # address rethinkdb is bound to\r\n    host: localhost\r\n\r\n    # port rethinkdb is bound to\r\n    port: 28015\r\n\r\n    # optional authentication key for rethinkdb\r\n    authKey: someString\r\n\r\n    # optional database name, defaults to `deepstream`\r\n    database: someDb\r\n\r\n    # optional table name for records without a splitChar\r\n    # defaults to deepstream_records\r\n    defaultTable: someTable\r\n\r\n    # optional character that's used as part of the\r\n    # record names to split it into a tabel and an id part, e.g.\r\n    #\r\n    #books/dream-of-the-red-chamber\r\n    #\r\n    # would create a table called 'books' and store the record under the name\r\n    # 'dream-of-the-red-chamber'. Defaults to '/'\r\n    splitChar: /\r\n```\r\n\r\n## search provider\r\n<a class=\"npm-download big\" href=\"https://www.npmjs.com/package/deepstream.io-provider-search-rethinkdb\">download search provider</a>\r\n\r\nThe RethinkDB search provider is an independent process that sits between deepstream and RethinkDB. It let's you create lists with dynamic names such as `search?{\"table\":\"book\",\"query\":[[\"title\",\"match\",\"^Harry Potter.*\"],[\"price\",\"lt\",15.3]]}` on the client that automatically map to realtime searches on the backend\r\n\r\n![deepstream rethinkdb search provider diagram](deepstream-rethinkdb-search-provider.png)\r\n\r\nHere's an example: Say you're storing a number of books as records.\r\n\r\n```javascript\r\nclient.record.getRecord( 'book/i95ny80q-2bph9txxqxg' ).set({\r\n  'title': 'Harry Potter and the goblet of fire',\r\n  'price': 9.99\r\n})\r\n```\r\n\r\nand use deepstream.io's RethinkDB storage connector with\r\n\r\n```javascript\r\n{splitChar: '/'}\r\n```\r\n\r\nyou can now search for Harry Potter books that cost less than 15.30 like this\r\n\r\n```javascript\r\nvar queryString = JSON.stringify({\r\n  table: 'book',\r\n  query: [\r\n    ['title', 'match', '^Harry Potter.*'],\r\n    ['price', 'lt', 15.30]\r\n  ]\r\n})\r\nclient.record.getList('search?' + queryString)\r\n```\r\n\r\nand the best thing is: it's in realtime. Whenever a record that matches the search criteria is added or removed, the list will be updated accordingly.\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_frontend-angular_readme.md"}}
{"filePath":"tutorials/integrations/frontend-angular/readme.md","title":"AngularJS","description":"Learn how to use AngularJS with deepstream","content":"\r\n\r\nAngular 2 / .io is out and has been for a while, but for native JavaScript aficionados, Angular 1 is still the tool of choice.\r\n\r\n## Is Angular a good choice with deepstream?\r\nA reasonably good one. React's componentized architecture lends itself better to deepstream's granular data structures and Knockout's observables work like a charm with functional-reactive designs that model realtime data-flows as a network of interconnected pipelines.\r\nBut for application structure, rendering of partial view updates and usage of deepstream as a global service, Angular is still a good fit.\r\n\r\n## What are the challenges in using deepstream with Angular?\r\nAngular expects to be in charge of its digest loop and is built on the assumption that it will know when updates are necessary - either as a result of an user-interaction or an incoming response to a REST call through a `$resource`. Realtime updates via deepstream however can arrive at any time and require you to tell Angular when it needs to trigger a digest. This is entirely possible and works perfectly well in the real world, yet performing the necessary operation to do so, e.g. calling into\r\n\r\n```javascript\r\nif( !$scope.$$phase ) {\r\n    $scope.$apply();\r\n}\r\n```\r\n\r\nis considered a bit of an anti-pattern by passionate Angular-purists.\r\n\r\n## How to use deepstream with Angular?\r\nDeepstream's records are a great fit as models for an Angular application, but there are also a few more tips worth sharing:\r\n\r\n#### Create the deepstream client as a service\r\nThis not only makes it easily accessible within the app, but also makes it easy to mock in testing scenarios.\r\n\r\n```javascript\r\nangular.service( 'deepstream', function() {\r\n    var client = deepstream( 'localhost:6020' )\r\n    client.login({ username: 'ds-simple-input-' + client.getUid() });\r\n    return client;\r\n})\r\n```\r\n\r\n#### You can create one way bindings directly from html\r\n\r\nBy adding a record directly to the scope, your view can retrieve the latest values on every digest\r\n\r\n```javascript\r\n.controller( 'user', function( deepstream, $scope ) {\r\n    $scope.user = deepstream.record.getRecord( 'user/mike' );\r\n});\r\n```\r\n\r\nand in your template\r\n\r\n```html\r\n    <span ng-bind=\"user.get('firstname')\"></span>\r\n```\r\n\r\n#### You can create an auto-magic two-way binding service\r\nYou can create a service that binds `$scope` variables to paths within a record. This way, every time a value on the record changes, it is rendered to the view - and the record gets updated every time a user enters something in the view (don't worry, there are checks to prevent recursive loops)\r\n\r\n```javascript\r\nservice( 'twoWayBind', function(){\r\n    return function twoWayBind( $scope, record, name ) {\r\n        Object.defineProperty( $scope, name, {\r\n            get: function() {\r\n                return record.get( name );\r\n            },\r\n            set: function( newValue ) {\r\n                if( newValue !== undefined ) {\r\n                    record.set( name, newValue );\r\n                }\r\n            }\r\n        });\r\n\r\n        record.subscribe(function() {\r\n            if( !$scope.$$phase ) {\r\n                $scope.$apply();\r\n            }\r\n        });\r\n    };\r\n})\r\n```\r\n\r\n### Putting it all together\r\nTo see all this in context, have a look at deepstream's [Angular 1 example app](https://github.com/deepstreamIO/ds-demo-simple-app-ng)\r\n\r\n![Simple App using deepstream and Angular](simple-app.png)"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_frontend-knockout_readme.md"}}
{"filePath":"tutorials/integrations/frontend-knockout/readme.md","title":"KnockoutJs","description":"Learn how to use KnockoutJs with deepstream","content":"\r\n\r\n## What's KnockoutJs?\r\nYou've probably heard of Angular, React and EmberJS, maybe even Vue or Polymer. But [KnockoutJS](http://knockoutjs.com/)? No? That's a shame...\r\nKnockout is a clever library with only one concept: An \"observable\" - something with a value that tells you when that value changes.\r\n\r\nGranted, Knockout comes with all sorts of other concepts that let you work with observables as well: templating to render the observable's value to the view, observable arrays to maintain collections of observables or \"bindings\", similar to Angular directives, that let you link observables with form elements.\r\n\r\n## Is KnockoutJS a good fit for deepstream?\r\nLarger realtime applications can easily get complex. Changes occur at any time and updates need to be processed in all sorts of orders. Trying to orchestrate this yourself can be an enormous task.\r\nTo illustrate what I mean, here's what it takes to keep an input field in sync with a record's value using jQuery.\r\n\r\n```javascript\r\nclient.record.getRecord('book/moby-dick').whenReady(record => {\r\n  const input = $('#book-title')\r\n\r\n  record.subscribe('title', title => {\r\n    input.val(title)\r\n  }, true)\r\n\r\n  input.change(() => {\r\n    record.set('title', input.val())\r\n  })\r\n})\r\n```\r\n\r\nNow that's not exactly the end of the world, but with Knockout and deepstream's [KoTools](https://github.com/deepstreamIO/deepstream.io-tools-ko) this can be reduced to\r\n\r\n```javascript\r\nthis.title = koTools.getObservable(record, 'title')\r\n```\r\n\r\n## Functional-Reactive Programming (hurray!)\r\nKnockout makes it easy to implement a paradigm called [functional reactive programming](https://en.wikipedia.org/wiki/Functional_reactive_programming) that's perfectly suited for realtime apps.\r\nIn FRP, you model the data in your application as \"pipelines\", \"asynchronous data-flows\" or \"event-streams\". Logic lives within these streams as junctions, valves or task pipelines and is applied in response to incoming data. This keeps complexity low and releases you from having to orchestrate interrelated states.\r\n\r\n## Ko-Tools\r\nKnockout has observable properties and observable arrays. Deepstream has observable lists and records with path bindings. Our tool maps the two together. You can get if from NPM or Bower as `deepstream.io-tools-ko` or [browse the source on Github](https://github.com/deepstreamIO/deepstream.io-tools-ko).\r\n\r\n```javascript\r\n// Create a list that's two-way bound to a ko.observableArray\r\nAppViewModel = function() {\r\n  const userList = client.record.getList('users')\r\n  this.users = koTools.getViewList(UserViewModel, userList)\r\n}\r\n\r\n// Create a record and create two-way bound ko.observables\r\nUserViewModel = function(userRecordName, viewList) {\r\n  this.record = client.record.getRecord(userRecordName)\r\n  this.viewList = viewList;\r\n  this.firstname = koTools.getObservable(this.record, 'firstname')\r\n  this.lastname = koTools.getObservable(this.record, 'lastname')\r\n  this.isActive = ko.observable(false)\r\n}\r\n```\r\n\r\n## Example App\r\nYou can find an example app using deepstream, KnockoutJS and the ko-tools on [Github](https://github.com/deepstreamIO/ds-demo-simple-app-ko).\r\n\r\n![Example app using KnockoutJS](simple-app.png)\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_frontend-polymer_readme.md"}}
{"filePath":"tutorials/integrations/frontend-polymer/readme.md","title":"Polymer","description":"Learn how to use Polymer with deepstream","content":"\r\n\r\n![deepstream-polymer](deepstream-polymer.png)\r\n\r\nPolymer brings WebComponents to all browsers, allowing for the creation of web apps using DOM elements as the primary building blocks.\r\n\r\nDeepstream comes with its own polymer plugin that makes it easy to add data-sync directly to these DOM elements.\r\n\r\n```html\r\n<ds-record name=\"myRecordName\" data=\"{{recordData}}\"></ds-record>\r\n```\r\n\r\nor - if required - by extending existing elements with a minimal bit of javascript using [behaviours](//www.polymer-project.org/1.0/docs/devguide/behaviors.html):\r\n```javascript\r\nbehaviours: [ Polymer.DSRecordBehaviour ]\r\n```\r\n\r\nYou can find the documentation for all deepstream-polymer elements and behaviours here:\r\n\r\n<a class=\"mega\" href=\"//deepstreamio.github.io/deepstream.io-tools-polymer\"><i class=\"fa fa-book\"></i>Polymer Documentation</a>\r\n\r\n### Getting started with deepstream-polymer\r\nThe beauty of using Polymer is that you can build an entire data-sync application without a single line of javascript! Let's look at the building blocks you can use to get deepstream working with Polymer.\r\n\r\nYou can install deepstream-polymer via Bower using\r\n```bash\r\nbower install deepstream.io-tools-polymer\r\n```\r\nor get it directly from [Github](https://github.com/deepstreamIO/deepstream.io-tools-polymer).\r\n\r\nBefore we begin, let's start a deepstream server. If you haven't done that before, have a look at the [getting started tutorial](../../core/getting-started-quickstart/).\r\n\r\n#### Connectivity\r\nFirst, let's connect to deepstream. This can be accomplished by adding a `ds-connection` element. This creates a connection to deepstream based on the `url` attribute you've specified and exposes the [connection state](/docs/common/constants/#connection-state) and deepstream instance.\r\n\r\n```html\r\n<ds-connection\r\n    url=\"localhost:6020\"\r\n    state=\"{{connectionState}}\"\r\n    ds={{ds}}>\r\n</ds-connection>\r\n```\r\n\r\nOnce we have a connection it's time to login. This can be achieved via adding a `ds-login` element. You can also log in anonymously by having an `auto-login` attribute present.\r\n\r\n```html\r\n<ds-login\r\n    auto-login\r\n    logged-in=\"{{loggedIn}}\"\r\n    ds=\"[[ds]]\">\r\n</ds-login>\r\n```\r\n\r\nWant to login with an actual user-name or password? Just supply them via your `auth-params` and call `login` when you're ready. Note, this step does require a tiny bit of javascript!\r\n\r\n```html\r\n<ds-login\r\n    auth-params=\"{{credentials}}\"\r\n    logged-in=\"{{loggedIn}}\"\r\n    ds=\"[[ds]]\">\r\n    <input type=\"text\" value=\"{{credentials.username::input}}\"/>\r\n    <input type=\"password\" value=\"{{credentials.username::password}}\"/>\r\n    <!--\r\n        Note: login binding is on the current scope and will require\r\n        to be proxied to the login method - or you could create your own\r\n        element that uses the LoginBehaviour\r\n    -->\r\n    <button on-click=\"{{login}}\" />\r\n</ds-login>\r\n```\r\n\r\n#### Records\r\nNow that we're connected you can enable your elements to auto-sync their data with all other clients simply by using a `ds-record` element. This requires three attributes: a `name` to know what record to use, `data` to allow reading and writing to the record and `ds` to expose the deepstream client to the element.\r\n\r\n```html\r\n<template>\r\n    <ds-connection ds=\"{{ds}}\"></ds-connection>\r\n    <ds-record name=\"[[name]]\" data=\"{{data}}\" ds=\"[[ds]]\">\r\n        <input type=\"text\" value=\"{{data.name::input}}\">\r\n    </ds-record>\r\n</template>\r\n```\r\n\r\nNote how there is another `ds-connection` element present. This is to access the same deepstream connection using a [global variable](https://github.com/Polymer/docs/issues/334) across the application.\r\n\r\n#### Lists\r\nFinally, let's look at [lists](https://deepstream.io/tutorials/lists.html). Lists are collections of record that are related to each other and can be iterated. This can be achieved by using a `ds-list` element. The attributes used are the same as `ds-record`, except the record names are exposed via an array called `entries`.\r\n\r\n```html\r\n<template>\r\n    <ds-connection ds=\"{{ds}}\"></ds-connection>\r\n    <ds-list name=\"[[name]]\" entries=\"{{todos}}\" ds=\"[[ds]]\">\r\n        <template is=\"dom-repeat\" items=\"[[todos]]\" as=\"recordId\">\r\n            <div>RecordId : {{recordId}}</div>\r\n        </template>\r\n    </ds-list>\r\n</template>\r\n```\r\n\r\nAnd that's it, everything needed to create an application with data-sync!\r\n\r\n#### Building a more complex app?\r\nHere's an example of how to combine all these concepts to build a to-do list with Polymer:\r\n\r\n![basic todo app with deepstream-polymer](polymer-example-app.gif)\r\n\r\n<a class=\"mega\" href=\"//github.com/deepstreamIO/ds-tutorial-polymer\"><i class=\"fa fa-github\"></i>todo-list example</a>"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_frontend-react_readme.md"}}
{"filePath":"tutorials/integrations/frontend-react/readme.md","title":"React","description":"Learn how to use React with deepstream","content":"\r\n\r\n![deepstream-react](deepstream-react.png)\r\n\r\ndeepstream and react share the same belief: apps are best composed from reusable components, driven by state. What deepstream brings to react is the ability to store this state and sync it across connected clients.\r\n\r\nTo make this easier, we've developed **deepstream-react** - a mixin that let's you add realtime sync to any component with just a single line of code.\r\n\r\n```javascript\r\nmixins: [DeepstreamReact]\r\n```\r\n\r\n## Video Tutorial\r\n\r\n<iframe width=\"640\" height=\"480\" src=\"https://www.youtube.com/embed/Bg0nyf02jkw?rel=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>\r\n\r\n## How to use deepstream-react\r\nYou can get deepstream-react from NPM or bower as `deepstream.io-tools-react` or [browse the source on Github](https://github.com/deepstreamIO/deepstream.io-tools-react).\r\n\r\ndeepstream has a concept called \"records\". A record is a bit of JSON data that can be observed and manipulated by clients and that's stored and synced by the deepstream server.\r\n\r\ndeepstream-react binds a deepstream record to a react component's state. Here's what that looks like:\r\n\r\n![basic input with deepstream-react](basic-react-input.gif)\r\n\r\nLet's replicate the example above. First, you need a deepstream server running on port 6020. If you haven't used deepstream yet, quickly head over to the [getting started tutorial](/tutorials/core/getting-started-quickstart/)...don't worry, I'll wait.\r\n\r\nOnce your server is running, it's time to create our react-app. Let's start by installing the deepstream javascript client and deepstream-react\r\n\r\n```\r\nnpm install deepstream.io-client-js deepstream.io-tools-react --save\r\n```\r\n\r\nNext: connect to the server, log in and register the client instance with deepstream-react. Sounds tougher than it is:\r\n\r\n```javascript\r\nconst deepstream = require('deepstream.io-client-js')\r\nconst DeepstreamMixin = require('deepstream.io-tools-react')\r\n\r\nconst client = deepstream('localhost:6020').login({}, () => {\r\n  //ReactDOM.render call will go in here\r\n})\r\nDeepstreamMixin.setDeepstreamClient(client)\r\n```\r\n\r\nEvery deepstream record is identified by a unique name. To tell your component which record it should use, you need to specify a `dsRecord` property.\r\n\r\n```jsx\r\nReactDOM.render(\r\n  <SyncedInput dsRecord=\"some-input\" />,\r\n  document.getElementById('example')\r\n)\r\n```\r\n\r\nAnd that's it. Just write your react-components as usual, all changes will be persisted and synced via deepstream.\r\n\r\n```jsx\r\nconst SyncedInput = React.createClass({\r\n  mixins: [DeepstreamMixin],\r\n  setValue: function(e) {\r\n    this.setState({value: e.target.value})\r\n  },\r\n  render: function() {\r\n    return (\r\n      <input value={this.state.value} onChange={this.setValue} />\r\n    )\r\n  }\r\n})\r\n```\r\n\r\nYou can find also the code for this example on [Github](https://github.com/deepstreamIO/ds-tutorial-react/tree/master/synced-input)\r\n\r\n### What about state that I don't want to be synced?\r\nQuite often your component state contains data that you don't want to be stored or synced, e.g. temporary values from an input field that need to be validated first or composite values, e.g. `fullName` that are composed from a first and lastname entry.\r\n\r\nFor those values, deepstream-react supports a `local` namespace. Just store anything you want to be excluded under it.\r\n\r\n```javascript\r\nthis.setState({\r\n    importantData: 'this will be synced',\r\n    local: {\r\n        temporaryData: 'this will be excluded'\r\n    }\r\n});\r\n```\r\n\r\n### How about a more complex example?\r\nGranted, a single input doesn't constitute an app - and it's often easier to see things being used in context. So [here's a take on react's classic todo-app](https://github.com/deepstreamIO/ds-tutorial-react/tree/master/todo-list), using *deepstream-react*.\r\n\r\n![todo list example with deepstream-react](complex-react-example.gif)\r\n\r\n### Prefer to use deepstream directly?\r\nNo problem, raw deepstream works just as well with react. You can find an [example app that demonstrates that here](https://github.com/deepstreamIO/ds-demo-simple-app-react):\r\n\r\n![Simple App using React](simple-app.png)\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_msg-amqp_readme.md"}}
{"filePath":"tutorials/integrations/msg-amqp/readme.md","title":"AMQP Message Connector","description":"Using AMQP as a message bus for deepstream clusters","content":"\r\n\r\n#### What is AMQP?\r\nThe \"Advanced Message Queueing Protocol\" is implemented by a large number of brokers, e.g. [RabbitMQ](https://www.rabbitmq.com/), [Qpid](https://qpid.apache.org/), [HornetQ](http://hornetq.jboss.org/) or [ActiveMQ](http://activemq.apache.org/) to name just a few. AMQP supports powerful routing patterns and achieves high reliability through features such as persistent queues or guaranteed message delivery.\r\n\r\n#### Why use AMQP with deepstream?\r\nAMQP is reliable and widely adopted. It also comes fully managed as part of cloud hosting offerings like [Azure's Service Bus](https://azure.microsoft.com/en-us/services/service-bus/).\r\n\r\n#### When not to use AMQP with deepstream?\r\nDeepstream doesn't actually use the advanced messaging and routing patterns that AMQP offers. Instead, it sets up a single `topic-exchange` for name based routing and binds a queue per topic and client to it to create a publish/subscribe workflow.\r\n\r\nEqually, deepstream doesn't necessarily require the advanced persistence and guaranteed message delivery features that AMQP offers - data is stored within the storage layer and messaging is only used to propagate updates - meaning that subsequent messages will reconcile a corrupted state.\r\n\r\n#### How to use AMQP with deepstream?\r\nDeepstream offers an official plugin to connect to AMQP-clusters. It can be installed via deepstream's Command Line Interface using the `msg` keyword, e.g.\r\n\r\n```bash\r\ndeepstream install msg amqp\r\n```\r\n\r\nIf you're using deepstream in Node, you can also install it via [NPM](https://www.npmjs.com/package/deepstream.io-msg-amqp)\r\n\r\n#### How to configure the amqp connector?\r\nYou can configure the amqp connector in the `plugins` section of deepstream's config.yml file (by default either in the `conf` directory or in `/etc/deepstream` on Linux)\r\n\r\n```yaml\r\nplugins:\r\n  message:\r\n    name: amqp\r\n    options:\r\n      host: <String>\r\n      port: <Number>\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_msg-kafka_readme.md"}}
{"filePath":"tutorials/integrations/msg-kafka/readme.md","title":"Kafka Message Connector","description":"Learn how to use Kafka with deepstream","content":"\r\n\r\n#### What is Apache Kafka?\r\nKafka started life as the messaging system that powered LinkedIn. It was open-sourced by the Apache Foundation in 2011 and has since found its way as the message broker of choice into many large enterprise organisations.\r\n\r\nAt its core, Kafka is a distributed publish/subscribe system that can scale to almost biblical dimensions. It's highly reliable through features like message persistence, buffering, message replication and guaranteed delivery, but can be a bit of a handful to set up and run.\r\n\r\n![Apache Kafka](kafka.png)\r\n\r\n#### Why use Kafka with deepstream?\r\nDeepstream can scale horizontally by creating clusters of nodes that communicate with each other via a messagebus. Kafka can be used as such a message bus. It provides the same reliability as AMQP brokers, but is faster and can be extended to a much larger scale. It is the recommended choice for seriously large deepstream deployments with high availability requirements.\r\n\r\n#### When not to use Kafka with deepstream?\r\nKafka is a bit like a Jumbo Jet: Great if you need to fly many people over large distances at high speeds - but a bit much if you just want to pop down to the shop for some groceries. For many small to medium sized deployments, Redis might be a better choice. It doesn't offer the same messaging guarantees, but is faster than Kafka and way easier to set up and run. It also doubles as a cache and persists data to disk, making it the perfect companion for any but the largest deepstream clusters.\r\n\r\n#### How to use Kafka with deepstream?\r\nDeepstream offers an official plugin to connect to Kafka-clusters. It can be installed via deepstream's Command Line Interface using the `msg` keyword, e.g.\r\n\r\n```bash\r\ndeepstream install msg kafka\r\n```\r\n\r\nIf you're using deepstream in Node, you can also install it via [NPM](https://www.npmjs.com/package/deepstream.io-msg-kafka)\r\n\r\n#### How to configure the Kafka connector?\r\nYou can configure the Kafka connector in the `plugins` section of deepstream's config.yml file (by default either in the `conf` directory or in `/etc/deepstream` on Linux)\r\n\r\n```yaml\r\nplugins:\r\n  message:\r\n    name: kafka\r\n    options:\r\n      connectionString: <String>\r\n      clientId: <String>\r\n```\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_other-http_readme.md"}}
{"filePath":"tutorials/integrations/other-http/readme.md","title":"HTTP Servers","description":"Learn how to use a webserver together with deepstream","content":"\r\n\r\nDeepstream is a realtime data server and a great one of that. It is not an HTTP or general purpose webserver though. That means that if you're building a webapplication that needs to serve HTML or CSS files, images etc., you'll also need a classic HTTP server. For that, you have a number of choices\r\n\r\n## Using Nginx, Apache, Tomcat, IIS etc.\r\nYou can use any established webserver together with deepstream. Just make sure that the URL path to route HTTP/WS traffic (by default `yourdomain.com/deepstream`) is proxied forward.\r\n\r\nYou can change this path by setting `urlPath` in the server config and `path` in the client options to a different value.\r\n\r\nHere's an example of what the proxy configuration would look like for Nginx. To learn more about how to use Nginx as a reverse proxy and load balancer for deepstream, head over to the [Nginx Tutorial](../other-nginx/)\r\n\r\n```bash\r\n# in the http{} configuration block\r\nupstream deepstream {\r\n    ip_hash;\r\n    server localhost:6020;\r\n    # add more servers here for loadbalancing\r\n}\r\n\r\nserver {\r\n    server_name app.domain.com;\r\n    listen 80;\r\n    location /deepstream {\r\n        proxy_set_header Upgrade $http_upgrade;\r\n        proxy_set_header Connection \"upgrade\";\r\n        proxy_http_version 1.1;\r\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\r\n        proxy_set_header Host $host;\r\n        proxy_pass http://deepstream;\r\n    }\r\n}\r\n```\r\n\r\n## Using a CDN / static file hosting service\r\nEspecially for larger deployments it can make perfect sense to keep your server logic in deepstream and serve assets via a static file host, fronted by a Content Delivery Network (CDN). Similar to the section above, all that's necessary here is to exclude the `/deepstream` path from the CDN. Depending on your CDN of choice, this can be a bit tricky though. [AWS Cloudfront](https://aws.amazon.com/cloudfront/) for instance only allows proxying of HTTP traffic, so deepstream traffic needs to be re-routed on an [Elastic Load Balancer](https://aws.amazon.com/elasticloadbalancing/) level, [more about this here](https://forums.aws.amazon.com/thread.jspa?messageID=589328). Other CDNs like [CloudFlare](https://www.cloudflare.com/) support socket traffic directly, [more about this here](https://blog.cloudflare.com/cloudflare-now-supports-websockets/).\r\n\r\n## Using deepstream in Node with ExpressJS, Koa or Hapi\r\nIf you're using deepstream in Node.js, it can share a HTTP server with frameworks such as [Express](//expressjs.com/), [Koa](//koajs.com/) or [Hapi](//hapijs.com/).\r\n\r\nFor ExpressJS for instance, you'd create your server as follows\r\n\r\n```javascript\r\nvar Deepstream = require( 'deepstream.io' );\r\nvar http = require( 'http' );\r\nvar express = require( 'express' );\r\n\r\n// Create an express app\r\nvar app = express();\r\n\r\n// Explicitly create a http server and\r\n// register the express app as an request listener\r\nvar server = http.createServer( app );\r\n\r\n// Write your express code as usual\r\napp.get( '/hello', function ( req, res ) {\r\n  res.send( 'Hello to you too!' );\r\n});\r\n\r\n// Create your deepstream server\r\nvar deepstream = new Deepstream();\r\n// Pass it the existing HTTP server\r\ndeepstream.set( 'httpServer', server );\r\n// Start deepstream\r\ndeepstream.start();\r\n\r\n// Start the http server explicitly,\r\n// rather than calling app.listen()\r\nserver.listen( 6020, function(){\r\n    console.log( 'HTTP server listening on 6020' );\r\n});\r\n```\r\n\r\nHere's an example to see this in action\r\n\r\n<a class=\"mega\" href=\"//github.com/deepstreamIO/ds-tutorial-express\"><i class=\"fa fa-github\"></i>deepstream & express example on Github</a>\r\n"}
{"index":{"_index":"pages","_type":"tutorials","_id":"tutorials_integrations_other-nginx_readme.md"}}
{"filePath":"tutorials/integrations/other-nginx/readme.md","title":"Nginx","description":"Using Nginx as a reverse proxy and load balancer for HTTP, Websocket & TCP traffic","content":"\r\n\r\n## What is nginx?\r\n[Nginx](https://nginx.org/) (pronounced engine-x) is a multi purpose webserver. It's one of the most widely used HTTP servers and powers sites such as GitHub or reddit. Aside from serving static files via HTTP, it can be used as a reverse proxy, multi protocol load balancer or container for fast CGI scripts\r\n\r\n## Using nginx and deepstream\r\nNginx can be used as a web-facing reverse proxy and load balancer in front of deepstream servers.\r\n\r\n![A simple deployment with nginx and deepstream](deepstream-nginx-deployment-diagram.png)\r\n\r\n#### Reverse Proxy\r\nFor HTTP deployments it is common practice to not directly expose the webserver to the internet, but instead place a different server in front of it. For deepstream.io production deployments we highly recommend doing the same.\r\nThis \"reverse proxy\" handles tasks like SSL termination (decrypting incoming messages via HTTPS / WSS / TLS) and high availability / error reporting (e.g. replying to requests with a 500 status if the underlying server is unavailable).\r\n\r\n#### Load Balancer\r\nDeepstream can scale horizontally via clustering. If you want to provide a single URL for clients to connect to your cluster, you need to place something in front that distributes incoming connections between the available servers: a load balancer.\r\nLoad balancing persistent connections can be a bit tricky sometimes. Deepstream supports connections via TCP and via HTTP/WebSocket. Browser connections start as HTTP and are upgraded to WebSocket as soon as possible. The challenge here is to make sure that both the initial HTTP connection and the subsequent WebSocket connection by the same client are routed to the same server by the load balancer - a feature called \"cross protocol sticky sessions\". Nginx does a great job of solving this by associating sessions with a hash of the client's IP, as configured via `hash $remote_addr consistent;` in the `upstream` group (see example further down)\r\n\r\n## Alternatives to nginx\r\nInstead of nginx you could also use e.g. [HA Proxy](http://www.haproxy.org/) or [Apache](https://httpd.apache.org/)\r\n\r\n#### What about AWS Elastic Load Balancer?\r\nIf you're deploying deepstream on AWS, you'd probably want to use Amazon's well integrated load balancing service ELB. At the time of writing (June 2016) ELB however lacks support for \"sticky sessions during a protocol switch\", a crucial feature that's required for deepstream (and libraries that help with bi-directional browser-connectivity, e.g. socket.io).\r\n\r\n## Installing nginx for use with deepstream\r\nBy default, Nginx comes with everything you need to use it as an HTTP server. To use it as a stream/TCP server though, you need to build it with its stream module enabled (`--with-stream`). On CentOS/AWS Linux this works as follows, for other Linux distributions, have a [look here](https://www.nginx.com/resources/admin-guide/installing-nginx-open-source/).\r\n\r\n```bash\r\n# install gcc (needed to compile nginx)\r\nsudo yum update\r\nsudo yum install gcc\r\n\r\n# download and unzip nginx stable version (check for latest version number before using)\r\nwget http://nginx.org/download/nginx-1.10.1.tar.gz\r\ntar zxf nginx-1.10.1.tar.gz\r\ncd nginx-1.10.1\r\n\r\n# enable stream, disable unneeded http modules that require additional dependencies\r\n./configure --with-stream --without-http_rewrite_module --without-http_gzip_module\r\n\r\n# build and install\r\nmake\r\nsudo make install\r\n```\r\n\r\n## Configuring nginx as a stream proxy / load balancer\r\nThe following configuration shows how to use nginx as a load balancer, SSL termination point and reverse proxy for HTTP, WS and TCP connections. If you only want to use parts of this functionality, remove the unneeded bits.\r\n\r\n```nginx\r\nworker_processes  1;\r\n\r\nerror_log /usr/local/nginx/logs/error.log info;\r\n\r\nevents {\r\n    worker_connections  1024;\r\n}\r\n\r\nstream {\r\n    # define all http/ws endpoints\r\n    upstream browserendpoint {\r\n        hash $remote_addr consistent; #ensure sticky session between long-polling and ws\r\n        server localhost:6022; #load balance connections between these two\r\n        server localhost:6020; #add more as required\r\n    }\r\n\r\n    server {\r\n        listen 6040 ssl; #external browser port\r\n        proxy_pass browserendpoint;\r\n\r\n        # SSL Termination, comment this section if you don't\r\n        # want to use HTTPS/WSS (don't forget to remove the ssl after listen)\r\n        ssl_certificate         /etc/ssl/certs/yourcert.crt;\r\n        ssl_certificate_key     /etc/ssl/certs/yourkey.key;\r\n        ssl_protocols           TLSv1 TLSv1.1 TLSv1.2;\r\n        ssl_ciphers             HIGH:!aNULL:!MD5;\r\n        ssl_session_cache       shared:SSL:20m;\r\n        ssl_session_timeout     4h;\r\n        ssl_handshake_timeout   30s;\r\n    }\r\n\r\n     # define all tcp endpoints\r\n    upstream tcpendpoint {\r\n        hash $remote_addr consistent;\r\n        server localhost:6021; #load balance connections between these two\r\n        server localhost:6023; #add more as required\r\n    }\r\n\r\n    server {\r\n        listen 6041 ssl; #external tcp port\r\n        proxy_pass tcpendpoint;\r\n\r\n        # SSL Termination, comment this section if you don't\r\n        # want to use HTTPS/WSS\r\n        ssl_certificate         /etc/ssl/certs/yourcert.crt;\r\n        ssl_certificate_key     /etc/ssl/certs/yourkey.key;\r\n        ssl_protocols           TLSv1 TLSv1.1 TLSv1.2;\r\n        ssl_ciphers             HIGH:!aNULL:!MD5;\r\n        ssl_session_cache       shared:SSL:20m;\r\n        ssl_session_timeout     4h;\r\n        ssl_handshake_timeout   30s;\r\n    }\r\n}\r\n```\r\n"}
{"index":{"_index":"pages","_type":"info","_id":"info_specs_parsing_parsing.md"}}
{"filePath":"info/specs/parsing/parsing.md","title":"Parsing Features","description":"Cucumber features for Parsing in deepstream.io","content":"<h1>parsing</h1>\n{{> specs-paths messageSpecs.features.parsing }}"}
{"index":{"_index":"pages","_type":"info","_id":"info_specs_rpc_rpc.md"}}
{"filePath":"info/specs/rpc/rpc.md","title":"Rpc Features","description":"Cucumber features for Rpc in deepstream.io","content":"<h1>rpc</h1>\n{{> specs-paths messageSpecs.features.rpc }}"}
{"index":{"_index":"pages","_type":"info","_id":"info_specs_records_records.md"}}
{"filePath":"info/specs/records/records.md","title":"Records Features","description":"Cucumber features for Records in deepstream.io","content":"<h1>records</h1>\n{{> specs-paths messageSpecs.features.records }}"}
{"index":{"_index":"pages","_type":"info","_id":"info_specs_events_events.md"}}
{"filePath":"info/specs/events/events.md","title":"Events Features","description":"Cucumber features for Events in deepstream.io","content":"<h1>events</h1>\n{{> specs-paths messageSpecs.features.events }}"}
{"index":{"_index":"pages","_type":"info","_id":"info_specs_connectivity_connectivity.md"}}
{"filePath":"info/specs/connectivity/connectivity.md","title":"Connectivity Features","description":"Cucumber features for Connectivity in deepstream.io","content":"<h1>connectivity</h1>\n{{> specs-paths messageSpecs.features.connectivity }}"}
{"index":{"_index":"pages","_type":"info","_id":"info_protocol_all-messages_all-messages.md"}}
{"filePath":"info/protocol/all-messages/all-messages.md","title":"All Message Specs","description":"Message Specs in deepstream.io","content":"{{> specs-structure}}"}
